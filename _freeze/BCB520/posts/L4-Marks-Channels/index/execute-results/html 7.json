{
  "hash": "3b765d8edc83a3a418d9ce93233000e0",
  "result": {
    "markdown": "---\ntitle: \"LECTURE 4\"\nsubtitle: \"Marks and Channels\"\nformat:\n  revealjs: \n    slide-number: true\n    chalkboard: \n      buttons: false\n    preview-links: auto\n    css: styles.css\n    footer: <a href=\"https://canvas.uidaho.edu/courses/17806\" target=\"_blank\">CANVAS</a>...<a href=\"https://professorpolymorphic.github.io/RobisonWebSite/BCB520/BCB520.html\" target=\"_blank\">HOME</a>\n    theme: [default, custom.scss]\nauthor: \"Barrie Robison\"\ndate: \"2023-01-31\"\ncategories: [Lecture, DataViz, Idiom, Observable, Tidyverse]\nimage: \"idiom.png\"\ncode-fold: true\ndescription: \"Just what is an **IDIOM**, anyway?\"\n---\n\n\n## PLAN\n\n1.  Assignment 3 Review.\n2.  Aligning the VAD model with other frameworks.\n3.  Encoding with Marks and Channels.\n\n## ASSIGNMENT 3 REVIEW\n\n[Rondald's Portfolio](https://ronbentil.github.io/BCB504Portfolio/)\n\n[Jiyin's Portfolio](https://chubl.github.io/bcb504-blog/)\n\n[Cody's Portfolio](https://cody-appa.github.io/Data_Science_Portfolio/)\n\n[Erik's Portfolio](https://erickrios5.github.io/BCB-504-My-Data-Blog/)\n\n## VAD MODEL\n\n![](VADmodel.png)\n\n## UNDERSTAND THE DATA\n\nComputer-based visualization systems provide visual representations of [datasets]{.red} designed to help people carry out tasks more effectively.\n\n![](whatexpanded.png){.absolute bottom=\"0\" right=\"0\" width=\"450\"}\n\n![](what.png){.absolute bottom=\"0\" height=\"400\"}\n\n## UNDERSTAND THE TASK\n\nComputer-based visualization systems provide visual representations of datasets designed to [help people carry out tasks]{.red} more effectively.\n\n![](Whybig.png){.absolute bottom=\"0\" right=\"0\" width=\"550\"}\n\n![](Why.png){.absolute bottom=\"70\" height=\"300\"}\n\n## VISUAL ENCODING\n\nComputer-based visualization systems provide [visual representations]{.red} of datasets designed to help people carry out tasks more effectively.\n\n![](Howbig.png){.absolute bottom=\"0\" left=\"100\" width=\"700\"}\n\n## OTHER FRAMEWORKS\n\n1.  The Tidyverse\n2.  The Grammar of Graphics\n3.  Tufte\n\n## TIDYVERSE {.smaller}\n\n**R packages for data science:**\n\n::: columns\n::: {.column width=\"45%\"}\n[The tidyverse](https://www.tidyverse.org) is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. The best way to explore and understand the tidyverse is with [cheetsheets](), like [this one](https://posit.co/wp-content/uploads/2022/10/tidyr.pdf) for `tidyr`!\n:::\n\n::: {.column width=\"55%\"}\n![](tidyverse.png){height=\"400\" bottom=\"100\" left=\"0\"}\n:::\n:::\n\n## GRAMMAR OF GRAPHICS\n\nThe [ggplot2 cheatsheet](https://posit.co/wp-content/uploads/2022/10/data-visualization-1.pdf)!\n\n::: columns\n::: column\n![](ggplotbasics.png)\n:::\n\n::: column\n![](ggplottemplate.png)\n:::\n:::\n\n## TUFTE\n\n[Tufte's Website](https://www.edwardtufte.com/tufte/)\n\n[A Quarto Page Layout Example](https://quarto-dev.github.io/quarto-gallery/page-layout/tufte.html)\n\n![](5-books-stacked.jpg)\n\n## ANALYSIS FRAMEWORK {.smaller}\n\nFour levels, three questions\n\n::: columns\n::: {.column width=\"65%\"}\n-   **Domain** situation defines the target users.\n-   **Abstraction** translate from specifics of domain to vocabulary of vis\n    -   [WHAT]{.red} is shown? data abstraction\n    -   [WHY]{.yellow} is the user looking at it? task abstraction\n-   **Idiom** defines the visualization\n    -   [HOW]{.green} is it shown?\n        -   visual encoding idiom: how to draw\n        -   interaction idiom: how to manipulate\n-   **Algorithm** creates the visualization\n    -   evaluated with computational efficiency\n:::\n\n::: {.column width=\"35%\"}\n![](VADwithwhatwhyhow.png)\n:::\n:::\n\n## ENCODING\n\nWe are defining the structure of the visualization (the idiom).\n\nTo do this, we use [MARKS]{.red} and [CHANNELS]{.red}:\n\n-   [MARKS]{.red} represent **ITEMS** or **LINKS**\n\n-   [CHANNELS]{.red} change the appearance of **MARKS** based on **ATTRIBUTES**\n\n## MARKS FOR ITEMS\n\n![](basicgeometric.png)\n\n## MARKS FOR LINKS\n\n![](marksforlinks.png){.absolute top=\"100\" height=\"175\"}\n\n![](bubblesets.png){.absolute top=\"300\" left=\"0\" height=\"250\"} [[Bubblesets](vialab.science.uoit.ca/portfolio/bubblesets)]{.absolute left=\"0\" bottom=\"70\"}\n\n![](forcedirected.png){.absolute top=\"300\" left=\"330\" height=\"250\"} [[Force Directed Graph](https://observablehq.com/@d3/force-directed-graph)]{.absolute left=\"330\" bottom=\"70\"}\n\n## OBSERVABLE IN QUARTO!\n\n\n```{ojs}\n//| echo: true\n//| code-fold: true\n//| code-tools: true\nd3 = require(\"d3@7\")\n\n\nchart = ForceGraph(miserables, {\n  nodeId: d => d.id,\n  nodeGroup: d => d.group,\n  nodeTitle: d => `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l => Math.sqrt(l.value),\n  width,\n  height: 1000,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\nmiserables = FileAttachment(\"miserables.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d => d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) => source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) => target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 1000, // outer width, in pixels\n  height = 1000, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) => N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) => ({id: N[i]}));\n  links = d3.map(links, (_, i) => ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) => N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) => W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) => L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) => color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) => T[i]);\n  if (invalidation != null) invalidation.then(() => simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y);\n\n    node\n      .attr(\"cx\", d => d.x)\n      .attr(\"cy\", d => d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n```\n\n\n## CHANNELS {.smaller}\n\n::: columns\n::: {.column width=\"40%\"}\n-   **CHANNELS** control the appearance of **MARKS**.\\\n-   They are proportional to or based on **ATTRIBUTES**.\n-   Their properties differ in the type and amount of information that can be conveyed to the human perceptual system.\n:::\n\n::: {.column width=\"60%\"}\n![](channels.png)\n:::\n:::\n\n## VISUAL ENCODING EXAMPLE\n\nLet's analyze the idiom structures below in terms of marks and channels.\n\n![](simpleencode.png)\n\n## REDUNDANT ENCODING\n\nUses multiple channels for the same attribute.\n\n-   Sends a stronger message\n-   Uses up channels\n\n![](lengthluminance.png){.absolute right=\"0\" bottom=\"50\" height=\"400\"}\n\n## CHOOSING CHANNELS\n\n-   [EXPRESSIVENESS]{.red}\n    -   Match channel to data type.\n-   [EFFECTIVENESS]{.red}\n    -   Channels differ in accuracy of perception.\n\n## CHANNEL RANKINGS {.smaller}\n\n![](ChannelRank.png)\n\n[Note that spatial position ranks high for both types of channels.]{.absolute bottom=\"20\" right=\"0\" width=\"300\"}\n\n## GROUPING\n\n::: columns\n::: {.column width=\"50%\"}\n-   Containment\n-   Connection\n-   Proximity\n    -   Same spatial region.\n-   Similarity\n    -   Same values as other channels.\n:::\n\n::: {.column width=\"50%\"}\n![](marksforlinks.png)\n\n![](IdentityChannels.png)\n:::\n:::\n\n## SUMMARY SO FAR\n\n![](basicgeometric.png){.absolute left=\"0\" height=\"100\" top=\"100\"}\n\n![](marksforlinks.png){.absolute right=\"0\" height=\"100\" top=\"100\"}\n\n![](ChannelRank.png){.absolute bottom=\"0\" right=\"100\" width=\"650\"}\n\n## CHANNEL EFFECTIVENESS\n\n-   [Accuracy:]{.red} how precisely can we tell the difference between encoded items?\n-   [Discriminability:]{.red} how many unique steps can we perceive?\n-   [Separability:]{.red} is our ability to use this channel affected by another one?\n-   [Popout:]{.red} can things jump out using this channel?\n\n## ACCURACY (THEORY)\n\nSteven's Psychophisical Power Law: $S=I^N$\n\n::: {.columns .r-fit-text}\n::: {.column width=\"80%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"20%\"}\n[**LENGTH (N=1)**]{.fragment}\n\n[[ELECTRIC SHOCK (N=3.5)]{.red}]{.fragment}\n\n[[SATURATION (N=1.7)]{.blue}]{.fragment}\n\n[[AREA (N=0.7)]{.purple}]{.fragment}\n\n[[BRIGHTNESS (N=0.5)]{.yellow}]{.fragment}\n:::\n:::\n\n## ACCURACY (EXPERIMENTAL) {.smaller}\n\n:::: {.columns}\n::: {.column width=60%}\n![](ClevelandandMcGill.png)\n:::\n\n::: {.column width=40%}\n[[Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods](https://www.jstor.org/stable/2288400)]\n:::\n::::\n\n\n## DISCRIMINABILITY\n\nHow many usable steps are in the channel?  Are the differences between items perceptible to the human as intended?\n\n![](linewidths.png)\n\n## SEPARABILITY VS INTEGRALITY {.smaller}\n\nSeparable channels are orthogonal and independent.  Integral channels are inextricably combined.  Attempts to encode different information with integral channels creates [Interference]{.red}.\n\n![](sepandint.png)\n\n::: {.caption}\n**Figure 5.10.** Pairs of visual channels fall along a continuum from fully separable to intrinsically integral. Color and location are separable channels well suited to encode different data attributes for two different groupings that can be selectively attended to. However, size interacts with hue, which is harder to perceive for small objects. The horizontal size and and vertical size channels are automatically fused into an integrated perception of area, yielding three groups. Attempts to code separate information along the red and green axes of the RGB color space fail, because we simply perceive four different hues.\n\n\n:::\n\n\n## POPOUT {.smaller}\n**VISUAL POPOUT is often called preattentive processing or tunable detection.**\n\n::: columns\n::: {.column width=\"60%\"}\n**find the [red]{.red} dot! How long does it take?** \n\nPopout results from our low-level visual system performing massively parallel processing on certain visual channels, eliminating the need for the viewer to consciously direct attention to items one by one (serial search).\n\n:::{.caption}\n**Figure 5.11. Visual popout.** (a) The red circle pops out from a small set of blue circles. (b) The red circle pops out from a large set of blue circles just as quickly. (c) The red circle also pops out from a small set of square shapes, although a bit slower than with color. (d) The red circle also pops out of a large set of red squares. (e) The red circle does not take long to find from a small set of mixed shapes and colors. (f) The red circle does not pop out from a large set of red squares and blue circles, and it can only be found by searching one by one through all the objects.\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n![](popout.png)\n\n\n:::\n:::\n\n## POPOUT {.smaller}\n\n::: columns\n::: {.column width=\"60%\"}\nMany channels are compatible with preattentive processing and facilitate popout:\n\n-   tilt\n-   size\n-   shape\n-   proximity\n-   shadow direction\n\nBut not all! \n\n-   Example: parallel line pairs do not pop out from tilted pairs.\n\n:::\n\n::: {.column width=\"40%\"}\n![](popout2.png)\n:::\n:::\n\n\n\n## RELATIVE VS ABSOLUTE JUDGEMENTS {.smaller}\n\nThe human perceptual system is fundamentally based on relative judgements, not absolute ones. This is why accuracy increases with common frame/scale and alignment.\n\n[Weber's Law:]{.red} The detectable difference in stimulus intensity $I$ as a fixed percentage $K$ of the object magnitude:  $dI/I=K$ .  \n\n\n![](reljudge.png)\n\nThe filled rectangles differ in length by 1:9, and it is therefore difficult to detect the difference without aligment.  The white rectangles differ in length by 1:2, it is easier to see this difference even when the objects are unaligned.\n\n## RELATIVE LUMINANCE JUDGEMENTS\n\nHuman perception of luminance is completely contextual, and is based on contrast with surrounding colors.\n\n[![](luminance1.jpg){.absolute width=\"450\" bottom=\"100\" left=\"0\"}]{.fragment}\n\n[![](luminance2.jpg){.absolute width=\"450\" bottom=\"100\" right=\"0\"}]{.fragment}\n\n## RELATIVE COLOR JUDGEMENTS {.smaller}\n\nOur visual system evolved to provide color constancy so that the same surface is identifiable across a broad set of illumination conditions, even though a physical light meter would yield very different readings. While the visual system works very well in natural environments, many of its mechanisms work against simple approaches to visually encoding information with color.\n\n:::: {.columns}\n::: {.column width=40%}\n![](colorjudge.png)\n\n![](colorjudge2.png)\n:::\n\n::: {.column width=60%}\n**Figure 5.15 shows two colorful cubes.** In Figure 5.15(a) corresponding squares both appear to be red. In Figure 5.15(b), masks show that the tile color in the image apparently illuminated by a yellowish light source is actually orange, and for the bluish light the tiles are actually purple. \n:::\n::::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}