[
  {
    "objectID": "Teaching.html",
    "href": "Teaching.html",
    "title": "TEACHING",
    "section": "",
    "text": "1 credit, Fall semester only\nThe companion laboratory to Biology 310. This is an open lab course in which students test genetic hypotheses using Drosophila melanogaster or game based simulations.\nCanvas Page\n\n\n\n3 credits, Spring semester, asynchronous online\n\nThis class will help students establish a foundational understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R programming language. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using R. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\nCanvas Page\nCourse WebPage"
  },
  {
    "objectID": "Research.html",
    "href": "Research.html",
    "title": "RESEARCH",
    "section": "",
    "text": "My research group is an interdisciplinary blend of biologists, developers, modelers, programmers, and artists. I am always interested in recruiting new undergraduates, graduate students, and postdocs. Contact me via email if you are interested in learning more."
  },
  {
    "objectID": "Research.html#evolutionary-video-games",
    "href": "Research.html#evolutionary-video-games",
    "title": "RESEARCH",
    "section": "Evolutionary Video Games",
    "text": "Evolutionary Video Games\nIn 2015, Terry Soule and I co-founded Polymorphic Games, an experimental game studio that makes evolution based video games. Our original concept was that a model of biological evolution would make a great game mechanic. We observed that traditional video games were usually scripted, featuring “waves” of enemies with defined and predictable characteristics. A player’s success in such games is based on learning the predictable, rote script necessary to advance to subsequent levels. By integrating principles of evolutionary biology, we suspected that video games could be made more compelling. For example, by replacing the traditional waves of enemies with generations of enemies we could introduce adaptation through natural selection. If you want to learn more about our approach, scroll down to the section on Video Games and STEM Education.\nSince 2015 we have commercially released two evolutionary video games on Steam, and developed many other interactive simulations and outreach exhibits.\n\nCommercial Releases\n\nDarwin’s Demons is an arcade style space shooter in which you battle a population of evolving aliens. The alien’s morphology, fire type, and behaviors are all encoded by a digital genome. The aliens that are best able to shoot or evade you reproduce to make the next generation.\n\nProject Hastur is an evolutionary tower defense game. Defend humanity against the ever changing threat of the alien Proteans. Project Hastur combines elements of 3D tower defense and real-time strategy with models of biological evolution to create a uniquely challenging experience."
  },
  {
    "objectID": "Research.html#video-games-and-stem-education",
    "href": "Research.html#video-games-and-stem-education",
    "title": "RESEARCH",
    "section": "Video Games and STEM Education",
    "text": "Video Games and STEM Education\n\nAdding Evolution to Video Games\nThe best way to explain the concept is to frame our video games in the context of the “ingredients” of evolution:\n\nVariation\nInheritance\nSelection\nTime\n\nWhen these components are present, it leads to Adaptation.\n\nVariation\nThink about the enemies in the old school video games that I used to play in arcades. Space Invaders, Asteroids, Centipede, Tempest, etc. What kinds of variation can you describe in these enemies? In Space Invaders, you had three types of aliens in each wave, plus the occasional bonus flying saucer. In Centipede and Tempest, you actually had categorically different enemy types that varied in how they looked and in their capabilities. Below you can see a few examples of what I mean:\n\nSpace InvadersCentipedeTempest\n\n\n Enemy variation in Space Invaders includes the shape of each alien type (four types), but also their position in the armada (each type has its own row) and their speed of movement (which is adjusted based on how many remain in each wave). For now, we will ignore the flying saucer that passes across the top of the screen. The game increases in difficulty by having the movement speed of each wave increase. Ironically, the increased movement speed of the aliens within a wave as the population size was reduced was an unintended function of the increased frame rate achieved by having fewer aliens draw calls.\n\n\n Centipede includes enemy types with very different behaviors and abilities. The segmented centipede can split into component parts. Fleas drop vertically, leave a trail of mushrooms, and take two hits to destroy. Spiders move in a diagonal zig zag pattern. Scorpions move horizontally across the screen and turn mushrooms poisonous. Each wave the game difficulty increases by changing the behavior and segment number of the centipede.\n\n\n While not as famous as Space Invaders or Centipede, Tempest was one of my favorite games in the early 80s. The player navigated their ship across various geometric shapes, shooting at different types of enemies with different capabilities. These included player capturing Flippers, lane electrifying Pulsars, and obstacle creating Spikers. So many quarters gone…\n\n\n\nThis variation is a big part of what makes video games exciting. The difficulty in video games is usually increased over time by introducing new variations of enemies, changing the enemies’ capabilities (like speed or fire rate) over time, or increasing the number of enemies. The problem (as we see it) is that these variations are introduced as part of a script that is programmed and balanced by the game developers. Beating the game means memorizing the script.\n\n\nInheritance\nThe addition of an inheritance model is the primary differentiator in our games that creates adaptation.\nIn most games, there is no relationship between enemies within a wave or between waves. They are instantiated (spawned) with developer defined traits at a specified rate, location, and time (or level or wave). This is the key difference in the games that our lab creates. In our games, the enemies traits are specified by digital genomes that they pass on to their offspring through a form of reproduction. If the enemy has a speed value of 4, that means that when it reproduces, its offspring inherit that speed value of 4. Most of the games we work on specify about 40 to 80 “genes” in the enemy genome, each of which can influence one or more game traits such as speed, fire rate, morphology, behavior, health, armor, resistances, etc.\n\n\nSelection\nSelection occurs when there is a correlation between a trait (say… hit points or movement behavior) and Fitness. In biological populations, Fitness is usually defined in terms of the number of offspring produced by a given individual. We often measure proxies for fitness in empirical biology that are typically predictive of (or at least correlated with ) Fitness, such as seed set in plants, survivorship, or number of eggs. In our games, we can make explicit linkages between performance and fitness using Fitness Functions.\nFor example, in Darwin’s Demons the enemies accrue Aggression by moving downward on the screen. The closer they get to the bottom, the higher their Aggression score. if they move past the player and off the bottom of the screen, their Aggression score gets an even larger boost. After each wave, the enemies with the highest Aggression score also have the highest probability of mating and producing offspring. Thus, we have created an explicit link in which Aggression is correlated with Fitness (number of offspring). Our games often have more than one of these Fitness Functions. For example, the Protean Swarm in Darwin’s Demons can also increase their mating chance with Accuracy (which measures how often they shoot at the player and how close they came to hitting the player) and Lifetime (which measures how long they last on the screen) .\nIf we turn off the fitness functions, then after each wave the enemies undergo random mating. Adaptive evolution does not occur, but the enemy traits might still evolve randomly because of Random Genetic Drift.\n\n\nTime\nThe concept of time in video games is often defined in terms of waves or levels. Both of these terms imply a progression toward the game’s goal, along with a corresponding increase in difficulty. In our games, we usually specify time in terms of generations. A wave of enemies begin the game, the player defeats them, and then the next wave is created with Inheritance using a mating function specified as described above. As the generations (waves) proceed, the enemies with traits that are best able to optimize the fitness functions have more offspring, and the population adapts to the game play conditions. Most of these conditions are created by player choices and playstyle, and thus the enemies adapt to the player. Difficulty increases organically and repeated gameplay often creates novel adaptive solutions to the same play style.\n\n\n\nEducational Outcomes\nWe reasoned that playing these types of games might have implications for STEM education. The success of the player is based on her comprehension and application of principles of evolutionary biology. The parallels to real world examples are numerous, and include the rapid evolution of antibiotic resistance in microbial pathogens, adaptation of crop pests to chemical and biological control measures, and behavioral adaptation to captivity in domesticated animals.\nAdding biological evolution to video games makes the games better for the game player and facilitates player comprehension of complex concepts that are hard to teach."
  },
  {
    "objectID": "Research.html#get-involved",
    "href": "Research.html#get-involved",
    "title": "RESEARCH",
    "section": "Get Involved",
    "text": "Get Involved\nIf you are interested in joining the lab and working in the studio, reach out to me by email."
  },
  {
    "objectID": "Research.html#behavioral-genomics-and-adaptation-to-captivity",
    "href": "Research.html#behavioral-genomics-and-adaptation-to-captivity",
    "title": "RESEARCH",
    "section": "Behavioral Genomics and Adaptation to Captivity",
    "text": "Behavioral Genomics and Adaptation to Captivity\n The process of adaptation to captivity can cause striking changes in a variety of behavioral phenotypes, including aggressive, feeding, and reproductive behaviors. However, some of the most profound and consistent evolutionary changes occur in behaviors related to fearfulness and anxiety, with domesticated animals often described as less fearful than their wild progenitor populations. The evolution of reduced fearfulness during captivity has been observed in a variety of vertebrate taxa, including mammals (Harri et al., 2003), birds (Jensen & Andersson, 2005), and fish (Johnsson & Abrahams, 1991; Robison & Rowland, 2005). Despite the prevalence of behavioral evolution during adaptation to captivity, we know surprisingly little about the underlying genetic changes that occur during domestication. What kinds of molecular variation (amino acid sequence, regulatory, or non coding RNA) are associated with variation in fear and anxiety related behaviors? What kinds of genes are typically changed in response to domestication selection? Are the apparently parallel changes in behavior across independent domestication events reflective of parallel molecular evolution, or does domestication result from myriad molecular routes to the same phenotypic endpoint? The answers to these questions are important in many contexts, including the study of the molecular mechanisms of behavioral evolution, the genetic basis of complex behaviors, and the effective conservation of captively reared species.\nWe use the zebrafish to study the genetic basis of fear related behaviors that commonly change during domestication in fish. The zebrafish is an excellent system for this purpose, combining a robust genomics toolset with superior experimental tractability. Our lab has documented variation among wild and laboratory derived strains consistent with the effects of domestication. The behavioral variation among wild and domesticated strains can be seen in the video on this page. This video shows two tanks of zebrafish randomly pulled from our zebrafish colony. One tank contains a highly domesticated (&gt;30 generations) strain (Scientific Hatcheries), and the other tank contains a strain with a more recent history (~5 generations) of domestication (Nadia). We have quantified surface orientation (Robison and Rowland 2005, Benner et al 2010), observer orientation (Benner et al 2010), feeding latency (Oswald and Robison 2008), and open field behavior (Oswald and Robison in review) in these strains, and shown that they vary markedly in these anxiety related behaviors.\nWe use a variety of genomics approaches to understand the mechanisms that drive behavioral adaptation to captivity. We have compared the brain transcriptomes of wild and domesticated strains using both microarray platforms and Illumina and 454 based RNAseq. We have discovered many promising candidate genes that appear to differentiate behaviorally bold and behaviorally shy animals. We have also used RNAseq approaches to identify more than 20,000 SNPs that vary among our zebrafish strains. We are currently using these SNPs for quantitative trait locus (QTL) analyses.\nWe also combine these genomics approaches with quantitative genetics experiments. Mary Oswald, a recent Ph.D. graduate from our lab, has performed selection experiments on observer orientation. She has shown that these fear related behaviors have a significant genetic basis and are genetically correlated. Her selection experiment is now being continued by Matt Singer, who is a current Ph.D."
  },
  {
    "objectID": "Research.html#quantitative-genetics",
    "href": "Research.html#quantitative-genetics",
    "title": "RESEARCH",
    "section": "Quantitative Genetics",
    "text": "Quantitative Genetics"
  },
  {
    "objectID": "BCB520/posts/T6-APIsandJSON/index.html",
    "href": "BCB520/posts/T6-APIsandJSON/index.html",
    "title": "TUTORIAL 6 - APIs and JSON data",
    "section": "",
    "text": "library(tidyr)\nlibrary(dplyr)\nlibrary(repurrrsive)\nlibrary(tibblify)\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(rjson)\nlibrary(jsonlite)\nlibrary(ggplot2)\n\n\nBio&lt;-fromJSON(\"draft.json\")\n\nnoway&lt;-as.data.frame(Bio)\n\nplayers&lt;- noway %&gt;%\n  select(data.draftYear, data.playerId, data.playerName, data.draftYear,\n         data.roundNumber, data.overallPickNumber, data.pickInRound,\n         data.height, data.weight, data.position)\n\ncleanplayers &lt;- drop_na(players)\n\n\napi_url &lt;- \"https://statsapi.web.nhl.com\"\n\n\nrecentdrafts&lt;-cleanplayers%&gt;%\n  filter(data.draftYear&gt;1999, data.draftYear&lt;2018)\nrecentdrafts$playerId&lt;-recentdrafts$data.playerId\nplayerIds &lt;- as.list(recentdrafts$data.playerId)\n\ntestloop&lt;-data.frame()\n\nfor (player_id in playerIds){\n  \n  \n\nendpoint &lt;- paste0(\"/api/v1/people/\", player_id, \"/stats?stats=yearByYear\")\n\n\nurl_json &lt;- paste0(api_url,endpoint)\n\nraw_json &lt;- url_json %&gt;% \n  httr::GET() %&gt;% \n  httr::content()\n\nargh &lt;- tibblify(raw_json$stats[[1]]$splits)\n\nplayerstats&lt;- unnest_wider(argh, col=c(stat,league))\nplayerstats$playerId&lt;-player_id\nplayerstatsfinal&lt;-playerstats %&gt;%\n  filter(name==\"National Hockey League\") %&gt;%\n  select(season, games, \n         name, playerId) \n  \ntestloop&lt;-rbind(testloop,playerstatsfinal)\n}\n\nargh2 &lt;- tibblify(glorp$stats$splits[[1]])\n\nd2000on&lt;-full_join(testloop,recentdrafts, by=\"playerId\")\n\n\ndataIwant &lt;- rbind(d2000on, d2018, d2019, d2020, d2020on)\nwrite.csv(dataIwant, file=\"NHLdraftgames.csv\")"
  },
  {
    "objectID": "BCB520/posts/T8-MidtermExample/index.html",
    "href": "BCB520/posts/T8-MidtermExample/index.html",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)\n\n\nZOMBIES.\nZombies have long been a frequent trope of horror fiction. The concept of zombies first emerged from Haitian folklore and Voodoo, where they were depicted as reanimated corpses that were controlled by a sorcerer or bokor. However, in modern fiction, the magical or supernatural origin of zombies has been largely replaced by a scientific one. Zombies are now typically portrayed as mindless, flesh-eating creatures that are reanimated by a virus or some other infectious agent.\nThe evolution of zombies as a horror trope can be divided into several distinct eras:\n\nClassic Zombies: The classic zombie was the original Haitian zombie, which was introduced to Western audiences in the early 20th century through literature and film. These zombies were depicted as slow-moving, mind-controlled creatures that were raised from the dead by Voodoo magic.\nRomero Zombies: George A. Romero’s 1968 film “Night of the Living Dead” redefined the zombie genre by introducing the idea that zombies were reanimated by a mysterious virus that spread through bites and scratches. Romero’s zombies were slow-moving, flesh-eating creatures that could only be killed by destroying the brain. Many modern zombie franchises (e.g. Resident Evi, Walking Dead) still use the classic shambling undead Romeero Zombie concept.\nFast Zombies: In the early 2000s, a new type of zombie emerged in fiction that could run and move at incredible speeds. These fast zombies were popularized by films like 28 Days Later and World War Z. Fast zombies are often depicted as being more aggressive and intelligent than their slow-moving counterparts. Note that in many of these zombie franchises, the zombies were still created by an infectious agent such as a virus.\nPost-Apocalyptic Zombies: In recent years, zombies have been featured in a number of post-apocalyptic settings, where they are often portrayed as the cause of a global pandemic that has devastated humanity. These stories often focus on the struggle of survivors to rebuild civilization in a world overrun by the undead. These can be any of the above types (Classic, Romero, Fast) of zombie. This is the scenario that motivates this Blog post.\n\nWould an outbreak of a “zombie virus” actually consume the world and bring forth an apocalyptic new age of shambling horror?\nLet’s use disease modeling to find out!"
  },
  {
    "objectID": "BCB520/posts/T8-MidtermExample/index.html#data",
    "href": "BCB520/posts/T8-MidtermExample/index.html#data",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "Data",
    "text": "Data\nI’m going to use our new interactive simulation, OUTBREAK SIMULATOR, to understand the dynamics of a zombie virus outbreak.\n\nParameters\nIn order to model this outbreak, we’ll need to set some of the classic parameters of an SIR compartment model. Outbreak Simulator uses a compartment model of disease (Weissman et al., 2020) in which the population is divided into categories (Figure 1): Susceptible (S), Exposed (E), Infected (I), Vaccinated (V), Recovered (R), or Deceased (D). The model estimates the rates of exchange between categories over a given time interval (t) using differential equations. When the model parameters are known and key assumptions are met, the differential equations can estimate the epidemic curve of an outbreak. The two most critical assumptions are that the population is homogeneous and well mixed and is fixed in size.\n\nI’ve provided my own estimates of these parameters for various infectious zombie franchises in the table below.\n\n\nCode\nparams&lt;-read_xlsx(\"params.xlsx\")\n\nknitr::kable(params)\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDefinition\nResident Evil\nWorld War Z\nWalking Dead\n\n\n\n\nBETA\nRate of transmission\n19.8\n198\n20\n\n\ncontact rate\nNumber of infectious contacts (bites) per hour\n20\n200\n20\n\n\ninfection probability\nChance that a bite causes infection\n0.99\n0.99\n1\n\n\nALPHA\nTransformation rate\n0.5\n1000\n2.0833333333333332E-2\n\n\nlatency\naverage number of hours until transformation to Infected\n2\n1E-3\n48\n\n\nGAMMA\nRecovery rate\n0\n0\n0\n\n\nDELTA\nMortality rate\n1\n1\n1\n\n\ninfection duration\naverage lifespan of a zombie in hours\n250\n250\n250\n\n\nSIGMA\nVaccination rate\n0\n0\n0\n\n\nR0 Initial\nInitial basic reproduction rate of the virus\n4950\n49500\n5000\n\n\nSpeed\nHow fast can the zombies move?\nSlow\nFast\nSlow\n\n\n\n\n\n\n\nData Dictionary\nOutbreak Simulator runs the compartment model on a spatially explicit grid of the continental US. It writes the number of individuals in each compartment (SIERD) at each time step (an hour) for each of the 48 states and the total population.\nThis is a video of the simulation:\nVideo\n\n\nCode\ndf&lt;-read.csv(\"ZombieData.csv\")\n\n\nI’ll manipulate the raw data a bit to get to the visualizations I need. First, I want a tidy data set with only the total US population.\n\n\nCode\ndf&lt;-df%&gt;%\n  mutate_at(c(1:295), as.numeric)\n\ndftotal &lt;- df%&gt;%\n  select(Time, starts_with(\"Totals_\"))%&gt;%\n  rename(Time=Time,\n          S=Totals_S,\n         E=Totals_E,\n         V=Totals_V,\n         I=Totals_I,\n         R=Totals_R,\n         D=Totals_D)\n\nfor (i in 1:length(dftotal$Time)){\n  \n  dftotal$dS[i] &lt;- dftotal$S[i]-dftotal$S[i+1]\n  dftotal$dR[i] &lt;- dftotal$R[i+1]-dftotal$R[i]\n  dftotal$dI[i] &lt;- dftotal$I[i+1]-dftotal$I[i]\n  dftotal$dD[i] &lt;- dftotal$D[i+1]-dftotal$D[i]\n\n}\n\ndftotal&lt;- dftotal%&gt;%\n  mutate(N= S+E+I+V+R+D)%&gt;%\n  mutate(Beta = dS*N/(S*I+1))%&gt;%\n  mutate(Gamma = dD/(I+1))%&gt;%\n  mutate(R0 = Beta/Gamma)%&gt;%\n  filter(Beta&lt;10)%&gt;%\n  filter(R0&lt;10^3)\n\ndflong&lt;-dftotal%&gt;%\n  pivot_longer(cols = c(\"S\", \"E\", \"I\",\n                        \"V\", \"R\", \"D\"),names_to = \"Compartment\", values_to = \"Count\")\n\ndflong &lt;- dflong%&gt;%\n  mutate(Compartment = recode(Compartment, \n         S = \"Susceptible\",\n         E = \"Exposed\",\n         I = \"Infected\",\n         V = \"Vaccinated\",\n         R = \"Recovered\",\n         D = \"Deceased\"))%&gt;%\n  filter(Compartment != \"Vaccinated\")%&gt;%\n  filter(Compartment != \"Recovered\")\n\n\nThis allows me to produce the classic Epidemilogical Curve:\n\n\nCode\nggplot(dflong, aes(x=Time, y = Count/10^6, color = Compartment))+\n  geom_line()+\n   labs(\n    title = \"Epidemiological Curve\",\n    subtitle = \"Resident Evil - T Virus\",\n    caption = \"Data from a single run of Outbreak Simulator.\",\n    tag = \"Figure 1\",\n    x = \"Time (h)\",\n    y = \"Number of Individuals (millions)\",\n    colour = \"Compartment\"\n  )+\n   scale_colour_brewer(type = \"seq\", palette = \"Spectral\")"
  },
  {
    "objectID": "BCB520/posts/T8-MidtermExample/index.html#state-level-dynamics",
    "href": "BCB520/posts/T8-MidtermExample/index.html#state-level-dynamics",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "STATE LEVEL DYNAMICS",
    "text": "STATE LEVEL DYNAMICS\nWe can use the output from the simulation to show the Epidemilogical Curve for each state, but the problem is the number of states. There are 48 in the continental US - too many to ask a user to meaningfully process.\nHere are the curves for four states - the ones that collaborate on the Tickbase project that funds this work.\n\n\nCode\nstateslong &lt;- df %&gt;%\n  pivot_longer(cols = 2:295,\n               names_to = c(\"State\", \"Compartment\"),\n  names_pattern = \"(.+?)_(.)\",\n  values_to = \"Count\")\n\nfewerstates&lt;-stateslong%&gt;%\n  filter(State == \"New.Mexico\" | State == \"Idaho\"\n         | State ==\"Nevada\" | State == \"New.Hampshire\"\n         )%&gt;%\n    mutate(Compartment = recode(Compartment, \n         S = \"Susceptible\",\n         E = \"Exposed\",\n         I = \"Infected\",\n         V = \"Vaccinated\",\n         R = \"Recovered\",\n         D = \"Deceased\"))%&gt;%\n  filter(Compartment != \"Vaccinated\")%&gt;%\n  filter(Compartment != \"Recovered\")\n\n\nggplot(fewerstates, aes(x=Time, y = Count/10^6, color = Compartment))+\n  geom_line()+\n  facet_wrap(~State)+\n   labs(\n    title = \"Epidemiological Curve\",\n    subtitle = \"H1N1 Influenza\",\n    caption = \"Data from a single run of Outbreak Simulator.\",\n    tag = \"Figure 2\",\n    x = \"Time (h)\",\n    y = \"Number of Individuals (millions)\",\n    colour = \"Compartment\"\n  )+\n   scale_colour_brewer(type = \"seq\", palette = \"Spectral\")\n\n\n\n\n\nCode\nstatecolor&lt;-stateslong%&gt;%\n  filter(Compartment == \"S\" & State != \"Totals\" & Time == 0)%&gt;%\n  mutate(rank = rank(Count))\n\nstateslong2 &lt;- left_join(stateslong, statecolor, by = c(\"State\",\n                                                        \"Compartment\"))\n\nZomstates &lt;- stateslong2 %&gt;%\n  filter(Compartment == \"S\" & State != \"Totals\")%&gt;%\n  mutate(date = as_date(Time.x),\n         name = State,\n         category = rank,\n         value = Count.x)%&gt;%\n  select(c(8:11))\n\n\n\n\n\nwrite.csv(Zomstates, \"Zomstates.csv\")"
  },
  {
    "objectID": "BCB520/posts/T8-MidtermExample/index.html#visualizations",
    "href": "BCB520/posts/T8-MidtermExample/index.html#visualizations",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "VISUALIZATIONS",
    "text": "VISUALIZATIONS\nI’m really interested in an animated visualization that captures the changing population dynamics in each state. I’m going to use Observable for this, modifying an existing workbook.\n\n\nCode\ndata = d3.csvParse(await FileAttachment(\"Zomstates.csv\").text(), d3.autoType)\n\nviewof replay = html`&lt;button&gt;Replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nchart = {\n  \n  replay;\n\n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, height]);\n\n  const updateBars = bars(svg);\n  const updateAxis = axis(svg);\n  const updateLabels = labels(svg);\n  const updateTicker = ticker(svg);\n\n  yield svg.node();\n\n  for (const keyframe of keyframes) {\n    const transition = svg.transition()\n        .duration(duration)\n        .ease(d3.easeLinear);\n\n    // Extract the top bar’s value.\n    x.domain([0, keyframe[1][0].value]);\n\n    updateAxis(keyframe, transition);\n    updateBars(keyframe, transition);\n    updateLabels(keyframe, transition);\n    updateTicker(keyframe, transition);\n\n    invalidation.then(() =&gt; svg.interrupt());\n    await transition.end();\n  }\n}\n\n\nduration = 25\nn = 50\nk = 10\nnames = new Set(data.map(d =&gt; d.name))\n\n\ndatevalues = Array.from(d3.rollup(data, ([d]) =&gt; d.value, d =&gt; +d.date, d =&gt; d.name))\n  .map(([date, data]) =&gt; [new Date(date), data])\n  .sort(([a], [b]) =&gt; d3.ascending(a, b))\n  \n  \nfunction rank(value) {\n  const data = Array.from(names, name =&gt; ({name, value: value(name)}));\n  data.sort((a, b) =&gt; d3.descending(a.value, b.value));\n  for (let i = 0; i &lt; data.length; ++i) data[i].rank = Math.min(n, i);\n  return data;\n}\n\nkeyframes = {\n  const keyframes = [];\n  let ka, a, kb, b;\n  for ([[ka, a], [kb, b]] of d3.pairs(datevalues)) {\n    for (let i = 0; i &lt; k; ++i) {\n      const t = i / k;\n      keyframes.push([\n        new Date(ka * (1 - t) + kb * t),\n        rank(name =&gt; (a.get(name) || 0) * (1 - t) + (b.get(name) || 0) * t)\n      ]);\n    }\n  }\n  keyframes.push([new Date(kb), rank(name =&gt; b.get(name) || 0)]);\n  return keyframes;\n}\n\nnameframes = d3.groups(keyframes.flatMap(([, data]) =&gt; data), d =&gt; d.name)\n\nprev = new Map(nameframes.flatMap(([, data]) =&gt; d3.pairs(data, (a, b) =&gt; [b, a])))\n\nnext = new Map(nameframes.flatMap(([, data]) =&gt; d3.pairs(data)))\n\nfunction bars(svg) {\n  let bar = svg.append(\"g\")\n      .attr(\"fill-opacity\", 0.6)\n    .selectAll(\"rect\");\n\n  return ([date, data], transition) =&gt; bar = bar\n    .data(data.slice(0, n), d =&gt; d.name)\n    .join(\n      enter =&gt; enter.append(\"rect\")\n        .attr(\"fill\", color)\n        .attr(\"height\", y.bandwidth())\n        .attr(\"x\", x(0))\n        .attr(\"y\", d =&gt; y((prev.get(d) || d).rank))\n        .attr(\"width\", d =&gt; x((prev.get(d) || d).value) - x(0)),\n      update =&gt; update,\n      exit =&gt; exit.transition(transition).remove()\n        .attr(\"y\", d =&gt; y((next.get(d) || d).rank))\n        .attr(\"width\", d =&gt; x((next.get(d) || d).value) - x(0))\n    )\n    .call(bar =&gt; bar.transition(transition)\n      .attr(\"y\", d =&gt; y(d.rank))\n      .attr(\"width\", d =&gt; x(d.value) - x(0)));\n}\n\n\nfunction labels(svg) {\n  let label = svg.append(\"g\")\n      .style(\"font\", \"bold 12px var(--sans-serif)\")\n      .style(\"font-variant-numeric\", \"tabular-nums\")\n      .attr(\"text-anchor\", \"end\")\n    .selectAll(\"text\");\n\n  return ([date, data], transition) =&gt; label = label\n    .data(data.slice(0, n), d =&gt; d.name)\n    .join(\n      enter =&gt; enter.append(\"text\")\n        .attr(\"transform\", d =&gt; `translate(${x((prev.get(d) || d).value)},${y((prev.get(d) || d).rank)})`)\n        .attr(\"y\", y.bandwidth() / 2)\n        .attr(\"x\", -6)\n        .attr(\"dy\", \"-0.25em\")\n        .text(d =&gt; d.name)\n        .call(text =&gt; text.append(\"tspan\")\n          .attr(\"fill-opacity\", 0.7)\n          .attr(\"font-weight\", \"normal\")\n          .attr(\"x\", -6)\n          .attr(\"dy\", \"1.15em\")),\n      update =&gt; update,\n      exit =&gt; exit.transition(transition).remove()\n        .attr(\"transform\", d =&gt; `translate(${x((next.get(d) || d).value)},${y((next.get(d) || d).rank)})`)\n        .call(g =&gt; g.select(\"tspan\").tween(\"text\", d =&gt; textTween(d.value, (next.get(d) || d).value)))\n    )\n    .call(bar =&gt; bar.transition(transition)\n      .attr(\"transform\", d =&gt; `translate(${x(d.value)},${y(d.rank)})`)\n      .call(g =&gt; g.select(\"tspan\").tween(\"text\", d =&gt; textTween((prev.get(d) || d).value, d.value))))\n}\n\nfunction textTween(a, b) {\n  const i = d3.interpolateNumber(a, b);\n  return function(t) {\n    this.textContent = formatNumber(i(t));\n  };\n}\n\nformatNumber = d3.format(\",d\")\n\nfunction axis(svg) {\n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(0,${margin.top})`);\n\n  const axis = d3.axisTop(x)\n      .ticks(width / 160)\n      .tickSizeOuter(0)\n      .tickSizeInner(-barSize * (n + y.padding()));\n\n  return (_, transition) =&gt; {\n    g.transition(transition).call(axis);\n    g.select(\".tick:first-of-type text\").remove();\n    g.selectAll(\".tick:not(:first-of-type) line\").attr(\"stroke\", \"white\");\n    g.select(\".domain\").remove();\n  };\n}\n\nfunction ticker(svg) {\n  const now = svg.append(\"text\")\n      .style(\"font\", `bold ${barSize}px var(--sans-serif)`)\n      .style(\"font-variant-numeric\", \"tabular-nums\")\n      .attr(\"text-anchor\", \"end\")\n      .attr(\"x\", width - 6)\n      .attr(\"y\", margin.top + barSize * (n - 0.45))\n      .attr(\"dy\", \"0.32em\")\n      .text(formatDate(keyframes[0][0]));\n\n  return ([date], transition) =&gt; {\n    transition.end().then(() =&gt; now.text(formatDate(date)));\n  };\n}\n\nformatDate = d3.utcFormat(\"%Y\")\n\ncolor = {\n  const scale = d3.scaleSequential(d3.interpolate(\"red\", \"blue\")).domain([1, 48]);\n  if (data.some(d =&gt; d.category !== undefined)) {\n    const categoryByName = new Map(data.map(d =&gt; [d.name, d.category]))\n    scale.domain(Array.from(categoryByName.values()));\n    return d =&gt; scale(categoryByName.get(d.name));\n  }\n  return d =&gt; scale(d.name);\n}\n\n\n&lt;!-- color = { --&gt;\n&lt;!--   const scale = d3.scaleSequential(d3.interpolate(\"red\", \"blue\")).domain([1, 48]); --&gt;\n&lt;!--   if (data.some(d =&gt; d.category !== undefined)) { --&gt;\n&lt;!--     const categoryByName = new Map(data.map(d =&gt; [d.name, d.category])); --&gt;\n&lt;!--     const categories = Array.from(categoryByName.values()).filter((d, i, arr) =&gt; arr.indexOf(d) === i); --&gt;\n&lt;!--     const scaleByCategory = typeof categories[0] === \"number\" ?  --&gt;\n&lt;!--       d3.scaleSequential(d3.interpolateSpectral).domain(d3.extent(categories)) : --&gt;\n&lt;!--       d3.scaleOrdinal().domain(categories).range(d3.quantize(d3.interpolateSpectral, categories.length)); --&gt;\n&lt;!--     return d =&gt; scale(scaleByCategory(categoryByName.get(d.name))); --&gt;\n&lt;!--   } --&gt;\n&lt;!--   return (d, i) =&gt; scale(i); --&gt;\n&lt;!-- } --&gt;\n\nx = d3.scaleLinear([0, 1], [margin.left, width - margin.right])\n\ny = d3.scaleBand()\n    .domain(d3.range(n + 1))\n    .rangeRound([margin.top, margin.top + barSize * (n + 1 + 0.1)])\n    .padding(0.1)\n    \nheight = margin.top + barSize * n + margin.bottom\n\nbarSize = 48\n\nmargin = ({top: 16, right: 6, bottom: 6, left: 0})\n\nd3 = require(\"d3@6\")"
  },
  {
    "objectID": "BCB520/posts/T8-MidtermExample/index.html#summary",
    "href": "BCB520/posts/T8-MidtermExample/index.html#summary",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "SUMMARY",
    "text": "SUMMARY\nIt is pretty clear from the simulations that an outbreak of the T-Virus is really bad news for the continental United States. We go from ~308 million people to ~10 million people in about 500 hours. In addition, the most populated states at the beginning of the outbreak are hit disproportionately hard, presumably because their population density helps sustain the infection."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#last-lecture",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#last-lecture",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "LAST LECTURE",
    "text": "LAST LECTURE\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#what",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#what",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\n Before you design a visualization, you need to understand the data. Here, we consider the semantics to describe the DATA TYPES and DATA ATTRIBUTES."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#the-three-major-data-types",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#the-three-major-data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "THE THREE MAJOR DATA TYPES",
    "text": "THE THREE MAJOR DATA TYPES"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#data-attributes",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#data-attributes",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA ATTRIBUTES",
    "text": "DATA ATTRIBUTES"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#what-1",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#what-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat does this sequence of six numbers mean?\n14, 2.6, 30, 30, 15, 100001 Two points far from each other in 3D space?\n\n\nVIZ"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics-1",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat does this sequence of six numbers mean?\n14, 2.6, 30, 30, 15, 100001 Two points close to each other in 2D space, with 15 links between them, and a weight of 100001 for the link?\n\n\nVIZ"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics-2",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat about this data?\nBasil, 7, S, Pear:\nFood shipment of produce (basil & pear) arrived in satisfactory condition on 7th day of month\n\n\nVIZ"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics-3",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics-3",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat about this data?\nBasil, 7, S, Pear:\nBasil Point neighborhood of city had 7 inches of snow cleared by the Pear Creek Limited snow removal service\n\n\nVIZ"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics-4",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#data-semantics-4",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat about this data?\nBasil, 7, S, Pear:\nLab rat Basil made 7 attempts to find way through south section of maze, these trials used pear as reward food\n\n\nVIZ"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#semantics",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#semantics",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SEMANTICS",
    "text": "SEMANTICS\nThe meaning of a word, phrase, sentence, or text.\nBasil, 7, S, Pear"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#semantics-for-data",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#semantics-for-data",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SEMANTICS FOR DATA",
    "text": "SEMANTICS FOR DATA\n\n\n\nSemantics\n\nitem: individual entity, discrete\n\neg patient, car, stock, city\n“independent variable”\n\nattribute: property that is measured, observed, logged…\n\neg height, blood pressure for patient\neg horsepower, make for car\n“dependent variable”\n\n\n\n\nData Table\n\n\n\n\nITEM: Person\nATTRIBUTES: Name, Age, Shirt Size, Favorite Fruit"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#other-data-types",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#other-data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "OTHER DATA TYPES",
    "text": "OTHER DATA TYPES\n\nLinks\n\nexpress relationship between two items\ne.g/ friendship on facebook, interaction between proteins\n\nPositions\n\nspatial data: location in 2D or 3D\ne.g. pixels in photo, voxels in MRI scan, latitude/longitude\n\nGrids\n\nsampling strategy for continuous data"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#what-2",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#what-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#dataset-types-tables",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#dataset-types-tables",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATASET TYPES: TABLES",
    "text": "DATASET TYPES: TABLES\nFlat Table\n\n\n\nOne ITEM per row\n\noften called an observation\n\nEach column is an ATTRIBUTE\n\noften called a variable\n\nA cell holds the VALUE for an item/attribute pair\nA UNIQUE key can be used (implicitly or explicitly) to identify each item even if they share all measured attributes"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#flat-table-example",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#flat-table-example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "FLAT TABLE EXAMPLE",
    "text": "FLAT TABLE EXAMPLE"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#multidimensional-tables",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#multidimensional-tables",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MULTIDIMENSIONAL TABLES",
    "text": "MULTIDIMENSIONAL TABLES\nindexing based on multiple keys (eg genes, patients)"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#networks",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#networks",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "NETWORKS",
    "text": "NETWORKS\nNetwork/graph nodes (vertices) connected by links (edges). A tree is special case: no cycles, often have roots, and are directed."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#fields",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#fields",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "FIELDS",
    "text": "FIELDS"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#spatial-fields-1",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#spatial-fields-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 1",
    "text": "SPATIAL FIELDS 1\nAttribute values associated w/ cells cell contains value from continuous domain (eg temperature, pressure, wind velocity measured or simulated)."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#spatial-fields-2",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#spatial-fields-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 2",
    "text": "SPATIAL FIELDS 2\n\n\n\n\nAttribute values associated w/ cells.\nCell contains value from continuous domain\n\n(eg temperature, pressure, wind velocity)\n\nMeasured or simulated.\nMajor concerns\n\nsampling:\n\nwhere attributes are measured\n\ninterpolation:\n\nhow to model attributes elsewhere\n\ngrid types"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#spatial-fields-3",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#spatial-fields-3",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 3",
    "text": "SPATIAL FIELDS 3\n\n\n\nAttribute values associated w/ cells.\nCell contains value from continuous domain\n\n(eg temperature, pressure, wind velocity)\n\nMeasured or simulated.\nMajor concerns\n\nsampling:\n\nwhere attributes are measured\n\ninterpolation:\n\nhow to model attributes elsewhere\n\ngrid types\n\nMajor divisions - attributes per cell:\n\nscalar (1)\nvector (2)\ntensor (many)"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#geometry",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#geometry",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "GEOMETRY",
    "text": "GEOMETRY"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#geometry-2",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#geometry-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "GEOMETRY 2",
    "text": "GEOMETRY 2\n\n\n\nShape of items\nExplicit spatial positions / regions\n\npoints, lines, curves, surfaces, volumes\n\nBoundary between computer graphics and visualization\n\ngraphics: geometry taken as given\nvis: geometry is result of a design decision"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#collections",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#collections",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "COLLECTIONS",
    "text": "COLLECTIONS"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#collections-2",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#collections-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "COLLECTIONS 2",
    "text": "COLLECTIONS 2\n\n\nGrouping Items:\n\nsets\n\nunique items\nunordered\n\nlists\n\nordered\nduplicates possible\n\nclusters\n\ngroups of similar items"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#data-types",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA TYPES",
    "text": "DATA TYPES"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#attribute-types",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#attribute-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "ATTRIBUTE TYPES",
    "text": "ATTRIBUTE TYPES\n\n\n\nCategorical (nominal):\n\ncompare equality\nno implicit ordering\n\nOrdered:\n\nordinal\nless/greater than defined\n\nQuantitative:\n\nmeaningful magnitude\narithmetic possible"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#example",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "EXAMPLE",
    "text": "EXAMPLE\nCategorical\nOrdinal\nQuantitative"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#example-1",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#example-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "EXAMPLE",
    "text": "EXAMPLE\nCategorical\nOrdinal\nQuantitative"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#additional-semantic-components",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#additional-semantic-components",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "ADDITIONAL SEMANTIC COMPONENTS",
    "text": "ADDITIONAL SEMANTIC COMPONENTS"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#data-abstraction",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#data-abstraction",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA ABSTRACTION",
    "text": "DATA ABSTRACTION\n\nGOAL: Translate from domain-specific language to generic (and consistent) visualization language.\n\nIdentify dataset type(s) and attribute types.\nIdentify cardinality.\n\nhow many items in the dataset?\nwhat is cardinality of each attribute?\n\nnumber of levels for categorical data?\nrange for quantitative data\n\n\nConsider whether to transform the data.\n\nguided by your understanding of the task."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#models-data-vs-conceptual",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#models-data-vs-conceptual",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MODELS: DATA VS CONCEPTUAL",
    "text": "MODELS: DATA VS CONCEPTUAL\n\n\nData Model\n\nmathematical abstraction\nsets with operations, eg floats with * / - + variable data types in programming languages\n\nConceptual Model\n\nmental construction (semantics)\nsupports reasoning\ntypically based on understanding of tasks\n\nThe Data Abstraction process relies on conceptual model for transforming data if needed"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#models-example",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#models-example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MODELS: EXAMPLE",
    "text": "MODELS: EXAMPLE\n\nData Model: floats\n\n32.52, 54.06, -14.35,\n\nConceptual Model:\n\ntemperature\n\nPossible data abstractions:\n\nQUANTITATIVE: continuous to 2 significant figures:\n\nTASK: Forecasting the weather\n\nORDINAL: Hot, Warm, Cold:\n\nTASK: Deciding if my bath water is ready\n\nCATEGORICAL: Above Freezing, Below Freezing:\n\nTASK: Deciding if I should leave the house today"
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#derived-attributes",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#derived-attributes",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DERIVED ATTRIBUTES",
    "text": "DERIVED ATTRIBUTES\nDerived attribute: Data computed from original (collected, observed) attributes."
  },
  {
    "objectID": "BCB520/posts/L2-Data-Abstraction/index.html#summary",
    "href": "BCB520/posts/L2-Data-Abstraction/index.html#summary",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SUMMARY",
    "text": "SUMMARY\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "BCB520/posts/T4-BarriesData/index.html",
    "href": "BCB520/posts/T4-BarriesData/index.html",
    "title": "TUTORIAL 4",
    "section": "",
    "text": "In this assignment (detalied here), I will identify, import, describe, and host a data set that will be used throughout the remainder of the BCB 504 course for Data Visualizations.\n\n\nI’ve chosen a subset of a large dataset produced by our evolutionary video game, Project Hastur. We built Project Hatur to be an evolutionary video game, and we are bold in our assertions of that fact. But we haven’t really published any evidence that the evolutionary model works. This data set is the beginning of that exercise.\n\n\n\n\n\n\nNote\n\n\n\nPROJECT HASTUR creates a unique challenge by combining elements of 3D tower defense and real-time strategy with biological evolution. Fight against alien Proteans that evolve - using biologically accurate models of evolution - to overcome the player’s defenses.\nEach creature you will face has its own unique genome controlling its abilities, behaviors, and appearance. Those that make it the furthest and do the most damage to your defenses have the most offspring you will have to defeat in the next generation. The result? Evolution responds to the player’s strategy and makes every playthrough a unique experience.\nUse four upgradable turret classes, plus airstrikes and combat robots, to fight against the Protean invasion. Make strategic decisions about which turrets to build, when to upgrade them, and where to place them on the hex grid. A well-timed airstrike can change the flow of the game, but you’ll have to wait before you can use it again. Unlock powerful upgrades for each turret class as you move across the Nyx system. As you play, the Proteans evolve new weapon resistances, behaviors, and movement capabilities to better destroy your defenses.\nIn CAMPAIGN MODE, battle through a series of maps as a military defense commander to protect the planet Nyx from the ever-evolving threat of the Proteans. Unlock weapons and upgrades and use them to fight against the Protean swarm and learn about the mysteries of Project Hastur.\nIn EXPERIMENT MODE, choose any map, tweak the parameters, and play infinitely to see what you can evolve. Change the number of creatures and the parameters of evolution, make your turrets invincible, or crank up the biomatter and experiment with the most powerful turret upgrades. Experiment mode lets you experience Project Hastur your way.\n\n\n\n\nThe data were collected by running Project Hastur in Experiment mode using four predefined conditions:\nI: The CHIP SHREDDER towers when Fitness Functions were turned ON and Civilians were PRESENT.\nH: The CHIP SHREDDER towers when Fitness Functions were turned OFF and Civilians were PRESENT.\nG: The CHIP SHREDDER towers when Fitness Functions were turned ON and Civilians were ABSENT.\nK: The AUTOCANNON towers when Fitness Functions were turned ON and Civilians were ABSENT.\nEach experimental condition was run 9 times (9 replicates).\n\n\n\n\nI’m going to use the vroom package to import multiple files. Each file is a replicate and the filename tells us about the experimental condition. Below I convert the filename variable (I named it path) into a a single categorical attribute called Fit that uses the letter codes above.\n\n\nCode\nlibrary(vroom)\nlibrary(stringr)\nlibrary(tidyverse)\nlibrary(readxl)\nfiles &lt;- fs::dir_ls(glob = \"*.csv\")\n\nHastur &lt;- vroom(files, id = \"path\", \n                col_select = c(path, Generation, ID, Origin, AsexualReproduction, Fitness, Health,\n                               SightRange, Armor, Damage, WalkSpeed, RunSpeed, Acceleration, \n                               TurnRate, Attraction0, Attraction1, Attraction2))\n\nHastur$Fit &lt;- str_split_i(Hastur$path, pattern = \"\", 1)\nHastur$replicate &lt;- str_split_i(Hastur$path, pattern = \"\", 4)\n\n\nThe glimpse command in the Tidyverse package is a nice way to summarize the data frame:\n\n\nCode\nglimpse(Hastur)\n\n\nRows: 412,246\nColumns: 19\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n$ path                &lt;chr&gt; \"GSC1.csv\", \"GSC1.csv\", \"GSC1.csv\", \"GSC1.csv\", \"G…\n$ Generation          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ID                  &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ Origin              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ AsexualReproduction &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Fitness             &lt;dbl&gt; 57.83508, 66.87755, 66.14652, 65.88873, 62.12119, …\n$ Health              &lt;dbl&gt; 1006, 1012, 1011, 992, 983, 1020, 982, 963, 996, 9…\n$ SightRange          &lt;dbl&gt; 9.952521, 10.096590, 9.954091, 10.066170, 10.02955…\n$ Armor               &lt;dbl&gt; 0.05081077, 0.05080924, 0.05010696, 0.04903501, 0.…\n$ Damage              &lt;dbl&gt; 49, 51, 51, 50, 50, 51, 50, 49, 51, 50, 49, 50, 49…\n$ WalkSpeed           &lt;dbl&gt; 6.930266, 7.034348, 6.970608, 6.903729, 6.962081, …\n$ RunSpeed            &lt;dbl&gt; 20.03562, 19.88800, 19.80754, 19.94738, 19.95583, …\n$ Acceleration        &lt;dbl&gt; 14.70648, 15.05868, 14.85994, 14.89853, 15.01570, …\n$ TurnRate            &lt;dbl&gt; 356.3890, 361.2032, 358.9919, 361.8476, 360.6143, …\n$ Attraction0         &lt;dbl&gt; 0.158477500, -0.007134318, -0.063494000, 0.0125864…\n$ Attraction1         &lt;dbl&gt; -0.070695680, 0.059872570, -0.003332689, 0.0458469…\n$ Attraction2         &lt;dbl&gt; -0.108705200, 0.015018780, -0.007600136, 0.0120656…\n$ Fit                 &lt;chr&gt; \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", …\n$ replicate           &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\n\n\n\n\n\nWhat we have here is a (big) Flat Table. The Items are the rows, and each row is an individual alien enemy that existed during one of the replicates. Each Item (alien) is described by Attributes, which are arranged in the columns.\n\n\n\nThe glimpse we did in the preceding section gives us a hint as to what each attribute type might be. Let’s flesh that out a bit though. I’m going to create a new data frame that describes the attributes.\n\n\nCode\nAttributes &lt;- read_excel(\"Attributes.xlsx\")\nknitr::kable(Attributes)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nNote\n\n\n\n\npath\nCategorical\neach File Name is a unique replicate\n\n\nGeneration\nQuantitative\nEach Enemy Wave is a Generation\n\n\nID\nOrdinal\nEach enemy has a unique ID within each replicate\n\n\nOrigin\nCategorical\nThe hive from which the enemy was spawned\n\n\nAsexualReproduction\nCategorical\nWas the enemy spawned by infectiing a civilian?\n\n\nFitness\nQuantitative\nThe value of Fitness is used to determine probability of reproduction\n\n\nHealth\nQuantitative\nHit Points\n\n\nSightRange\nQuantitative\nHow far they can see civilians, towers, etc\n\n\nArmor\nQuantitative\nresistance to physical damage\n\n\nDamage\nQuantitative\nhow much damage they do to towers\n\n\nWalkSpeed\nQuantitative\nhow fast they can walk\n\n\nRunSpeed\nQuantitative\nhow fast they can run\n\n\nAcceleration\nQuantitative\nhow fast they can transition from walking to running\n\n\nTurnRate\nQuantitative\nhow fast they can turn\n\n\nAttraction0\nQuantitative\nnegative values is attraction to civilians, positive is avoidance\n\n\nAttraction1\nQuantitative\nnegative values is attraction to towers, positive is avoidance\n\n\nAttraction2\nQuantitative\nnegative values is attraction to the base, positive is avoidance\n\n\nFit\nCategorical\nI, H, G, or K - an inscrutable code about fitness conditions\n\n\n\n\n\nThe problem here is my inscrutable filename codes for that Fit variable. Those letter codes actually contain information on a couple hidden variables. I’m going to create a new variable called Gun and another called Civilians. I’ll add those to the main data file and also the Data Dicttionary.\n\n\nCode\nHastur$Gun &lt;- \"CHIP SHREDDER\"\nHastur$Civilians &lt;- \"Present\"\n  \n\n  Hastur$Gun[Hastur$Fit==\"K\"]&lt;- \"AUTOCANNON\"\n     \n  Hastur$Civilians[Hastur$Fit==\"K\" | Hastur$Fit ==\"G\"] &lt;- \"ABSENT\"\n     \n\n  Attributes&lt;-rbind(Attributes, c(\"Gun\",\"Categorical\", \"Autocannon or Chip Shredder\"))\n  Attributes&lt;-rbind(Attributes, c(\"Civilians\",\"Categorical\", \"Present or Absent\"))\n\n\n\n\nCode\n  knitr::kable(Attributes)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nNote\n\n\n\n\npath\nCategorical\neach File Name is a unique replicate\n\n\nGeneration\nQuantitative\nEach Enemy Wave is a Generation\n\n\nID\nOrdinal\nEach enemy has a unique ID within each replicate\n\n\nOrigin\nCategorical\nThe hive from which the enemy was spawned\n\n\nAsexualReproduction\nCategorical\nWas the enemy spawned by infectiing a civilian?\n\n\nFitness\nQuantitative\nThe value of Fitness is used to determine probability of reproduction\n\n\nHealth\nQuantitative\nHit Points\n\n\nSightRange\nQuantitative\nHow far they can see civilians, towers, etc\n\n\nArmor\nQuantitative\nresistance to physical damage\n\n\nDamage\nQuantitative\nhow much damage they do to towers\n\n\nWalkSpeed\nQuantitative\nhow fast they can walk\n\n\nRunSpeed\nQuantitative\nhow fast they can run\n\n\nAcceleration\nQuantitative\nhow fast they can transition from walking to running\n\n\nTurnRate\nQuantitative\nhow fast they can turn\n\n\nAttraction0\nQuantitative\nnegative values is attraction to civilians, positive is avoidance\n\n\nAttraction1\nQuantitative\nnegative values is attraction to towers, positive is avoidance\n\n\nAttraction2\nQuantitative\nnegative values is attraction to the base, positive is avoidance\n\n\nFit\nCategorical\nI, H, G, or K - an inscrutable code about fitness conditions\n\n\nGun\nCategorical\nAutocannon or Chip Shredder\n\n\nCivilians\nCategorical\nPresent or Absent\n\n\n\n\n\n\n\n\n\nI’m publishing to GitHub! We will elaborate on this step as everyone progresses through the assignment.\n\n\n\nFor this data set, I am currently defining the user as … me! My hypothesis is that the two Fitness conditions create different evolutionary outcomes of the aliens in Project Hastur. Some relevant ACTION TARGET pairs might be:\nDISCOVER TRENDS\nDISCOVER DISTRIBUTION\nDISCOVER SIMILARITY\nCOMPARE TRENDS\nCOMPARE DISTRIBUTION\nI’m going to try COMPARE TRENDS. I want to COMPARE the TREND in Health over time (Generation) between the two Gun types. To do this, I’ll create a scatterplot, faceted by Gun. I’m suspicious that Acceleration is involved somehow, so I’m coloring with that variable.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Health))+\n  geom_point(aes(color=Acceleration), alpha = 0.01, size = 1)+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_grid(replicate~Gun)\n\n\n\n\n\nInteresting… it looks like a clear trend for Health to increase under the withering fire of the AUTO CANNONS, but not when the player uses the CHIP SHREDDER. It is a bit hard to see what is going on with Acceleration, so let’s reverse the graphs so that we plot Acceleration on the y axis but color by Health.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Acceleration))+\n  geom_point(aes(color=Health), alpha = 0.01, size = 1)+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_grid(replicate~Gun)\n\n\n\n\n\nI’m now confident that the replicates within each Gun type are pretty similar, and I can SUMMARIZE the individual data points. This will help with the COMPARE TRENDS task, I think.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Health))+\n  geom_point(aes(color=Acceleration), alpha = 0.01, size = 1)+\n  geom_smooth()+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_wrap(~Gun)\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Acceleration))+\n  geom_point(aes(color=Health), alpha = 0.01, size = 1)+\n  geom_smooth()+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_wrap(~Gun)\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#plan",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#plan",
    "title": "LECTURE 4",
    "section": "PLAN",
    "text": "PLAN\n\nAssignment 3 Review.\nAligning the VAD model with other frameworks.\nEncoding with Marks and Channels."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#assignment-3-review",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#assignment-3-review",
    "title": "LECTURE 4",
    "section": "ASSIGNMENT 3 REVIEW",
    "text": "ASSIGNMENT 3 REVIEW\nRondald’s Portfolio\nJiyin’s Portfolio\nCody’s Portfolio\nErik’s Portfolio"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#vad-model",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#vad-model",
    "title": "LECTURE 4",
    "section": "VAD MODEL",
    "text": "VAD MODEL"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#understand-the-data",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#understand-the-data",
    "title": "LECTURE 4",
    "section": "UNDERSTAND THE DATA",
    "text": "UNDERSTAND THE DATA\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#understand-the-task",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#understand-the-task",
    "title": "LECTURE 4",
    "section": "UNDERSTAND THE TASK",
    "text": "UNDERSTAND THE TASK\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#visual-encoding",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#visual-encoding",
    "title": "LECTURE 4",
    "section": "VISUAL ENCODING",
    "text": "VISUAL ENCODING\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#other-frameworks",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#other-frameworks",
    "title": "LECTURE 4",
    "section": "OTHER FRAMEWORKS",
    "text": "OTHER FRAMEWORKS\n\nThe Tidyverse\nThe Grammar of Graphics\nTufte"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#tidyverse",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#tidyverse",
    "title": "LECTURE 4",
    "section": "TIDYVERSE",
    "text": "TIDYVERSE\nR packages for data science:\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. The best way to explore and understand the tidyverse is with cheetsheets, like this one for tidyr!"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#grammar-of-graphics",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#grammar-of-graphics",
    "title": "LECTURE 4",
    "section": "GRAMMAR OF GRAPHICS",
    "text": "GRAMMAR OF GRAPHICS\nThe ggplot2 cheatsheet!"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#tufte",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#tufte",
    "title": "LECTURE 4",
    "section": "TUFTE",
    "text": "TUFTE\nTufte’s Website\nA Quarto Page Layout Example"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#analysis-framework",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#analysis-framework",
    "title": "LECTURE 4",
    "section": "ANALYSIS FRAMEWORK",
    "text": "ANALYSIS FRAMEWORK\nFour levels, three questions\n\n\n\nDomain situation defines the target users.\nAbstraction translate from specifics of domain to vocabulary of vis\n\nWHAT is shown? data abstraction\nWHY is the user looking at it? task abstraction\n\nIdiom defines the visualization\n\nHOW is it shown?\n\nvisual encoding idiom: how to draw\ninteraction idiom: how to manipulate\n\n\nAlgorithm creates the visualization\n\nevaluated with computational efficiency"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#encoding",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#encoding",
    "title": "LECTURE 4",
    "section": "ENCODING",
    "text": "ENCODING\nWe are defining the structure of the visualization (the idiom).\nTo do this, we use MARKS and CHANNELS:\n\nMARKS represent ITEMS or LINKS\nCHANNELS change the appearance of MARKS based on ATTRIBUTES"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#marks-for-items",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#marks-for-items",
    "title": "LECTURE 4",
    "section": "MARKS FOR ITEMS",
    "text": "MARKS FOR ITEMS"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#marks-for-links",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#marks-for-links",
    "title": "LECTURE 4",
    "section": "MARKS FOR LINKS",
    "text": "MARKS FOR LINKS\n\n Bubblesets\n Force Directed Graph"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#observable-in-quarto",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#observable-in-quarto",
    "title": "LECTURE 4",
    "section": "OBSERVABLE IN QUARTO!",
    "text": "OBSERVABLE IN QUARTO!\n\n\nCode\nd3 = require(\"d3@7\")\n\n\nchart = ForceGraph(miserables, {\n  nodeId: d =&gt; d.id,\n  nodeGroup: d =&gt; d.group,\n  nodeTitle: d =&gt; `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l =&gt; Math.sqrt(l.value),\n  width,\n  height: 1000,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\nmiserables = FileAttachment(\"miserables.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d =&gt; d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) =&gt; source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) =&gt; target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 1000, // outer width, in pixels\n  height = 1000, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) =&gt; N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) =&gt; ({id: N[i]}));\n  links = d3.map(links, (_, i) =&gt; ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) =&gt; N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) =&gt; W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) =&gt; L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) =&gt; color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) =&gt; T[i]);\n  if (invalidation != null) invalidation.then(() =&gt; simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d =&gt; d.source.x)\n      .attr(\"y1\", d =&gt; d.source.y)\n      .attr(\"x2\", d =&gt; d.target.x)\n      .attr(\"y2\", d =&gt; d.target.y);\n\n    node\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\""
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#channels",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#channels",
    "title": "LECTURE 4",
    "section": "CHANNELS",
    "text": "CHANNELS\n\n\n\nCHANNELS control the appearance of MARKS.\n\nThey are proportional to or based on ATTRIBUTES.\nTheir properties differ in the type and amount of information that can be conveyed to the human perceptual system."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#visual-encoding-example",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#visual-encoding-example",
    "title": "LECTURE 4",
    "section": "VISUAL ENCODING EXAMPLE",
    "text": "VISUAL ENCODING EXAMPLE\nLet’s analyze the idiom structures below in terms of marks and channels."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#redundant-encoding",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#redundant-encoding",
    "title": "LECTURE 4",
    "section": "REDUNDANT ENCODING",
    "text": "REDUNDANT ENCODING\nUses multiple channels for the same attribute.\n\nSends a stronger message\nUses up channels"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#choosing-channels",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#choosing-channels",
    "title": "LECTURE 4",
    "section": "CHOOSING CHANNELS",
    "text": "CHOOSING CHANNELS\n\nEXPRESSIVENESS\n\nMatch channel to data type.\n\nEFFECTIVENESS\n\nChannels differ in accuracy of perception."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#channel-rankings",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#channel-rankings",
    "title": "LECTURE 4",
    "section": "CHANNEL RANKINGS",
    "text": "CHANNEL RANKINGS\n\nNote that spatial position ranks high for both types of channels."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#grouping",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#grouping",
    "title": "LECTURE 4",
    "section": "GROUPING",
    "text": "GROUPING\n\n\n\nContainment\nConnection\nProximity\n\nSame spatial region.\n\nSimilarity\n\nSame values as other channels."
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#summary-so-far",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#summary-so-far",
    "title": "LECTURE 4",
    "section": "SUMMARY SO FAR",
    "text": "SUMMARY SO FAR"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#channel-effectiveness",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#channel-effectiveness",
    "title": "LECTURE 4",
    "section": "CHANNEL EFFECTIVENESS",
    "text": "CHANNEL EFFECTIVENESS\n\nAccuracy: how precisely can we tell the difference between encoded items?\nDiscriminability: how many unique steps can we perceive?\nSeparability: is our ability to use this channel affected by another one?\nPopout: can things jump out using this channel?"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#accuracy-theory",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#accuracy-theory",
    "title": "LECTURE 4",
    "section": "ACCURACY (THEORY)",
    "text": "ACCURACY (THEORY)\nSteven’s Psychophisical Power Law: \\(S=I^N\\)\n\n\n\n\n\n\n\n\nLENGTH (N=1)\nELECTRIC SHOCK (N=3.5)\nSATURATION (N=1.7)\nAREA (N=0.7)\nBRIGHTNESS (N=0.5)"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#accuracy-experimental",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#accuracy-experimental",
    "title": "LECTURE 4",
    "section": "ACCURACY (EXPERIMENTAL)",
    "text": "ACCURACY (EXPERIMENTAL)"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#discriminability",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#discriminability",
    "title": "LECTURE 4",
    "section": "DISCRIMINABILITY",
    "text": "DISCRIMINABILITY\nHow many usable steps are in the channel?"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#separability-vs-integrality",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#separability-vs-integrality",
    "title": "LECTURE 4",
    "section": "SEPARABILITY VS INTEGRALITY",
    "text": "SEPARABILITY VS INTEGRALITY"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#popout",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#popout",
    "title": "LECTURE 4",
    "section": "POPOUT",
    "text": "POPOUT\n\n\nfind the red dot how long does it take? parallel processing on many individual channels speed independent of distractor count speed depends on channel and amount of difference from distractors serial search for (almost all) combinations speed depends on number of distractors"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#popout-1",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#popout-1",
    "title": "LECTURE 4",
    "section": "POPOUT",
    "text": "POPOUT\n\n\nmany channels tilt, size, shape, proximity, shadow direction, … but not all! parallel line pairs do not pop out from tilted pairs"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#factors-affecting-accuracy",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#factors-affecting-accuracy",
    "title": "LECTURE 4",
    "section": "FACTORS AFFECTING ACCURACY",
    "text": "FACTORS AFFECTING ACCURACY\nalignment distractors distance common scale / alignment"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#relative-vs-absolute-judgements",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#relative-vs-absolute-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE VS ABSOLUTE JUDGEMENTS",
    "text": "RELATIVE VS ABSOLUTE JUDGEMENTS\nperceptual system mostly operates with relative judgements, not absolute that’s why accuracy increases with common frame/scale and alignment\nWeber’s Law: ratio of increment to background is constant filled rectangles differ in length by 1:9, difficult judgement white rectangles differ in length by 1:2, easy judgement"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#relative-luminance-judgements",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#relative-luminance-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE LUMINANCE JUDGEMENTS",
    "text": "RELATIVE LUMINANCE JUDGEMENTS\nperception of luminance is contextual based on contrast with surroundings"
  },
  {
    "objectID": "BCB520/posts/L4-Marks-Channels/index.html#relative-color-judgements",
    "href": "BCB520/posts/L4-Marks-Channels/index.html#relative-color-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE COLOR JUDGEMENTS",
    "text": "RELATIVE COLOR JUDGEMENTS\ncolor constancy across broad range of illumination conditions\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "BCB520/posts/Certificate/index.html",
    "href": "BCB520/posts/Certificate/index.html",
    "title": "CERTIFICATE",
    "section": "",
    "text": "Learn how to think about, organize, analyze, and visualize data. Communicate data-driven insights to technical and lay audiences."
  },
  {
    "objectID": "BCB520/posts/Certificate/index.html#overview",
    "href": "BCB520/posts/Certificate/index.html#overview",
    "title": "CERTIFICATE",
    "section": "OVERVIEW",
    "text": "OVERVIEW\nWe live in an increasingly data-driven world. Basic data literacy and data science skills are becoming central to virtually every industry. Yet, limited opportunities exist to gain these skills without an advanced background in math and computer science. To address this workforce development need, we propose a competitively valued on-line graduate certificate in the Professional Applications in Data Science. The certificate is designed to offer rigorous training in the foundations of data science to anyone with a bachelor’s degree. Participants will learn how to think about, organize, analyze, and visualize data, and communicate data driven insights to diverse audiences. The curriculum emphasizes the use of data sets drawn from each student’s individual discipline, aligning the certificate’s workforce development impacts with the University of Idaho’s land grant mission.\n\nValue Proposition:\nThe graduate certificate in Professional Applications in Data Science will provide unique value to UI constituencies by:\n\nAligning data science training with fields of nascent demand that are part of our land grant mission, including Agriculture, Natural Resources, and Education.\nRequiring a digital data science portfolio with which students can demonstrate their proficiencies to potential employers.\nEmphasizing training in data communication - including verbal presentation and data visualization - two components of data science that are underrepresented in competing certificates.\nFilling a growing workforce development gap by offering a unique data science certificate that is appropriate for professionals with a bachelor’s degree who do not have a rigorous background in mathematics, statistics, or computer science.\n\n\n\nIntended Audience:\nThis certificate leverages the University of Idaho’s interdisciplinary culture to provide integrative training in the foundations of data science. It is intended for:\n\nWorking professionals with a bachelor’s degree whose career increasingly involves the generation, management, analysis, and visualization of large data sets. The certificate is appropriate for professionals in STEM fields, Health Care, Business, Government, Education, Journalism, Athletics, Natural Resources, and Agriculture.\nGraduate students in programs outside of the core technical disciplines of data science (statistics, math, engineering, or computer science). The certificate will complement disciplinary research methods courses with training in data management, visualization, and communication.\nUndergraduates at the UI who wish to incorporate data science training into their degree and graduate with a Bachelor’s degree and a graduate certificate.\n\n\n\nStudent Learning Outcomes:\nUpon completion of the certificate, students will be able to:\n\nUse open-source software to reproducibly manage, analyze, and visualize large, complex, and noisy data sets.\nPractice high quality and ethical data stewardship.\nUnderstand and execute data exploration.\nEffectively communicate data driven insights to experts and non-experts.\nDemonstrate their skills with an online portfolio of analyses and visualizations relevant to their field of specialization."
  },
  {
    "objectID": "BCB520/posts/Certificate/index.html#curriculum",
    "href": "BCB520/posts/Certificate/index.html#curriculum",
    "title": "CERTIFICATE",
    "section": "CURRICULUM",
    "text": "CURRICULUM\n\nPrerequisites:\nA Bachelor’s degree OR the student has senior standing and is enrolled in a bachelor’s degree program at the University of Idaho.\n\n\nCertificate Requirements (12 Credits Total)\n\n\n\n\n\n\n\n\n\n\n\n\nCourse\nName\nCredits\nPrerequisites\nSchedule\n\n\n\n\nINTR 509\nIntroduction to Applied Data Science\n3\nBS degree or permission\nSpring and asynchronous online\n\n\nBCB 551\nCommunicating with Data\n2\nINTR 509 or BS degree or permission\nFall and asynchronous online\n\n\nBCB 520\nData Visualization\n3\nSTAT 251 or INTR 509 or permission\nSpring and asynchronous online\n\n\nBCB 522\nData Science Portfolio\n1\nINTR 509 and BCB 520 (Data Viz)\nAsynchronous online\n\n\nElective\nVaries\n3\nVaries\nVaries\n\n\n\n\n\nnote: Courses designated with “BCB 5XX” are new courses that will be offered in the 2023-24 academic year\n\n\nCourse Descriptions\n\nINTR 509 Introduction to Applied Data Science (3 credits)\nIn person (spring) and asynchronous online.\nStudents are provided a foundation for “thinking with data” through the introduction of computational, statistical, and data literacy skills. This includes the selection, collection, cleaning, management, descriptive analysis, and exploratory analysis of a dataset unique to their professional interests using modern computing languages. This course is taught by Dr. Michael Overton.\n\n\nBCB 521 Communicating with Data (2 credits)\nIn person (fall) and asynchronous online.\nStudents are taught writing and presentation skills to improve their communication of data-driven insights to specialist and lay audiences. The course emphasizes reproducible research practices, including literate programming (R Markdown) and version control (GitHub). Course content includes the conceptual foundations of communicating with data along with written and verbal communication assignments using data sets individualized to each student’s interest.\nText: Nolan and Stoudt. 2021. Communicating with data: The art of writing for data science. Oxford University Press.\nPrerequisites: INTR 509 OR A BS degree OR permission.\n\n\nBCB 520 Data Visualization (3 credits)\nIn person (spring) and asynchronous online\nThis course covers the conceptual foundations of data visualization and design. Students will learn how visualization design choices related to marks and channels, color, and spatial arrangement interact with the human perceptual system. The course considers tabular, network, and spatial data, and students will implement visualizations in R.\nText: Munzner. 2014. Visualization Analysis & Design. CRC Press.\nPrerequisites: INTR 509 OR A BS degree OR Stat 251 OR Permission.\n\n\nBCB 522 Online Portfolio (1 credit)\nAsynchronous online\nThis course provides feedback, review, and approval of the student’s online data science portfolio. This portfolio is intended to represent the body of work accumulated by the student over the course of the certificate. It should contain examples of novel data products (such as FAIR data sets), analyses, and visualizations. All elements of the portfolio will be hosted online (likely in a GitHub repository or professional website), be open source, and demonstrate best practices of literate programming and reproducible research.\n\n\nElectives:\nThe certificate allows each student to customize their training by choosing a 3-credit graduate elective.\nFor students seeking foundational training who have not already taken Stat 431 or its equivalent, we recommend Stat 431 or a 3-credit graduate level disciplinary research methods course.\nFor students seeking to add the certificate to an existing degree at UI, or students who already have some advanced technical training, additional electives are possible. Note that many of these optional electives have substantial disciplinary pre-requisites. Not all electives are available in an online format.\n\n\nChoose one of the following:\n\n\n\n\n\nCourse\nName\nCredits\nPrerequisites\n\n\n\n\nAVS 531\nPractical Methods in Analyzing Animal Science Experiments\n3\n400-level statistics course\n\n\nBE 521\nImage Processing and Computer Vision\n3\n(BE 242 and MATH 275) or permission\n\n\nBE 541\nInstrumentation and Measurements\n3\nENGR 240; Coreqs: STAT 301\n\n\nBIOL 526\nSystems Biology\n3\n(BIOL 115, BIOL 115L and MATH 170) or permission of instructor\n\n\nBIOL 545\nPhylogenetics\n3\nPLSC 205 or BIOL 213 and BIOL 310\n\n\nBIOL 549\nComputer Skills for Biologists\n3\nBIOL 310 and STAT 251 or STAT 301; or Permission\n\n\nBIOL 563\nMathematical Genetics\n3\nMATH 160 or MATH 170 and STAT 251 or STAT 301\n\n\nCE 526\nAquatic Habitat Modeling\n3\nA minimum grade of ‘C’ or better is required for all pre/corequisites; Prereqs: CE 322 and CE 325 or BE 355; or Permission\n\n\nCE 579\nSimulation of Transportation Systems\n3\nPermission\n\n\nCS 511\nParallel Programming\n3\nCS 395\n\n\nCS 574\nDeep Learning\n3\n(CS 121 or MATH 330) and STAT 301\n\n\nCS 570\nArtificial Intelligence\n3\nCS 210\n\n\nCS 572\nEvolutionary Computation\n3\nCS 211\n\n\nCS 575\nMachine Learning\n3\nCS 210\n\n\nCS 577\nPython for Machine Learning\n3\n(CS 121 or MATH 330) and STAT 301\n\n\nCS 578\nNeural Network Design\n3\nPermission\n\n\nCS 579\nData Science\n3\nMATH 330 or Permission\n\n\nCS 589\nSemantic Web and Open Data\n3\nCS 360 or CS 479 or CS 579\n\n\nCTE 519\nDatabase Applications and Information Management\n3\nNA\n\n\nCYB 520\nDigital Forensics\n3\nCYB 310\n\n\nED 571\nIntroduction to Quantitative Research\n3\nGraduate standing\n\n\nED 584\nUnivariate Quantitative Research in Education\n3\nED 571\n\n\nED 587\nMultivariate Quantitative Analysis in Education\n3\nED 584 or Permission\n\n\nED 589\nTheoretical Applications and Designs of Qualitative Research\n3\nED 574 or Permission\n\n\nED 590\nData Analysis and Interpretation of Qualitative Research\n3\nED 574 and ED 589\n\n\nED 591\nIndigenous and Decolonizing Research Methods\n3\nNA\n\n\nED 592\nDecolonizing, Indigenous, and Action-Based Research Methods\n3\nNA\n\n\nED 595\nSurvey Design for Social Science Research\n3\nRecommended Preparation: Foundations of Research course at graduate level.\n\n\nEDAD 570\nMethods of Educational Research\n3\nNA\n\n\nENT 504\nApplied Bioinformatics\n3\nPermission\n\n\nENVS 511\nData Wizardry in Environmental Sciences\n3\nSTAT 251\n\n\nENVS 551\nResearch Methods in the Environmental Social Sciences\n3\nOne course or experience in basic statistics or Instructor Permission\n\n\nFOR 514\nForest Biometrics\n3\nSTAT 431 or equivalent\n\n\nFOR 535\nRemote Sensing of Fire\n3\nFOR 375 or permission\n\n\nGEOG 507\nSpatial Statistics and Modeling\n3\nSTAT 431 or permission\n\n\nGEOG 583\nRemote Sensing/GIS Integration\n3\nCoreqs: GEOG 385 or equivalent.\n\n\nMath 538\nStochastic Models\n3\nMATH 451 or Permission\n\n\nMIS 555\nData Management for Big Data\n3\nNA\n\n\nNRS 578\nLidar and optical remote sensing analysis using open-source software\n3\nSTAT251 & WLF370 or STAT427 and NRS/FOR 472 or equivalent/instructor permission\n\n\nPOLS 558\nResearch Methods for Local Government and Community Administration\n3\nSTAT 251\n\n\nREM 507\nLandscape and Habitat Dynamics\n3\nPermission; Recommended Preparation: courses in ecology, statistics, and GIS.\n\n\nStat 431\nStatistical Analysis\n3\nSTAT 251 or STAT 301\n\n\nSTAT 514\nNonparametric Statistics\n3\nSTAT 431\n\n\nSTAT 516\nApplied Regression Modeling\n3\nSTAT 431\n\n\nStat 517\nStatistical Learning and Predictive Modeling\n3\nSTAT 431\n\n\nStat 519\nMultivariate Analysis\n3\nSTAT 431 or equivalent.\n\n\nSTAT 535\nIntroduction to Bayesian Statistics\n3\nSTAT 431\n\n\nSTAT 555\nStatistical Ecology\n3\nMATH 451 or Permission\n\n\nStat 565\nComputer Intensive Methods\n3\n STAT 451, STAT 452, MATH 330, and computer programming experience or Permission\n\n\nWLF 552\nEcological Modeling\n3\nMATH 175 and FOR 221 or Permission.\n\n\nWLF 555\nStatistical Ecology\n3\nMATH 451 or permission\n\n\nWR 552\nWater Economics and Policy\n3\nAGEC 301 or AGEC 302, or ECON 351 or ECON 352, or by permission"
  },
  {
    "objectID": "BCB520/posts/Certificate/index.html#general-university-requirements",
    "href": "BCB520/posts/Certificate/index.html#general-university-requirements",
    "title": "CERTIFICATE",
    "section": "GENERAL UNIVERSITY REQUIREMENTS",
    "text": "GENERAL UNIVERSITY REQUIREMENTS\nIn addition to the requirements specified in this document, the certificate would be subject to all UI Policies regarding Graduate Certificates."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#who-am-i",
    "href": "BCB520/posts/L1-Intro/index.html#who-am-i",
    "title": "LECTURE 1 - INTRO",
    "section": "WHO AM I?",
    "text": "WHO AM I?\nBarrie Robison\nDepartment of Biological Sciences\nInstitute for Interdisicplinary Data Sciences\nPolymorphic Games\nUniversity of Idaho"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#who-are-you",
    "href": "BCB520/posts/L1-Intro/index.html#who-are-you",
    "title": "LECTURE 1 - INTRO",
    "section": "WHO ARE YOU?",
    "text": "WHO ARE YOU?\n“…The course is designed to be”discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests…”\nBriefly:\n\nYour name\nYour discipline\nYour degree progress\nYour technical proficiency with data visualization"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#course-summary",
    "href": "BCB520/posts/L1-Intro/index.html#course-summary",
    "title": "LECTURE 1 - INTRO",
    "section": "COURSE SUMMARY",
    "text": "COURSE SUMMARY\nStudents completing this course will be able to:\n\n\nDescribe and manipulate tabular, network, and spatial data; transform these data into a form suitable for visualization.\nMake effective data visualization design choices related to marks and channels, spatial arrangement, and components of color.\nDesign effective data visualizations for tabular, network, and spatial data with quantitative and categorical attributes.\nImplement their data visualization designs using existing tools in R (or other toolkits preferred by the student).\nExplain whether a visual encoding is perceptually appropriate for a specific combination of task and data.\nDemonstrate their skills with at least two novel visualizations suitable suitable for inclusion in an online Data Science Portfolio.\n\nThe course materials are located on Canvas and the course website."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#visualization",
    "href": "BCB520/posts/L1-Intro/index.html#visualization",
    "title": "LECTURE 1 - INTRO",
    "section": "VISUALIZATION",
    "text": "VISUALIZATION\nComputers provide visual representations of datasets designed to help people carry out tasks more effectively.\nTamara Munzner\nDepartment of Computer Science\nInfoVis Group\nUniversity of British Columbia"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#the-human",
    "href": "BCB520/posts/L1-Intro/index.html#the-human",
    "title": "LECTURE 1 - INTRO",
    "section": "THE HUMAN",
    "text": "THE HUMAN\nWhy have a human in the loop?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\nWe don’t need visualization when a trusted fully automatic solution exists.\nVisualization is suitable when there is a need to augment human capabilities rather than replace people with computational decision-making methods."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#when-to-visualize",
    "href": "BCB520/posts/L1-Intro/index.html#when-to-visualize",
    "title": "LECTURE 1 - INTRO",
    "section": "WHEN TO VISUALIZE",
    "text": "WHEN TO VISUALIZE\nVisualization is useful when:\n\n\nThe analysis problem is ill-specified and we don’t know exactly what questions to ask in advance.\nWe are interested in long-term use for end users (ex: exploratory analysis of scientific data).\nWe are presenting known results (ex: DATA JOURNALISM - New York Times Upshot).\nWe need a stepping stone to assess requirements before developing models.\nDevelopers of an automatic solution want to refine & determine parameters.\nWe need to help end users of automatic solutions verify and build trust."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#the-representation",
    "href": "BCB520/posts/L1-Intro/index.html#the-representation",
    "title": "LECTURE 1 - INTRO",
    "section": "THE REPRESENTATION",
    "text": "THE REPRESENTATION\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\nEXTERNAL REPRESENTATIONS: Replace cognition with perception."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#why-depend-on-vision",
    "href": "BCB520/posts/L1-Intro/index.html#why-depend-on-vision",
    "title": "LECTURE 1 - INTRO",
    "section": "WHY DEPEND ON VISION?",
    "text": "WHY DEPEND ON VISION?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\nThe human visual system is a high-bandwidth channel to the brain.\nOverview is possible due to background processing, providing the subjective experience of seeing everything simultaneously.\nSignificant processing occurs in parallel and pre-attentively.\nWhat about sound? lower bandwidth and different semantics, overview not supported, subjective experience of sequential stream.\nWhat about touch/haptics? impoverished record/replay capacity, only very low-bandwidth communication thus far.\nWhat about taste, smell? no viable record/replay devices."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#why-represent-all-the-data",
    "href": "BCB520/posts/L1-Intro/index.html#why-represent-all-the-data",
    "title": "LECTURE 1 - INTRO",
    "section": "WHY REPRESENT (ALL THE) DATA?",
    "text": "WHY REPRESENT (ALL THE) DATA?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\nsummaries lose information\ndetails matter\nconfirm expected and find unexpected patterns\nassess validity of statistical model\nANSCOMBE’S QUARTET is a fun example that we shall use to illustrate these points!"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#anscombes-quartet",
    "href": "BCB520/posts/L1-Intro/index.html#anscombes-quartet",
    "title": "LECTURE 1 - INTRO",
    "section": "ANSCOMBE’S QUARTET",
    "text": "ANSCOMBE’S QUARTET\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(datasets)\nlibrary(tidyverse)\nlibrary(dplyr)\ndatasets::anscombe\n\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\n\nAnscombe’s Quartet\nThe four x-y pairs have identical summary statistics.\n\n\nCode\ntidy_anscombe &lt;- anscombe %&gt;%\n pivot_longer(cols = everything(),\n              names_to = c(\".value\", \"set\"),\n              names_pattern = \"(.)(.)\")\ntidy_anscombe_summary &lt;- tidy_anscombe %&gt;%\n  group_by(set) %&gt;%\n  summarise(across(.cols = everything(),\n                   .fns = lst(min,max,median,mean,sd,var),\n                   .names = \"{col}_{fn}\"))\n#&gt; `summarise()` ungrouping output (override with `.groups` argument)\n\nvars&lt;-c(\"set\", \"x_mean\", \"x_var\",  \"y_mean\", \"y_var\")\nthing&lt;- as.data.frame(tidy_anscombe_summary[vars])\nknitr::kable(thing)\n\n\n\n\n\nset\nx_mean\nx_var\ny_mean\ny_var\n\n\n\n\n1\n9\n11\n7.500909\n4.127269\n\n\n2\n9\n11\n7.500909\n4.127629\n\n\n3\n9\n11\n7.500000\n4.122620\n\n\n4\n9\n11\n7.500909\n4.123249"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#viz-matters",
    "href": "BCB520/posts/L1-Intro/index.html#viz-matters",
    "title": "LECTURE 1 - INTRO",
    "section": "VIZ MATTERS",
    "text": "VIZ MATTERS\n\n\nCode\nggplot(tidy_anscombe,\n       aes(x = x,\n           y = y)) +\n  geom_point() +\n  geom_point(data = tidy_anscombe_summary, aes(x=x_mean, y = y_mean, color = \"red\", size = 5),\n             show.legend = FALSE)+\n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nLearn more: TIDY ANSCOMBE"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#resource-limitations",
    "href": "BCB520/posts/L1-Intro/index.html#resource-limitations",
    "title": "LECTURE 1 - INTRO",
    "section": "RESOURCE LIMITATIONS",
    "text": "RESOURCE LIMITATIONS\nVisualization designers must take into account three very different kinds of resource limitations:\n\nLimitations of computers.\nLimitations of humans.\nLimitations of displays."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#computational-limits",
    "href": "BCB520/posts/L1-Intro/index.html#computational-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "COMPUTATIONAL LIMITS",
    "text": "COMPUTATIONAL LIMITS\nCPU time\nSystem Memory"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#display-limits",
    "href": "BCB520/posts/L1-Intro/index.html#display-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "DISPLAY LIMITS",
    "text": "DISPLAY LIMITS\nPixels are precious and are the most constrained resource.\n\nInformation Density: ratio of space used to encode information vs unused whitespace.\nThere is a tradeoff between clutter and wasting space.\nDesigner must find the sweet spot between dense and sparse."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#human-limits",
    "href": "BCB520/posts/L1-Intro/index.html#human-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "HUMAN LIMITS",
    "text": "HUMAN LIMITS\n\nTime\nMemory\nAttention"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#visualization-analysis-and-design",
    "href": "BCB520/posts/L1-Intro/index.html#visualization-analysis-and-design",
    "title": "LECTURE 1 - INTRO",
    "section": "VISUALIZATION ANALYSIS AND DESIGN",
    "text": "VISUALIZATION ANALYSIS AND DESIGN\nANALYSIS?\n\nimposes structure on huge design space.\nscaffold to help you think systematically about choices.\nanalyzing existing designs serves as stepping stone to designing new visualizations.\nmost possibilities in a design space are ineffective for a particular task/data combination."
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#analysis-framework",
    "href": "BCB520/posts/L1-Intro/index.html#analysis-framework",
    "title": "LECTURE 1 - INTRO",
    "section": "ANALYSIS FRAMEWORK",
    "text": "ANALYSIS FRAMEWORK"
  },
  {
    "objectID": "BCB520/posts/L1-Intro/index.html#analysis-framework-example",
    "href": "BCB520/posts/L1-Intro/index.html#analysis-framework-example",
    "title": "LECTURE 1 - INTRO",
    "section": "ANALYSIS FRAMEWORK EXAMPLE",
    "text": "ANALYSIS FRAMEWORK EXAMPLE\nTree Juxtaposer\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "BCB520/posts/T3-Python/index.html",
    "href": "BCB520/posts/T3-Python/index.html",
    "title": "TUTORIAL # - PYTHON",
    "section": "",
    "text": "I relied heavily on the Quarto documentation for this tutorial.\nThe basic things I learned were:\n\nInstall Python (again). I downloaded the newest version from the Python website.\nInstall the `jupyter’ package using the terminal:\n\n\nWindows: py -m pip install jupyter\nMac: python3 -m pip install jupyter\n\n\nInstall the modules you need using the terminal (Mac examples below):\n\n\npython3 -m pip install numpy\npython3 -m pip install matplotlib\n\nAfter I did this, the code from the Quarto website example (below) seemed to work!"
  },
  {
    "objectID": "BCB520/posts/T3-Python/index.html#quarto-example-code",
    "href": "BCB520/posts/T3-Python/index.html#quarto-example-code",
    "title": "TUTORIAL # - PYTHON",
    "section": "QUARTO EXAMPLE CODE",
    "text": "QUARTO EXAMPLE CODE\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "BCB520/posts/A1-Lit-Prog/index.html",
    "href": "BCB520/posts/A1-Lit-Prog/index.html",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "",
    "text": "The idea of Literate Programming is that source code that is executed as part of the program’s purpose is interspersed with documentation that describes the program’s logic. The concept of literate programming was first articulated by David Knuth in 1984. You know… back when music was good? Modern Data Science leans pretty heavily on literate programming, and to be honest, there aren’t very many good arguments as to why you WOULDN’T want to implement this approach in your own work. Bearing this in mind, we will adopt this framework for most of the activities, exercises, and assignments in this course. All of us will benefit by practicing these skills."
  },
  {
    "objectID": "BCB520/posts/A1-Lit-Prog/index.html#literate-programming-publishing-systems",
    "href": "BCB520/posts/A1-Lit-Prog/index.html#literate-programming-publishing-systems",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "LITERATE PROGRAMMING PUBLISHING SYSTEMS",
    "text": "LITERATE PROGRAMMING PUBLISHING SYSTEMS\nI’m trying to keep this course as technology agnostic as I can. The idea is that you should be practicing and building competencies in the languages and algorithms that are most useful to you. Who am I to tell you to use R instead of Python? If you have skills in a particular language I encourage you to keep using that during this course. That being said, I am going to work the examples using R and R Studio, and I will (mostly) use Quarto as the literate programming framework.\nIf all of this is new to you, no problem. Just follow along in R and Quarto and start your skill building journey with those languages.\nIf you are a Python person, great! Quarto can accommodate that language as well. If you have another preference for literate programming, such as sticking with R Markdown until the Quarto bugs are fixed, that is great. Find the framework and tools that work for you, and practice, practice, practice!\n\nQuarto\nAn open source publishing system that allows you to create websites, documents, blogs, books, publications, presentations, and more while using R, Python, Julia, or Observable. Quarto is intended to be the more functional successor of R Markdown. I intend to use Quarto for most of my work in this course.\n\n\nR Markdown\nAnother publishing system for creating all the things … websites, slides, manuscripts, dashboards, etc. While most people (including me!) instinctively think of R and Python within R Markdown, the list of supported language engines is pretty extensive.\n\nnames(knitr::knit_engines$get())\n\n [1] \"awk\"       \"bash\"      \"coffee\"    \"gawk\"      \"groovy\"    \"haskell\"  \n [7] \"lein\"      \"mysql\"     \"node\"      \"octave\"    \"perl\"      \"php\"      \n[13] \"psql\"      \"Rscript\"   \"ruby\"      \"sas\"       \"scala\"     \"sed\"      \n[19] \"sh\"        \"stata\"     \"zsh\"       \"asis\"      \"asy\"       \"block\"    \n[25] \"block2\"    \"bslib\"     \"c\"         \"cat\"       \"cc\"        \"comment\"  \n[31] \"css\"       \"ditaa\"     \"dot\"       \"embed\"     \"eviews\"    \"exec\"     \n[37] \"fortran\"   \"fortran95\" \"go\"        \"highlight\" \"js\"        \"julia\"    \n[43] \"python\"    \"R\"         \"Rcpp\"      \"sass\"      \"scss\"      \"sql\"      \n[49] \"stan\"      \"targets\"   \"tikz\"      \"verbatim\"  \"ojs\"       \"mermaid\"  \n[55] \"include\""
  },
  {
    "objectID": "BCB520/posts/A1-Lit-Prog/index.html#languages-and-toolsets",
    "href": "BCB520/posts/A1-Lit-Prog/index.html#languages-and-toolsets",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "LANGUAGES AND TOOLSETS",
    "text": "LANGUAGES AND TOOLSETS\nThere are quite a few, but the five that seemed to keep coming up as I prepped this course are:\n\nR\nA very powerful open source framework for statistical computing and graphics. R has a lot of base functionality, and its capabilities are increased by 100 fold with packages created by R users. Packages are the core units of R code. I’m going to use R for the vast majority of demonstrations in this course.\n\n\nPython\nPython is an open source general purpose programming language. It wasn’t developed just for statistical computing or data science, and people use this language for tons of different applications. There is no denying it has become a very powerful language for data science and data visualization.\n\n\nTableau\nTableau is proprietary software that is very powerful for creating beautiful and functional data visualizations. It can integrate with all sorts of data sources and is used a lot for analytics, especially in the business world. The downsides (that occur to me at least) are that it costs money, it is not open source, and is more of a one-trick-pony than the programming languages on this list.\n\n\nJavascript\nJavascript has been around for about 25 years, and is (I think) the world’s most popular programming language. Along with HTML and CSS, Javascript drives pretty much the entire internet. I mention Javascript here because it has the D3 library, which can create super cool interactive data visualizaitons. In my experience, the learning curve with Javascript and D3 was pretty steep. I bought a book about it once, but just haven’t been able to allocate the amount of time necessary to really start using it. Check out the gallery of examples. Amazing!\n\n\nObservable / D3\nObservable is a set of extensions to Javascript that features something called reactive runtime. This means that the code blocks are executed and compiled as they are written, and changes are implemented instantaneously. Observable is pretty great for data exploration, and is well supported by Quarto. In addition, you can use the Observable JS libraries in Quarto to access D3. We’ll use some of these tools in this course, especially when we start considering interactivity."
  },
  {
    "objectID": "BCB520/posts/A1-Lit-Prog/index.html#assignment",
    "href": "BCB520/posts/A1-Lit-Prog/index.html#assignment",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nAfter that long introduction, I suppose you are wondering what I want you to actually DO today.\nWell, I want you to set up your publishing system and preferred language on your computer. Then I want you to recreate the classic figure from Anscombe’s Quartet.\nNow, you might be asking…\n“How am I supposed to do that? You haven’t taught me how to do anything yet!”\nHere is the dirty little secret of modern education.\nThe Internet Exists.\nWhile I could use up an entire 90 minute lecture telling you how to:\n\nDownload and install R, R-Studio, and Quarto (included by default with R-Studio).\nCreate a Quarto document that will publish in the .html format\nInstall the R packages you will need\nTidy up the Anscombe’s Quartet data\nCalculate the summary statistics for each x y pair\nMake a nice little plot…\n\nI’m not going to do that.\nInstead, I want you to use the resources I point towards, or other resources that make more sense to you, to figure out how to do those things."
  },
  {
    "objectID": "BCB520/posts/A1-Lit-Prog/index.html#resources",
    "href": "BCB520/posts/A1-Lit-Prog/index.html#resources",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "RESOURCES",
    "text": "RESOURCES\nTidyverse and Anscombe’s Quartet\nHandy cheat-sheets for many different R packages\nTutorial 1 - Literate Programming\nTutorial 2 - Literate Programming and Anscombe’s Quartet\nTutorial 3 - Python"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#last-lecture",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#last-lecture",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "LAST LECTURE",
    "text": "LAST LECTURE\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#task-abstraction",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#task-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TASK ABSTRACTION",
    "text": "TASK ABSTRACTION\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#from-domain-to-abstraction",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#from-domain-to-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "FROM DOMAIN TO ABSTRACTION",
    "text": "FROM DOMAIN TO ABSTRACTION"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#key-components-of-task-abstraction",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#key-components-of-task-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "KEY COMPONENTS OF TASK ABSTRACTION",
    "text": "KEY COMPONENTS OF TASK ABSTRACTION\n{action, target} pairs\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#actions-and-targets",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#actions-and-targets",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS AND TARGETS",
    "text": "ACTIONS AND TARGETS"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#actions---analyze",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#actions---analyze",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Analyze",
    "text": "ACTIONS - Analyze\n\n\n\nConsume: Information has already been generated and stored as data.\n\nDiscover: new knowledge, test hypothesis, generate new hypothesis, verify\nPresent: communicate something specific and already understood\nEnjoy: casual encounters with visualization\n\nProduce: generate new material or information\n\nAnnotate: addition of graphical or text to existing visualization elements\nRecord: saves or captures visualization elements as persistent artifacts (screenshots, lists, parameter sets, annotations)\nDerive: produce new data based on existing data (aka transform)"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#actions---search",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#actions---search",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Search",
    "text": "ACTIONS - Search\n\n\n\nLookup: Location and target both known\n\nExample: Look up humans in the Tree of Life, knowing they are mammals.\n\nLocate: Location unknown and target known\n\nExample: Look up rabbits in the Tree of Life, not knowing they are lagomorphs.\n\nBrowse: Location known and target unknown\n\nExample: Find any clades within Mammalia that have only one species.\n\nExplore: Location unknown and target unknown\n\nExample: Searching for anomalies in time series data."
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#actions---query",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#actions---query",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Query",
    "text": "ACTIONS - Query\n\n\n\nQuery: How much of the data matters to the task?\n\nIdentify: One (specific Item, individual, cell, etc)\nCompare: Some (multiple targets)\nSummarize: All (very common, aka Overview)"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#targets---all-data",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#targets---all-data",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - All Data",
    "text": "TARGETS - All Data"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#targets---attributes",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#targets---attributes",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - Attributes",
    "text": "TARGETS - Attributes"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#targets---other-data",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#targets---other-data",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - Other Data",
    "text": "TARGETS - Other Data"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#summary",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#summary",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "SUMMARY",
    "text": "SUMMARY\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#examples",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#examples",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "EXAMPLES!",
    "text": "EXAMPLES!\n\n\nCode\nlibrary(readxl)\nlibrary(vroom)\nlibrary(tidyverse)\nPerson &lt;- read_excel(\"Tasks.xlsx\", sheet = \"Person\",\n    col_names = TRUE)\nAction &lt;- read_excel(\"Tasks.xlsx\", sheet = \"Action\",\n    col_names = TRUE)\nTarget &lt;- read_excel(\"Tasks.xlsx\", sheet = \"Target\",\n    col_names = TRUE)\n\n\n\n\n\n\n\nPerson\n\n\n\n\nBarrie\n\n\nJiyin\n\n\nRonald\n\n\nCody\n\n\nEric\n\n\n\n\n\n\n\nAction\n\n\n\n\nDiscover\n\n\nPresent\n\n\nEnjoy\n\n\nAnnotate\n\n\nRecord\n\n\nDerive\n\n\nLookup\n\n\nBrowse\n\n\nLocate\n\n\nExplore\n\n\nIdentify\n\n\nCompare\n\n\nSummarize\n\n\n\n\n\n\n\nTarget\n\n\n\n\nTrends\n\n\nOutliers\n\n\nFeatures\n\n\nDistribution\n\n\nExtremes\n\n\nDependency\n\n\nCorrelation\n\n\nSimilarity\n\n\nTopology\n\n\nPaths\n\n\nShape"
  },
  {
    "objectID": "BCB520/posts/L3-TaskAbstraction/index.html#mandatory-fun",
    "href": "BCB520/posts/L3-TaskAbstraction/index.html#mandatory-fun",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "MANDATORY FUN",
    "text": "MANDATORY FUN\nWe will do these until everyone has done at least one example.\nJiyin Browse Trends\nRonald Identify Similarity\nRonald Lookup Similarity\nRonald Annotate Dependency\nBarrie Record Correlation\nCody Locate Distribution\nJiyin Enjoy Correlation\nCody Summarize Extremes\nEric Lookup Similarity\nJiyin Annotate Distribution\nCody Discover Topology\nEric Record Dependency\nEric Record Outliers\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "BCB520/posts/A9-NetworksinObservable/index.html",
    "href": "BCB520/posts/A9-NetworksinObservable/index.html",
    "title": "NETWORKS IN OBSERVABLE",
    "section": "",
    "text": "The QUARTO documentation on Observable can be found here.\nThe preamble of that document summarizes things nicely:\n\nQuarto includes native support for Observable JS, a set of enhancements to vanilla JavaScript created by Mike Bostock (also the author of D3). Observable JS is distinguished by its reactive runtime, which is especially well suited for interactive data exploration and analysis.\nThe creators of Observable JS (Observable, Inc.) run a hosted service at https://observablehq.com/ where you can create and publish notebooks. Additionally, you can use Observable JS (“OJS”) in standalone documents and websites via its core libraries. Quarto uses these libraries along with a compiler that is run at render time to enable the use of OJS within Quarto documents.\nOJS works in any Quarto document (plain markdown as well as Jupyter and Knitr documents). Just include your code in an {ojs} executable code block.\n\n\n\nI’m going to start by trying to replicate this observable notebook:\n\n\nCode\nviewof graph = {\n  const form = html`&lt;form style=\"font: 12px var(--sans-serif); display: flex; height: 33px; align-items: center;\"&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"1\" style=\"margin-right: 0.5em;\" checked&gt; Graph 1\n  &lt;/label&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"2\" style=\"margin-right: 0.5em;\"&gt; Graph 2\n  &lt;/label&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"3\" style=\"margin-right: 0.5em;\"&gt; Graph 3\n  &lt;/label&gt;\n&lt;/form&gt;`;\n  const graphs = {1: graph1, 2: graph2, 3: graph3};\n  const timeout = setInterval(() =&gt; {\n    form.value = graphs[form.radio.value = (+form.radio.value) % 3 + 1];\n    form.dispatchEvent(new CustomEvent(\"input\"));\n  }, 2000);\n  form.onchange = () =&gt; form.dispatchEvent(new CustomEvent(\"input\")); // Safari\n  form.oninput = event =&gt; { \n    if (event.isTrusted) clearInterval(timeout), form.onchange = null;\n    form.value = graphs[form.radio.value];\n  };\n  form.value = graphs[form.radio.value];\n  invalidation.then(() =&gt; clearInterval(timeout));\n  return form;\n}\n\n\n\n\nchart2 = {\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height]);\n\n  const simulation = d3.forceSimulation()\n      .force(\"charge\", d3.forceManyBody().strength(-1000))\n      .force(\"link\", d3.forceLink().id(d =&gt; d.id).distance(200))\n      .force(\"x\", d3.forceX())\n      .force(\"y\", d3.forceY())\n      .on(\"tick\", ticked);\n\n  let link = svg.append(\"g\")\n      .attr(\"stroke\", \"#000\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll(\"line\");\n\n  let node = svg.append(\"g\")\n      .attr(\"stroke\", \"#fff\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll(\"circle\");\n\n  function ticked() {\n    node.attr(\"cx\", d =&gt; d.x)\n        .attr(\"cy\", d =&gt; d.y)\n\n    link.attr(\"x1\", d =&gt; d.source.x)\n        .attr(\"y1\", d =&gt; d.source.y)\n        .attr(\"x2\", d =&gt; d.target.x)\n        .attr(\"y2\", d =&gt; d.target.y);\n  }\n\n  // Terminate the force layout when this cell re-runs.\n  invalidation.then(() =&gt; simulation.stop());\n\n  return Object.assign(svg.node(), {\n    update({nodes, links}) {\n\n      // Make a shallow copy to protect against mutation, while\n      // recycling old nodes to preserve position and velocity.\n      const old = new Map(node.data().map(d =&gt; [d.id, d]));\n      nodes = nodes.map(d =&gt; Object.assign(old.get(d.id) || {}, d));\n      links = links.map(d =&gt; Object.assign({}, d));\n\n      simulation.nodes(nodes);\n      simulation.force(\"link\").links(links);\n      simulation.alpha(1).restart();\n\n      node = node\n        .data(nodes, d =&gt; d.id)\n        .join(enter =&gt; enter.append(\"circle\")\n          .attr(\"r\", 8)\n          .attr(\"fill\", d =&gt; color(d.id)));\n\n      link = link\n        .data(links, d =&gt; `${d.source.id}\\t${d.target.id}`)\n        .join(\"line\");\n    }\n  });\n}\n\n\nupdate = chart2.update(graph)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows us how Dr. Bostock creates the basic architecture of an EDGE LIST called links along with NODE LABELS called nodes. Have a close look at the structure. He is setting this up in a heirarchy very similar to a JSON file, which we will examine in EXAMPLE 2.\n\n\nCode\ngraph1 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"},\n    {id: \"c\"}\n  ],\n  links: []\n})\n\n\ngraph2 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"},\n    {id: \"c\"}\n  ],\n  links: [\n    {source: \"a\", target: \"b\"},\n    {source: \"b\", target: \"c\"},\n    {source: \"c\", target: \"a\"}\n  ]\n})\n\n\ngraph3 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"}\n  ],\n  links: [\n    {source: \"a\", target: \"b\"}\n  ]\n})\n\n\ncolor = d3.scaleOrdinal(d3.schemeTableau10)\n\nheight = 400\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the graph3 object so that it contains six nodes called “Jiyin”, “Ronald”, “Erick”, “Cody”, “Mac”, and “PC”. Change the links object to reflect our shared understanding of those links.\n\n\n\nI honestly cannot believe this works! I mean… Hey! Look at this cool interactive network!\n\n\nCode\nchart = ForceGraph(miserables, {\n  nodeId: d =&gt; d.id,\n  nodeGroup: d =&gt; d.group,\n  nodeTitle: d =&gt; `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l =&gt; Math.sqrt(l.value),\n  width,\n  height: 600,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\n\n\n\n\n\n\n\nThe first line of code in the chunk below defines the data object from a .json file called miserables.json. Have a look at this file within RStudio. Does the overall structure look familiar?\nCould we possibly replace the stupid data file about a stupid musical with something of our own design???\n\n\nCode\nmiserables = FileAttachment(\"network_graph.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d =&gt; d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) =&gt; source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) =&gt; target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) =&gt; N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) =&gt; ({id: N[i]}));\n  links = d3.map(links, (_, i) =&gt; ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) =&gt; N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) =&gt; W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) =&gt; L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) =&gt; color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) =&gt; T[i]);\n  if (invalidation != null) invalidation.then(() =&gt; simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d =&gt; d.source.x)\n      .attr(\"y1\", d =&gt; d.source.y)\n      .attr(\"x2\", d =&gt; d.target.x)\n      .attr(\"y2\", d =&gt; d.target.y);\n\n    node\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if we replaced the datafile by making our own json file??\n\n\nCode\nlibrary(jsonlite)\n\n# create data frames for nodes and links\nnodes &lt;- data.frame(\n  id = c(\"Barrie\", \"Ronald\", \"Cody\", \"Erick\", \"Jiyin\", \"Cthulhu\"),\n  group = c(1, 1, 1 , 2, 2, 3)\n)\n\nlinks &lt;- data.frame(\n  source = c(\"Barrie\", \"Ronald\", \"Cody\", \"Barrie\", \"Erick\", \"Jiyin\", \"Ronald\"),\n  target = c(\"Cthulhu\", \"Erick\", \"Jiyin\", \"Erick\", \"Cthulhu\", \"Ronald\", \"Cody\"),\n  value = c(1, 8, 10, 6, 1, 1, 1)\n)\n\n# convert data frames to JSON objects\nnodes_json &lt;- toJSON(list(nodes = nodes), pretty = TRUE)\nlinks_json &lt;- toJSON(list(links = links), pretty = TRUE)\n\n# merge JSON objects into one\njson &lt;- paste0( nodes_json, links_json)\n\n# write JSON object to file\nwrite(json, file = \"network_graph2.json\")\n\n\nOh god…. now go back and point the stuff to the stuff…\nAnyway…. here is where I want to go:\nAMAZING"
  },
  {
    "objectID": "BCB520/posts/T1-Lit-Prog/index.html",
    "href": "BCB520/posts/T1-Lit-Prog/index.html",
    "title": "TUTORIAL 1 - Literate Programming",
    "section": "",
    "text": "Learning new tools is hard. Plowing though the tomes of the Data Science Mythos is hard. Perhaps this tutorial will guide you through the mind shattering truths of… LITERATE PROGRAMMING."
  },
  {
    "objectID": "BCB520/posts/Syllabus/index.html",
    "href": "BCB520/posts/Syllabus/index.html",
    "title": "SYLLABUS 2023",
    "section": "",
    "text": "Barrie D. Robison\nSpring 2023\n\n\nThis class will help students establish a foundational understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R programming language. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using R.  The course is designed to be “discipline agnostic” -  each student is encouraged to use data sets that they deem important / interesting.  The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\n\n\n\nStudents completing this course will be able to:\n\nDescribe and manipulate tabular, network, and spatial data; transform these data into a form suitable for visualization.\nAnalyze data visualization design choices related to marks and channels, spatial arrangement, and components of color.\nDesign new data visualizations with appropriate use of visual channels for tabular, network, and spatial data with quantitative and categorical attributes.\nImplement their data visualization designs using existing tools in R (or other toolkits preferred by the student).\nExplain whether a visual encoding is perceptually appropriate for a specific combination of task and data.\nDemonstrate their skills with at least two novel visualizations suitable for inclusion in an online Data Science Portfolio.\n\n\n\n\nTamara Munzner. Visualization Analysis and Design. A K Peters Visualization Series, CRC Press, 2014. For the 2023 version of the class, I am trying out the Inclusive Access option. Erolled students should have access to the text on the course canvas page.\nHard Copy on Amazon\nkindle/ebook on Amazon\n\n\n\n\n\n50% of your grade will be determined by homework exercises related to each course unit.\n20% of your grade will be determined by a mid term project (which would be a great item to include in your Data Science Portfolio).\n20% of your grade will be determined by a final project (which would be great item to include in your Data Science Portfolio).\n10% of your grade will be determined by participation in class discussions.\nGRADING SCALE: The grading scale is standard: A (90 -100 %), B (89 - 80 %), C (79 - 70 %), D (69-60 %), F( below 60 %).\n\n\n\nIt is a longstanding tradition that Vandals take care of Vandals, and we all do our best to look out for the Vandal Family. These simple precautions go a long way in reducing the impact of coronavirus on our campuses and in our communities. With everyone engaging in these small actions, we can continue to participate in our vibrant campus culture where we are able to learn, live, and grow. Please bookmark the University of Idaho Covid-19 webpage and visit it often for the most up-to-date information about the U of I’s response to Covid-19.\n\nAll classes are offered in the modality listed in the catalog.\nAll Vandals are highly encouraged to be vaccinated.\nCOVID-19 tests are not required to attend class in person.\n\nAdditionally, faculty and students must follow the Healthy Vandal Pledge: Daily Symptom Monitoring and In-Person Class Attendance. Evaluate your own health status before attending in-person classes and refrain from attending class in-person if you are ill, if you are experiencing any of the known symptoms of Coronavirus, or if you have tested positive for COVID-19 or have been potentially exposed to someone with COVID-19. Stay home if you experience any symptoms related to COVID 19 and that are not attributed to a non-infectious health condition regardless of how mild. Contact your medical provider or local Idaho Public Health District for assessment of symptoms and possible COVID19 testing. Positive COVID 19 tests should be submitted via a VandalCare Report in order to make arrangements that involve classroom absences due to illness, and/or quarantine or isolation requirements directed by a medical provider.\n\n\n\nMissing a scheduled lecture is at your discretion. I will be posting all the lecture notes online, and recording the lectures for future online versions of the course. I have no intention of offering a zoom option this semester. If a discussion or in-class exercise occurs and you miss it, you will lose those participation points. There is no way to make up those points.\n\n\n\nThe R Markdown template I used for this syllabus was created by Dr. Steven V. Miller at Stockholm University. It contained this section, which I found amusing and have therefore retained. Professor Miller’s current university asks professors to have policies written into their syllabus about what students should do if the professor is more than 15 minutes late to class. Here is my version of that policy:\nI will inform students via e-mail in advance of class if class is cancelled for the day. Events that might create such a scenario include travel obligations that emerged after the semester has begun, a family emergency that encompasses multiple days, or some other thing. I will also contact our department secretary in emergent situations, such as something happening on the way to work. Failing that, assume the worst. Alien abduction, the return of one or more Old Ones to our plane, or some kind of attack by wizards are all viable explanations for my inability to attend class. I ask that the students make sure that my story gets the proper treatment on the “Mr. Ballen” YouTube channel. I also ask that my story be narrated by Morgan Freeman and that the role of me in the made for TV movie be played by Keanu Reeves or Danny DeVito.\n\n\n\nThe bad news is that there are NO make-ups for missed exams. Don’t bother asking. The good news is that there aren’t any exams.\n\n\n\nAll students are expected to uphold the highest standards of academic honesty. This includes but is not limited to: not cheating, not using the ideas of others without giving appropriate credit (including Wikipedia!), and not falsifying data. Any incident of academic dishonesty will be handled according to the guidelines of the University of Idaho. \n\n\n\n\n\nlibrary(readxl)\nSchedule &lt;- read_excel(\"Schedule.xlsx\")\n\nknitr::kable(Schedule, caption = '')\n\n\n\n\nDATE\nTOPIC\nREADING\nRECEIVE\nTURN IN\n\n\n\n\n2023-01-12\nINTRODUCTION.  Why Visualization Matters\nVAD Ch 1.\nAssignment 1 - Fun with literate programming.\nNA\n\n\n2023-01-17\nWHAT?  Abstraction of Data\nVAD Ch 2.\nNA\nAssignment 1 - Fun with literate programming.\n\n\n2023-01-19\nWorking Session\nNA\nAssignment 2 - You’ll need some data…\nNA\n\n\n2023-01-24\nWorking Session\nNA\nNA\nAssignment 2 - You’ll need some data…\n\n\n2023-01-26\nWHY?  Task Abstraction\nVAD Ch 3.\nAssignment 3 - Data Science Portfolio\nNA\n\n\n2023-01-31\nWorking Session\nNA\nNA\nNA\n\n\n2023-02-02\nWorking Session\nNA\nNA\nNA\n\n\n2023-02-07\nPortfolio Review - Action/Target Pairs\nNA\nNA\nAssignment 3 - Data Science Portfolio\n\n\n2023-02-09\nMARKS. Geometric elements to depict data\nVAD Ch 5.\nNA\nNA\n\n\n2023-02-14\nNA\nNA\nNA\nNA\n\n\n2023-02-16\nNA\nNA\nNA\nMidterm Assignment Presentations\n\n\n2023-02-21\nNA\nNA\nNA\nMidterm Assignment Presentations\n\n\n2023-02-23\nCHANNELS. Controlling the appearance of marks.\nVAD Ch 5.\nAssignment 6 - Spatial things\nNA\n\n\n2023-02-28\nRULES OF THUMB.\nVAD Ch. 6.\nNA\nNA\n\n\n2023-03-02\nTABULAR DATA I\nVAD Ch. 7.\nAssignment 7 - Network things\nNA\n\n\n2023-03-07\nTABULAR DATA II\nVAD Ch. 7.\nNA\nNA\n\n\n2023-03-09\nSPATIAL DATA I: Geographic Maps\nVAD Ch 8.\nAssignment 8 - Find some stuff for critique\nNA\n\n\n2023-03-14\nNo Class\nNA\nNA\nSPRING BREAK\n\n\n2023-03-16\nNo Class\nNA\nNA\nSPRING BREAK\n\n\n2023-03-21\nBarrie in NV\nNA\nNA\nNA\n\n\n2023-03-23\nSPATIAL DATA II:  Spatial Fields\nVAD Ch 8.\nAssignment 9 - Let’s practice Observable!\nNA\n\n\n2023-03-28\nNETWORK DATA I\nVAD Ch 9.\nNA\nNA\n\n\n2023-03-30\nNETWORK DATA II\nVAD Ch 9.\nAssignment 10 - Cool interactive stuff\nNA\n\n\n2023-04-04\nCOLOR I\nVAD Ch 10.\nNA\nNA\n\n\n2023-04-06\nCOLOR II\nVAD Ch 10.\nAssignment 11 - Data Science Competition\nNA\n\n\n2023-04-11\nCOLOR III\nVAD Ch 10.\nNA\nNA\n\n\n2023-04-13\nINTERACTIVITY\nVAD Ch 11.\nNA\nNA\n\n\n2023-04-18\nMULTIPLE VIEWS\nVAD Ch 12.\nNA\nNA\n\n\n2023-04-20\nAGGREGATION\nVAD Ch 13.\nNA\nNA\n\n\n2023-04-25\nFILTERING\nVAD Ch 13.\nNA\nFINAL PROJECT PRESENTATIONS\n\n\n2023-04-27\nEMBEDDING: Focus and Context\nVAD Ch 14.\nNA\nFINAL PROJECT PRESENTATIONS\n\n\n2023-05-02\nDEAD WEEK\nNA\nPortfolio Review\nNA\n\n\n2023-05-04\nDEAD WEEK\nNA\nPortfolio Review\nNA\n\n\n2023-05-09\nFINALS WEEK\nNA\nNA\nNA"
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html",
    "href": "BCB520/posts/A8-NetworkData/index.html",
    "title": "Practice with Network Data",
    "section": "",
    "text": "In this assignment, we’ll consider some of the tools and techniques for visualizing network data. Network data is characterized by two unique items that are not found in tabular or spatial data - Nodes and Links. In addition, there is a sub-type of network data that we will consider - Hierarchical or Tree data. Let’s practice a few visualizations to get a feel for how these things work!"
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#igraph",
    "href": "BCB520/posts/A8-NetworkData/index.html#igraph",
    "title": "Practice with Network Data",
    "section": "IGRAPH",
    "text": "IGRAPH\nLet’s start with igraph, which is an open source toolset for network analysis. The great thing about igraph is that you can use these tools in R, Python, Mathematica, and C++. It is very flexible and very powerful.\n\nigraph in R\nFirst up, we’ll install R/igraph and load the library (note that I’ve commented out the package installation because I’ve already installed igraph on my machine):\n\n\nCode\n# install.packages(\"igraph\")\nlibrary(igraph)\n\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n\nNow I’m going to walk you through a modified version of the igraph tutorial, which you can find here"
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#creating-a-graph",
    "href": "BCB520/posts/A8-NetworkData/index.html#creating-a-graph",
    "title": "Practice with Network Data",
    "section": "Creating a graph",
    "text": "Creating a graph\nigraph offers many ways to create a graph. The simplest one is the function make_empty_graph:\n\n\nCode\ng &lt;- make_empty_graph()\n\n\nThe most common way to create a graph is make_graph, which constructs a network based on specified edges. For example, to make a graph with 10 nodes (numbered 1 to 10) and two edges connecting nodes 1-2 and 1-5:\n\n\nCode\ng &lt;- make_graph(edges = c(1,2, 1,5), n=10, directed = FALSE)\n\n\nWe can print the graph to get a summary of its nodes and edges:\n\n\nCode\ng\n\n\nIGRAPH bc76029 U--- 10 2 -- \n+ edges from bc76029:\n[1] 1--2 1--5\n\n\nThis means: Undirected Named graph with 10 vertices and 2 edges, with the exact edges listed out. If the graph has a [name] attribute, it is printed as well.\n\n\n\n\n\n\nNote\n\n\n\nsummary does not list the edges, which is convenient for large graphs with millions of edges:\n\n\n\n\nCode\nsummary(g)\n\n\nIGRAPH bc76029 U--- 10 2 -- \n\n\nThe same function make_graph can create some notable graphs by just specifying their name. For example you can create the graph that represents the social network of Zachary’s karate club, that shows the friendship between 34 members of a karate club at a US university in the 1970s:\n\n\nCode\ng &lt;- make_graph('Zachary')\n\n\nTo visualize a graph you can use plot:\n\n\nCode\nplot(g)"
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#vertex-and-edge-ids",
    "href": "BCB520/posts/A8-NetworkData/index.html#vertex-and-edge-ids",
    "title": "Practice with Network Data",
    "section": "Vertex and edge IDs",
    "text": "Vertex and edge IDs\nVertices and edges have numerical vertex IDs in igraph. Vertex IDs are always consecutive and they start with 1. For a graph with n vertices the vertex IDs are always between 1 and n. If some operation changes the number of vertices in the graphs, e.g. a subgraph is created via induced_subgraph, then the vertices are renumbered to satisfy this criterion.\nThe same is true for the edges as well: edge IDs are always between 1 and m, the total number of edges in the graph.\nIn addition to IDs, vertices and edges can be assigned a name and other attributes. That makes it easier to track them whenever the graph is altered."
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#addingdeleting-vertices-and-edges",
    "href": "BCB520/posts/A8-NetworkData/index.html#addingdeleting-vertices-and-edges",
    "title": "Practice with Network Data",
    "section": "Adding/deleting vertices and edges",
    "text": "Adding/deleting vertices and edges\nLet’s continue working with the Karate club graph. To add one or more vertices to an existing graph, use add_vertices:\n\n\nCode\ng &lt;- add_vertices(g, 3)\n\n\nSimilarly, to add edges you can use add_edges:\n\n\nCode\ng &lt;- add_edges(g, edges = c(1,35, 1,36, 34,37))\n\n\nEdges are added by specifying the source and target vertex IDs for each edge. This call added three edges, one connecting vertices 1 and 35, one connecting vertices 1 and 36, and one connecting vertices 34 and 37.\nIn addition to the add_vertices and add_edges functions, the plus operator can be used to add vertices or edges to graph. The actual operation that is performed depends on the type of the right hand side argument:\n\n\nCode\ng &lt;- g + edges(c(1,35, 1,36, 34,37))\n\n\nYou can add a single vertex/edge at a time using add_vertex and add_edge.\nLet us add some more vertices and edges to our graph. In igraph we can use the magrittr package, which provides a mechanism for chaining commands with the operator %\\&gt;%:\n\n\nCode\ng &lt;- g %&gt;% add_edges(edges=c(1,34)) %&gt;% add_vertices(3) %&gt;%\n     add_edges(edges=c(38,39, 39,40, 40,38, 40,37))\ng\n\n\nIGRAPH 3dacd24 U--- 40 86 -- Zachary\n+ attr: name (g/c)\n+ edges from 3dacd24:\n [1]  1-- 2  1-- 3  1-- 4  1-- 5  1-- 6  1-- 7  1-- 8  1-- 9  1--11  1--12\n[11]  1--13  1--14  1--18  1--20  1--22  1--32  2-- 3  2-- 4  2-- 8  2--14\n[21]  2--18  2--20  2--22  2--31  3-- 4  3-- 8  3--28  3--29  3--33  3--10\n[31]  3-- 9  3--14  4-- 8  4--13  4--14  5-- 7  5--11  6-- 7  6--11  6--17\n[41]  7--17  9--31  9--33  9--34 10--34 14--34 15--33 15--34 16--33 16--34\n[51] 19--33 19--34 20--34 21--33 21--34 23--33 23--34 24--26 24--28 24--33\n[61] 24--34 24--30 25--26 25--28 25--32 26--32 27--30 27--34 28--34 29--32\n[71] 29--34 30--33 30--34 31--33 31--34 32--33 32--34 33--34  1--35  1--36\n+ ... omitted several edges\n\n\nCode\nplot(g)\n\n\n\n\n\nWe now have an undirected graph with 40 vertices and 86 edges. Vertex and edge IDs are always contiguous, so if you delete a vertex all subsequent vertices will be renumbered. When a vertex is renumbered, edges are not renumbered, but their source and target vertices will be. Use delete_vertices and delete_edges to perform these operations. For instance, to delete the edge connecting vertices 1-34, get its ID and then delete it:\n\n\nCode\nget.edge.ids(g, c(1,34))\n\n\n[1] 82\n\n\n\n\nCode\ng &lt;- delete_edges(g, 82)\n\n\nAs an example, to create a broken ring:\n\n\nCode\ng &lt;- make_ring(10) %&gt;% delete_edges(\"10|1\")\nplot(g)\n\n\n\n\n\nThe example above shows that you can also refer to edges with strings containing the IDs of the source and target vertices, connected by a pipe symbol |. \"10|1\" in the above example means the edge that connects vertex 10 to vertex 1. Of course you can also use the edge IDs directly, or retrieve them with the get.edge.ids function:\n\n\nCode\ng &lt;- make_ring(5)\ng &lt;- delete_edges(g, get.edge.ids(g, c(1,5, 4,5)))\nplot(g)"
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#constructing-graphs",
    "href": "BCB520/posts/A8-NetworkData/index.html#constructing-graphs",
    "title": "Practice with Network Data",
    "section": "Constructing graphs",
    "text": "Constructing graphs\nIn addition to make_empty_graph, make_graph, and make_graph_from_literal, igraph includes many other function to construct a graph. Some are deterministic, i.e. they produce the same graph each single time, e.g. make_tree:\n\n\nCode\ngraph1 &lt;- make_tree(127, 2, mode = \"undirected\")\nsummary(graph1)\n\n\nIGRAPH 2fc7e28 U--- 127 126 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c)\n\n\nCode\nplot(graph1)\n\n\n\n\n\nThis generates a regular tree graph with 127 vertices, each vertex having two children. No matter how many times you call make_tree, the generated graph will always be the same if you use the same parameters:\n\n\nCode\ngraph2 &lt;- make_tree(127, 2, mode = \"undirected\")\n\n\n\n\nCode\nidentical_graphs(graph1,graph2)\n\n\n[1] TRUE\n\n\nOther functions generate graphs stochastically, i.e. they produce a different graph each time. For instance sample_grg:\n\n\nCode\ngraph1 &lt;- sample_grg(100, 0.2)\nsummary(graph1)\n\n\nIGRAPH 01c9e84 U--- 100 549 -- Geometric random graph\n+ attr: name (g/c), radius (g/n), torus (g/l)\n\n\nCode\nplot(graph1)\n\n\n\n\n\nThis generates a geometric random graph: n points are chosen randomly and uniformly inside the unit square and pairs of points closer to each other than a predefined distance d are connected by an edge. If you generate GRGs with the same parameters, they will be different:\n\n\nCode\ngraph2 &lt;- sample_grg(100, 0.2)\nidentical_graphs(graph1, graph2)\n\n\n[1] FALSE\n\n\nCode\nplot(graph2)\n\n\n\n\n\nA slightly looser way to check if the graphs are equivalent is via isomorphic. Two graphs are said to be isomorphic if they have the same number of components (vertices and edges) and maintain a one-to-one correspondence between vertices and edges, i.e., they are connected in the same way.\n\n\nCode\nisomorphic(graph1, graph2)\n\n\n[1] FALSE\n\n\nChecking for isomorphism can take a while for large graphs (in this case, the answer can quickly be given by checking the degree sequence of the two graphs). identical_graph is a stricter criterion than isomorphic: the two graphs must have the same list of vertices and edges, in exactly the same order, with same directedness, and the two graphs must also have identical graph, vertex and edge attributes."
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#setting-and-retrieving-attributes",
    "href": "BCB520/posts/A8-NetworkData/index.html#setting-and-retrieving-attributes",
    "title": "Practice with Network Data",
    "section": "Setting and retrieving attributes",
    "text": "Setting and retrieving attributes\nIn addition to IDs, vertex and edges can have attributes such as a name, coordinates for plotting, metadata, and weights. The graph itself can have such attributes too (e.g. a name, which will show in summary). In a sense, every graph, vertex and edge can be used as an R namespace to store and retrieve these attributes.\nTo demonstrate the use of attributes, let us create a simple social network:\n\n\nCode\ng &lt;- make_graph(~ Alice-Bob:Claire:Frank, Claire-Alice:Dennis:Frank:Esther,\n                George-Dennis:Frank, Dennis-Esther)\n\n\nEach vertex represents a person, so we want to store ages, genders and types of connection between two people (is_formal refers to whether a connection between one person or another is formal or informal, i.e. colleagues or friends). The \\$ operator is a shortcut to get and set graph attributes. It is shorter and just as readable as graph_attr and set_graph_attr.\n\n\nCode\nV(g)$age &lt;- c(25, 31, 18, 23, 47, 22, 50) \nV(g)$gender &lt;- c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")\nE(g)$is_formal &lt;- c(FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE)\nsummary(g)\n\n\nIGRAPH b41a80c UN-- 7 9 -- \n+ attr: name (v/c), age (v/n), gender (v/c), is_formal (e/l)\n\n\nV and E are the standard way to obtain a sequence of all vertices and edges, respectively. This assigns an attribute to all vertices/edges at once. Another way to generate our social network is with the use of set_vertex_attr and set_edge_attr and the operator %\\&gt;%:\n\n\nCode\ng &lt;- make_graph(~ Alice-Bob:Claire:Frank, Claire-Alice:Dennis:Frank:Esther,\n                George-Dennis:Frank, Dennis-Esther) %&gt;%\n  set_vertex_attr(\"age\", value = c(25, 31, 18, 23, 47, 22, 50)) %&gt;%\n  set_vertex_attr(\"gender\", value = c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")) %&gt;%\n  set_edge_attr(\"is_formal\", value = c(FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE))\nsummary(g)\n\n\nTo assign or modify an attribute for a single vertex/edge:\n\n\nCode\nE(g)$is_formal\n\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nCode\nE(g)$is_formal[1] &lt;- TRUE\nE(g)$is_formal\n\n\n[1]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nAttribute values can be set to any R object, but note that storing the graph in some file formats might result in the loss of complex attribute values. Vertices, edges and the graph itself can all be used to set attributes, e.g. to add a date to the graph:\n\n\nCode\ng$date &lt;- c(\"2022-02-11\")\ngraph_attr(g, \"date\")\n\n\n[1] \"2022-02-11\"\n\n\nTo retrieve attributes, you can also use graph_attr, vertex_attr, and edge_attr. To find the ID of a vertex you can use the function match:\n\n\nCode\nmatch(c(\"George\"), V(g)$name)\n\n\n[1] 7\n\n\nTo assign attributes to a subset of vertices or edges, you can use:\n\n\nCode\nV(g)$name[1:3] &lt;- c(\"Alejandra\", \"Bruno\", \"Carmina\")\nV(g)\n\n\n+ 7/7 vertices, named, from b41a80c:\n[1] Alejandra Bruno     Carmina   Frank     Dennis    Esther    George   \n\n\nTo delete attributes:\n\n\nCode\ng &lt;- delete_vertex_attr(g, \"gender\")\nV(g)$gender\n\n\nNULL\n\n\nIf you want to save a graph in R with all the attributes use the R’s standard function dput function and retrieve it later with dget. You can also just save the R workspace and restore it later."
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#structural-properties-of-graphs",
    "href": "BCB520/posts/A8-NetworkData/index.html#structural-properties-of-graphs",
    "title": "Practice with Network Data",
    "section": "Structural properties of graphs",
    "text": "Structural properties of graphs\nigraph provides a large set of functions to calculate various structural properties of graphs. It is beyond the scope of this tutorial to document all of them, hence this section will only introduce a few of them for illustrative purposes. We will work on the small social network constructed in the previous section.\nPerhaps the simplest property one can think of is the degree. The degree of a vertex equals the number of edges adjacent to it. In case of directed networks, we can also define in-degree (the number of edges pointing towards the vertex) and out-degree (the number of edges originating from the vertex). igraph is able to calculate all of them using a simple syntax:\n\n\nCode\ndegree(g)\n\n\nAlejandra     Bruno   Carmina     Frank    Dennis    Esther    George \n        3         1         4         3         3         2         2 \n\n\nIf the graph was directed, we would have been able to calculate the in- and out-degrees separately using degree(mode=\"in\") and degree(mode=\"out\"). You can also pass a single vertex ID or a list of vertex IDs to degree if you want to calculate the degrees for only a subset of vertices:\n\n\nCode\ndegree(g, 7)\n\n\nGeorge \n     2 \n\n\n\n\nCode\ndegree(g, v=c(3,4,5))\n\n\nCarmina   Frank  Dennis \n      4       3       3 \n\n\nMost functions that accept vertex IDs also accept vertex names (i.e. the values of the name vertex attribute) as long as the names are unique:\n\n\nCode\ndegree(g, v=c(\"Carmina\", \"Frank\", \"Dennis\"))\n\n\nCarmina   Frank  Dennis \n      4       3       3 \n\n\nIt also works for single vertices:\n\n\nCode\ndegree(g, \"Bruno\")\n\n\nBruno \n    1 \n\n\nA similar syntax is used for most of the structural properties igraph can calculate. For vertex properties, the functions accept a vertex ID, a vertex name, or a list of vertex IDs or names (and if they are omitted, the default is the set of all vertices). For edge properties, the functions accept a single edge ID or a list of edge IDs.\n\nNOTE: For some measures, it does not make sense to calculate them only for a few vertices or edges instead of the whole graph, as it would take the same time anyway. In this case, the functions won’t accept vertex or edge IDs, but you can still restrict the resulting list later using standard operations. One such example is eigenvector centrality (evcent()).\n\nBesides degree, igraph includes built-in routines to calculate many other centrality properties, including vertex and edge betweenness (edge_betweenness) or Google’s PageRank (page_rank) just to name a few. Here we just illustrate edge betweenness:\n\n\nCode\nedge_betweenness(g)\n\n\n[1] 6 6 4 3 4 4 4 2 3\n\n\nNow we can also figure out which connections have the highest betweenness centrality:\n\n\nCode\nebs &lt;- edge_betweenness(g)\nas_edgelist(g)[ebs == max(ebs), ]\n\n\n     [,1]        [,2]     \n[1,] \"Alejandra\" \"Bruno\"  \n[2,] \"Alejandra\" \"Carmina\""
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#querying-vertices-and-edges-based-on-attributes",
    "href": "BCB520/posts/A8-NetworkData/index.html#querying-vertices-and-edges-based-on-attributes",
    "title": "Practice with Network Data",
    "section": "Querying vertices and edges based on attributes",
    "text": "Querying vertices and edges based on attributes\n\nSelecting vertices\nImagine that in a given social network, you want to find out who has the largest degree. You can do that with the tools presented so far and the which.max function:\n\n\nCode\nwhich.max(degree(g))\n\n\nCarmina \n      3 \n\n\nAnother example would be to select only vertices that have only odd IDs but not even ones, using the V function:\n\n\nCode\ngraph &lt;- graph.full(n=10)\nonly_odd_vertices &lt;- which(V(graph)%%2==1)\nlength(only_odd_vertices)\n\n\n[1] 5\n\n\nOf course, it is possible to select vertices or edges by positional indices:\n\n\nCode\nseq &lt;- V(graph)[2, 3, 7]\nseq\n\n\n+ 3/10 vertices, from 34c4bd9:\n[1] 2 3 7\n\n\n\n\nCode\nseq &lt;- seq[1, 3]    # filtering an existing vertex set\nseq\n\n\n+ 2/10 vertices, from 34c4bd9:\n[1] 2 7\n\n\nSelecting a vertex that does not exist results in an error:\n\n\nCode\nseq &lt;- V(graph)[2, 3, 7, \"foo\", 3.5]\n## Error in simple_vs_index(x, ii, na_ok) : Unknown vertex selected\n\n\nAttribute names can also be used as-is within the indexing brackets of V() and E(). This can be combined with R’s ability to use boolean vectors for indexing to obtain very concise and readable expressions to retrieve a subset of the vertex or edge set of a graph. For instance, the following command gives you the names of the individuals younger than 30 years in our social network:\n\n\nCode\nV(g)[age &lt; 30]$name\n\n\n[1] \"Alejandra\" \"Carmina\"   \"Frank\"     \"Esther\"   \n\n\nOf course, &lt; is not the only boolean operator that can be used for this. Other possibilities include the following:\n\n\n\n\n\n\n\nOperator\nMeaning\n\n\n\n\n==\nThe attribute/property value must be equal to\n\n\n!=\nThe attribute/property value must not be equal to\n\n\n&lt;\nThe attribute/property value must be less than\n\n\n&lt;=\nThe attribute/property value must be less than or equal to\n\n\n&gt;\nThe attribute/property value must be greater than\n\n\n&gt;=\nThe attribute/property value must be greater than or equal to\n\n\n%in%\nThe attribute/property value must be included in\n\n\n\nYou can also create a “not in” operator from %in% using the Negate function:\n\n\nCode\n`%notin%` &lt;- Negate(`%in%`)\n\n\nIf an attribute has the same name as an igraph function, you should be careful as the syntax can become a little confusing. For instance, if there is an attribute named degree that represents the grades of an exam for each person, that should not be confused with the igraph function that computes the degrees of vertices in a network sense:\n\n\nCode\nV(g)$degree &lt;- c(\"A\", \"B\", \"B+\", \"A+\", \"C\", \"A\", \"B\")\nV(g)$degree[degree(g) == 3]\n\n\n[1] \"A\"  \"A+\" \"C\" \n\n\n\n\nCode\nV(g)$name[degree(g) == 3]\n\n\n[1] \"Alejandra\" \"Frank\"     \"Dennis\"   \n\n\n\n\nSelecting edges\nEdges can be selected based on attributes just like vertices. As mentioned above, the standard way to get edges is E. Moreover, there are a few special structural properties for selecting edges.\nUsing .from allows you to filter the edge sequence based on the source vertices of the edges. E.g., to select all the edges originating from Carmina (who has vertex index 3):\n\n\nCode\nE(g)[.from(3)]\n\n\n+ 4/9 edges from b41a80c (vertex names):\n[1] Alejandra--Carmina Carmina  --Frank   Carmina  --Dennis  Carmina  --Esther \n\n\nOf course it also works with vertex names:\n\n\nCode\nE(g)[.from(\"Carmina\")]\n\n\n+ 4/9 edges from b41a80c (vertex names):\n[1] Alejandra--Carmina Carmina  --Frank   Carmina  --Dennis  Carmina  --Esther \n\n\nUsing .to filters edge sequences based on the target vertices. This is different from .from if the graph is directed, while it gives the same answer for undirected graphs. Using .inc selects only those edges that are incident on a single vertex or at least one of the vertices, irrespectively of the edge directions.\nThe %--% operator can be used to select edges between specific groups of vertices, ignoring edge directions in directed graphs. For instance, the following expression selects all the edges between Carmina (vertex index 3), Dennis (vertex index 5) and Esther (vertex index 6):\n\n\nCode\nE(g) [ 3:5 %--% 5:6 ]\n\n\n+ 3/9 edges from b41a80c (vertex names):\n[1] Carmina--Dennis Carmina--Esther Dennis --Esther\n\n\nTo make the %--% operator work with names, you can build string vectors containing the names and then use these vectors as operands. For instance, to select all the edges that connect men to women, we can do the following after re-adding the gender attribute that we deleted earlier:\n\n\nCode\nV(g)$gender &lt;- c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")\n\n\n\n\nCode\nmen &lt;- V(g)[gender == \"m\"]$name\nmen\n\n\n[1] \"Bruno\"  \"Frank\"  \"Dennis\" \"George\"\n\n\n\n\nCode\nwomen &lt;- V(g)[gender == \"f\"]$name\nwomen\n\n\n[1] \"Alejandra\" \"Carmina\"   \"Esther\"   \n\n\n\n\nCode\nE(g)[men %--% women]\n\n\n+ 5/9 edges from b41a80c (vertex names):\n[1] Alejandra--Bruno  Alejandra--Frank  Carmina  --Frank  Carmina  --Dennis\n[5] Dennis   --Esther"
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#treating-a-graph-as-an-adjacency-matrix",
    "href": "BCB520/posts/A8-NetworkData/index.html#treating-a-graph-as-an-adjacency-matrix",
    "title": "Practice with Network Data",
    "section": "Treating a graph as an adjacency matrix",
    "text": "Treating a graph as an adjacency matrix\nThe adjacency matrix is another way to represent a graph. In an adjacency matrix, rows and columns are labeled by graph vertices, and the elements of the matrix indicate the number of edges between vertices i and j. The adjacency matrix for the example graph is:\n\n\nCode\nget.adjacency(g)\n\n\n7 x 7 sparse Matrix of class \"dgCMatrix\"\n          Alejandra Bruno Carmina Frank Dennis Esther George\nAlejandra         .     1       1     1      .      .      .\nBruno             1     .       .     .      .      .      .\nCarmina           1     .       .     1      1      1      .\nFrank             1     .       1     .      .      .      1\nDennis            .     .       1     .      .      1      1\nEsther            .     .       1     .      1      .      .\nGeorge            .     .       .     1      1      .      .\n\n\nFor example, Carmina (1, 0, 0, 1, 1, 1, 0) is directly connected to Alejandra (who has vertex index 1), Frank (index 4), Dennis (index 5) and Esther (index 6), but not to Bruno (index 2) or to George (index 7)."
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#layouts-and-plotting",
    "href": "BCB520/posts/A8-NetworkData/index.html#layouts-and-plotting",
    "title": "Practice with Network Data",
    "section": "Layouts and plotting",
    "text": "Layouts and plotting\nA graph is an abstract mathematical object without a specific representation in 2D, 3D or any other geometric space. This means that whenever we want to visualise a graph, we have to find a mapping from vertices to coordinates in two- or three-dimensional space first, preferably in a way that is useful and/or pleasing for the eye. A separate branch of graph theory, namely graph drawing, tries to solve this problem via several graph layout algorithms. igraph implements quite a few layout algorithms and is also able to draw them onto the screen or to any output format that R itself supports.\n\nLayout algorithms\nThe layout functions in igraph always start with layout. The following table summarises them:\n\n\n\n\n\n\n\nMethod name\nAlgorithm description\n\n\n\n\nlayout_randomly\nPlaces the vertices completely randomly\n\n\nlayout_in_circle\nDeterministic layout that places the vertices on a circle\n\n\nlayout_on_sphere\nDeterministic layout that places the vertices evenly on the surface of a sphere\n\n\nlayout_with_drl\nThe Drl (Distributed Recursive Layout) algorithm for large graphs\n\n\nlayout_with_fr\nFruchterman-Reingold force-directed algorithm\n\n\nlayout_with_kk\nKamada-Kawai force-directed algorithm\n\n\nlayout_with_lgl\nThe LGL (Large Graph Layout) algorithm for large graphs\n\n\nlayout_as_tree\nReingold-Tilford tree layout, useful for (almost) tree-like graphs\n\n\nlayout_nicely\nLayout algorithm that automatically picks one of the other algorithms based on certain properties of the graph\n\n\n\nLayout algorithms can be called directly with a graph as its first argument. They will return a matrix with two columns and as many rows as the number of vertices in the graph; each row will correspond to the position of a single vertex, ordered by vertex IDs. Some algorithms have a 3D variant; in this case they return three columns instead of 2.\n\n\nCode\nlayout &lt;- layout_with_kk(g)\n\n\nSome layout algorithms take additional arguments; e.g., when laying out a graph as a tree, it might make sense to specify which vertex is to be placed at the root of the layout:\n\n\nCode\nlayout &lt;- layout_as_tree(g, root = 2)\n\n\n\n\nDrawing a graph using a layout\nWe can plot our imaginary social network with the Kamada-Kawai layout algorithm as follows:\n\n\nCode\nlayout &lt;- layout_with_kk(g)\n\n\n\n\nCode\nplot(g, layout = layout, main = \"Social network with the Kamada-Kawai layout algorithm\")\n\n\n\n\n\nThis should open a new window showing a visual representation of the network. Remember that the exact placement of nodes may be different on your machine since the layout is not deterministic.\nThe layout argument also accepts functions; in this case, the function will be called with the graph as its first argument. This makes it possible to just pass the name of a layout function directly, without creating a layout variable:\n\n\nCode\nplot(g, layout = layout_with_fr,\n     main = \"Social network with the Fruchterman-Reingold layout algorithm\")\n\n\n\n\n\nTo improve the visuals, a trivial addition would be to color the vertices according to the gender. We should also try to place the labels slightly outside the vertices to improve readability:\n\n\nCode\nV(g)$color &lt;- ifelse(V(g)$gender == \"m\", \"yellow\", \"red\")\nplot(g, layout = layout, vertex.label.dist = 3.5,\n     main = \"Social network - with genders as colors\")\n\n\n\n\n\nYou can also treat the gender attribute as a factor and provide the colors with an argument to plot(), which takes precedence over the color vertex attribute. Colors will be assigned automatically to levels of a factor:\n\n\nCode\nplot(g, layout=layout, vertex.label.dist=3.5, vertex.color=as.factor(V(g)$gender))\n\n\n\n\n\nAs seen above with the vertex.color argument, you can specify visual properties as arguments to plot instead of using vertex or edge attributes. The following plot shows the formal ties with thick lines while informal ones with thin lines:\n\n\nCode\nplot(g, layout=layout, vertex.label.dist=3.5, vertex.size=20,\n     vertex.color=ifelse(V(g)$gender == \"m\", \"yellow\", \"red\"),\n     edge.width=ifelse(E(g)$is_formal, 5, 1))\n\n\n\n\n\nThis latter approach is preferred if you want to keep the properties of the visual representation of your graph separate from the graph itself.\nIn summary, there are special vertex and edge properties that correspond to the visual representation of the graph. These attributes override the default settings of igraph (i.e color, weight, name, shape,layout,etc.). The following two tables summarise the most frequently used visual attributes for vertices and edges, respectively:\n\n\nVertex attributes controlling graph plots\n\n\n\n\n\n\n\n\nAttribute name\nKeyword argument\nPurpose\n\n\n\n\ncolor\nvertex.color\nColor of the vertex\n\n\nlabel\nvertex.label\nLabel of the vertex. They will be converted to character. Specify NA to omit vertex labels. The default vertex labels are the vertex ids.\n\n\nlabel.cex\nvertex.label.cex\nFont size of the vertex label, interpreted as a multiplicative factor, similarly to R’s text function\n\n\nlabel.color\nvertex.label.color\nColor of the vertex label\n\n\nlabel.degree\nvertex.label.degree\nIt defines the position of the vertex labels, relative to the center of the vertices. It is interpreted as an angle in radian, zero means ‘to the right’, and ‘pi’ means to the left, up is -pi/2 and down is pi/2. The default value is -pi/4\n\n\nlabel.dist\nvertex.label.dist\nDistance of the vertex label from the vertex itself, relative to the vertex size\n\n\nlabel.family\nvertex.label.family\nFont family of the vertex, similarly to R’s text function\n\n\nlabel.font\nvertex.label.font\nFont within the font family of the vertex, similarly to R’s text function\n\n\nshape\nvertex.shape\nThe shape of the vertex, currently “circle”, “square”, “csquare”, “rectangle”, “crectangle”, “vrectangle”, “pie” (see vertex.shape.pie), ‘sphere’, and “none” are supported, and only by the plot.igraph command.\n\n\nsize\nvertex.size\nThe size of the vertex, a numeric scalar or vector, in the latter case each vertex sizes may differ\n\n\n\n\n\nEdge attributes controlling graph plots\n\n\n\n\n\n\n\n\nAttribute name\nKeyword argument\nPurpose\n\n\n\n\ncolor\nedge.color\nColor of the edge\n\n\ncurved\nedge.curved\nA numeric value specifies the curvature of the edge; zero curvature means straight edges, negative values means the edge bends clockwise, positive values the opposite. TRUE means curvature 0.5, FALSE means curvature zero\n\n\narrow.size\nedge.arrow.size\nCurrently this is a constant, so it is the same for every edge. If a vector is submitted then only the first element is used, ie. if this is taken from an edge attribute then only the attribute of the first edge is used for all arrows.\n\n\narrow.width\nedge.arrow.width\nThe width of the arrows. Currently this is a constant, so it is the same for every edge\n\n\nwidth\nedge.width\nWidth of the edge in pixels\n\n\nlabel\nedge.label\nIf specified, it adds a label to the edge.\n\n\nlabel.cex\nedge.label.cex\nFont size of the edge label, interpreted as a multiplicative factor, similarly to R’s text function\n\n\nlabel.color\nedge.label.color\nColor of the edge label\n\n\nlabel.family\nedge.label.family\nFont family of the edge, similarly to R’s text function\n\n\nlabel.font\nedge.label.font\nFont within the font family of the edge, similarly to R’s text function\n\n\n\n\n\nGeneric arguments of plot()\nThese settings can be specified as arguments to the plot function to control the overall appearance of the plot.\n\n\n\n\n\n\n\nKeyword argument\nPurpose\n\n\n\n\nlayout\nThe layout to be used. It can be an instance of Layout, a list of tuples containing X-Y coordinates, or the name of a layout algorithm. The default is auto, which selects a layout algorithm automatically based on the size and connectedness of the graph.\n\n\nmargin\nThe amount of empty space below, over, at the left and right of the plot, it is a numeric vector of length four."
  },
  {
    "objectID": "BCB520/posts/A8-NetworkData/index.html#assignment",
    "href": "BCB520/posts/A8-NetworkData/index.html#assignment",
    "title": "Practice with Network Data",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nImprove the network above by:\n\nColoring the edges according to Advisor / BCB520 attribute.\nColoring the nodes according to Department.\nAdujsting the labels to improve readability."
  },
  {
    "objectID": "BCB520/posts/A3-PrototypeVizPortfolio/index.html",
    "href": "BCB520/posts/A3-PrototypeVizPortfolio/index.html",
    "title": "ASSIGNMENT 3",
    "section": "",
    "text": "Enough with the theory and conceptual mumbo jumbo! Let’s get down to making a visualization and posting it somewhere for all the world to see! The basic idea of this assignment is to set up a repository that will serve as an experimental portfolio, and then create your first novel visualization element inside the portfolio. Its gonna be fun! Or at least educational…"
  },
  {
    "objectID": "BCB520/posts/A3-PrototypeVizPortfolio/index.html#assignment",
    "href": "BCB520/posts/A3-PrototypeVizPortfolio/index.html#assignment",
    "title": "ASSIGNMENT 3",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThis assignment has two parts. The first part is technical. We’ll set up a Quarto Blog project as a new repository in your GitHub account. Then you’ll be a Blogger! Prestigious! The second part should be more fun. We are going to create your first Blog post as a visualization that explores an ACTION - TARGET pair relevant to your data set from Assignment 2."
  },
  {
    "objectID": "BCB520/posts/A3-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "href": "BCB520/posts/A3-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "title": "ASSIGNMENT 3",
    "section": "PART 1 TECHNICAL SORCERY",
    "text": "PART 1 TECHNICAL SORCERY\n\n1A - Create your BLOG project.\nHopefully by now you have created / dusted off / logged in to your GitHub account. Go ahead and log in to your account on the web and leave it open in a tab in your browser. There is a really great video about the next few steps (also linked below in RESORUCES) from Posit, but I’m giving you the condensed version here.\n\nFire up RStudio.\nGo to File-&gt;New Project and then select NEW DIRECTORY.\nNow select QUARTO BLOG.\nYou are going to create a the project in a new working directory. It is best practice to put this in a senstible directory structure on your local hard drive where your other GitHub repositories also live. Here is what mine looks like:\n\n 4. I suggest making the directory name something informative, like BCB504Portfolio, but hey… If you want to call your repository HasturBoxerShorts I won’t stop you. 5. Most of you will select Knitr as your Engine, but Cody “Mr. Hacker McPythonPants” might select Jupyter. 6. Check Create a git repository. The other boxes are optional and we can talk more about them later. 7. Click CREATE PROJECT.\nNow you’ve got a BLOG template all set up! Ha Ha! Onward to Internet Fame!\n\n\n1B - Make the BLOG about you.\nWe won’t spend a ton of time here, because this will be an ongoing process. You’ll go and watch all those cool videos and tutorials this weekend to figure this out. But lets do a couple things.\n\nMaybe you should modify the about.qmd file so that your name is in there somewhere.\nMaybe you should modify the index.qmd file with a better title in the YAML header.\nMaybe you should navigate to the posts folder, open the Welcome to my blog folder, open index.qmd from that directory, and add a sentence or two.\n\n\n\n1C - Customize your first post.\n\nNavigate to the posts folder, open the post with code folder, and open index.qmd.\nReplace ALL of the content of index.qmd with the most recent version of your .qmd file from ASSIGNMENT 2. Keep the file name index.qmd. Save that file!\nMove your data files to the post with code folder.\nRender the index.qmd file from this folder. Hopefully it worked!\n\n\n\n1D - Render the BLOG as a website.\n\nIMPORTANT Open your _quarto.yml file and add output-dir: docs under project:\n\n\nThe indentations matter here.\n\nSave all the files you’ve modified.\nGo to the BUILD tab in the (probably) top right section of RStudio.\nClick RENDER WEBSITE.\nClick through your new Blog and see how it works!\n\n\n\n1E - Push to GitHub.\nThere are quite a few ways to do this part. I’m going to use GitHub Desktop, but those video will show you other ways.\n\nGo to GitHub Desktop.\nType some text in the summary box.\nClick COMMIT TO MASTER.\nClick PUSH ORIGIN.\nGo to your GitHub in your browser. You should see your new repository! Yay!\n\n\n\n1F - Make it a website with GitHub pages.\n\nIn your browser, click on your repository.\nGo to SETTINGS.\nSelect PAGES.\nSet the SOURCE option to Deploy from a branch.\nSet the BRANCH to master and the directory to docs\nDeploy that stuff and wait. Then visit your site!"
  },
  {
    "objectID": "BCB520/posts/A3-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "href": "BCB520/posts/A3-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "title": "ASSIGNMENT 3",
    "section": "PART 2 ELDRITCH VISUALIZATION RITUAL",
    "text": "PART 2 ELDRITCH VISUALIZATION RITUAL\n\n2A Define your ACTION - TARGET pair(s)\nIn [LECTURE 3] we discussed the concept of Task Abstraction in which you define the viz task that you want to help the user accomplish. This was represented as sets of ACTIONS that the user would perform (e.g. Discover, Present, Browse, Identify) on TARGETS related to the data set (e.g. Trends, Attributes, etc.).\nThink about one or two visualizations you wish to construct with your data, and try to define them in terms of ACTION - TARGET pairs. While you are at it, why don’t you update the index.qmd file of your BLOG POST with a new seciton at the bottom titled TASK ABSTRACTION, and put a sentence describing your visualizations and the ACTION - TARGET pairs they represent?\n\n\n2B Construct your Visualization\nLet’s get to work! Using whatever tools you can, code up your visualization in that new section of your BLOG post. You can check out how I approached this part in TUTORIAL 4."
  },
  {
    "objectID": "BCB520/posts/A3-PrototypeVizPortfolio/index.html#resources",
    "href": "BCB520/posts/A3-PrototypeVizPortfolio/index.html#resources",
    "title": "ASSIGNMENT 3",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nQuarto’s BLOG Documentation\nA YouTube Video from Posit on Building a BLOG with Quarto"
  },
  {
    "objectID": "BCB520/posts/A6-Midterm/index.html",
    "href": "BCB520/posts/A6-Midterm/index.html",
    "title": "Midterm Assignment",
    "section": "",
    "text": "This assignment provides you the opportunity to synthesize all of the concepts we’ve covered in the course to date. The basic framework is that you will create a COMPLETE data visualization BLOG post that is suitable as a showcase component of your Data Science Portfolio. The point is to SHOW people your skills."
  },
  {
    "objectID": "BCB520/posts/A6-Midterm/index.html#structure",
    "href": "BCB520/posts/A6-Midterm/index.html#structure",
    "title": "Midterm Assignment",
    "section": "STRUCTURE",
    "text": "STRUCTURE\nThe basic formatting guidelines for this assignment are:\n\nInclude code fold or code tools options (or both) that allow users to view and copy your code while maintaining overall readability of your post.\nSuppress all output and warnings that might distract from your visualizations and writing.\nProperly title your assignment. The main title should be “BCB 520 - Midterm Portfolio Post”, and the subtitle should be a descriptive title related to your question or topic.\nInclude author, date, categories, and a description in your YAML header.\nWrite clear, complete sentences for a target audience with some scientific background but little training in your specific discipline.\nInclude references if appropriate and use hyperlinks to external sources of data, inspiration, or examples.\nUse the header hierarchy and create a sensible document outline with white space. Format for readability! Use bold and italic fonts to emphasize things! Use color by customizing your .css file!\n\nIn addition to the above formatting guidelines, your portfolio post must contain the following sections:\n\nPreamble\nWrite a brief paragraph describing the primary question or purpose of the post. Ideally, the concept should be challenging enough that it requires at least two visualizations that use different idioms (ie. don’t just make two scatterplots with different variables). The concept should also be challenging enough that it captures the interest of the reader (i.e. a plot of height and weight that shows they are correlated is trivial and not appropriate). The best approach is to explore a topic or question in which YOU are very interested.\n\n\nData\nWrite a summary of the data sources you will use. Include a Data Dictionary table that fully describes each individual data file used. You may use your own research data or publicly available data from any source you like (with attribution). There aren’t any minimum or maximum data set size requirments, other than you need something big enough to be interesting and not so big that we don’t have a supercomputer capable of creating your visualization.\n\n\nVisualizations\nCreate your visualizations and include text that explains any steps or design choices. Be sure to include clearly labeled axes and a concise but complete figure caption for each visualization. Make deliberate choices for color palettes, point marks, line types, etc. Demonstrate that you understand the concepts we have covered!\n\n\nConclusions or Summary\nAnswer your research question. Draw a conclusion or inference related to your topic. Summarize your results. What new questions have emerged as a result of your visualizations? What interesting next steps have emerged?"
  },
  {
    "objectID": "BCB520/posts/A6-Midterm/index.html#rubric",
    "href": "BCB520/posts/A6-Midterm/index.html#rubric",
    "title": "Midterm Assignment",
    "section": "RUBRIC",
    "text": "RUBRIC\nI will evaluate the following for your portfolio post:\n1. Clarity of writing (15%): Complete, clear sentences. Good Grammar. Understandable to target audience. Logical flow of ideas.\n2. Adherence to format (10%): Did you follow directions?\n3. Topic suitability (15%): Is the topic interesting? Are the visualizations challenging and interesting enough to showcase your skills?\n4. Viz Execution (40%): Are the visualizations effective? Do they adhere to the principles of effectiveness? Are choices for idiom, marks, channels, etc made deliberately and well justified?\n5. Creativity (20%): Did you push your boundaries and learn new techniques? Is the overall post compelling and interesting? Are the visualizations inspiring, creative, unique, and generally impressive? If I were recruiting a new data scientist (and I often am), would this portfolio post impress me, or would it damage your candidacy during review?"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#the-three-major-data-types",
    "href": "BCB520/posts/L6-TabularData1/index.html#the-three-major-data-types",
    "title": "LECTURE 6 - Tabular Data",
    "section": "THE THREE MAJOR DATA TYPES",
    "text": "THE THREE MAJOR DATA TYPES"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#what",
    "href": "BCB520/posts/L6-TabularData1/index.html#what",
    "title": "LECTURE 6 - Tabular Data",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#semantics-for-data",
    "href": "BCB520/posts/L6-TabularData1/index.html#semantics-for-data",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SEMANTICS FOR DATA",
    "text": "SEMANTICS FOR DATA\n\n\n\nSemantics\n\nitem: individual entity, discrete\n\neg patient, car, stock, city\n“independent variable”\n\nattribute: property that is measured, observed, logged…\n\neg height, blood pressure for patient\neg horsepower, make for car\n“dependent variable”\n\n\n\n\nData Table\n\n\n\n\nITEM: Person\nATTRIBUTES: Name, Age, Shirt Size, Favorite Fruit"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#dataset-types-tables",
    "href": "BCB520/posts/L6-TabularData1/index.html#dataset-types-tables",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DATASET TYPES: TABLES",
    "text": "DATASET TYPES: TABLES\nFlat Table\n\n\n\nOne ITEM per row\n\noften called an observation\n\nEach column is an ATTRIBUTE\n\noften called a variable\n\nA cell holds the VALUE for an item/attribute pair\nA unique KEY can be used (implicitly or explicitly) to identify each item even if they share all measured attributes"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#flat-table-example",
    "href": "BCB520/posts/L6-TabularData1/index.html#flat-table-example",
    "title": "LECTURE 6 - Tabular Data",
    "section": "FLAT TABLE EXAMPLE",
    "text": "FLAT TABLE EXAMPLE\n\n\n\n\n\n\n\nA dumb spreadsheet\n\n\n\n\n\n\n\nA cool spreadsheet\n\n\n\n\nFigure 1: Flat Tables"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#multidimensional-tables",
    "href": "BCB520/posts/L6-TabularData1/index.html#multidimensional-tables",
    "title": "LECTURE 6 - Tabular Data",
    "section": "MULTIDIMENSIONAL TABLES",
    "text": "MULTIDIMENSIONAL TABLES\nIndexing based on multiple keys (eg genes, patients)"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#keys-and-values",
    "href": "BCB520/posts/L6-TabularData1/index.html#keys-and-values",
    "title": "LECTURE 6 - Tabular Data",
    "section": "KEYS AND VALUES",
    "text": "KEYS AND VALUES\n\n\nKEY: an independent attribute used as unique index to look up items.\n\nSimple tables: 1 key\nMultidimensional tables: multiple keys\n\nValue: a dependent attribute, value of cell\nWe will classify visualization idioms by the number of keys used.\n0, 1, 2, …"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#framework",
    "href": "BCB520/posts/L6-TabularData1/index.html#framework",
    "title": "LECTURE 6 - Tabular Data",
    "section": "FRAMEWORK",
    "text": "FRAMEWORK"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-scatterplot",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-scatterplot",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Scatterplot",
    "text": "IDIOM: Scatterplot\n0 KEYS: Only values are available or salient to the task.\n\n\nExpress values (magnitudes) of quantitative attributes.\nData: 2 quantitative attributes\nMark: points\nChannels: horizontal and verical position\nTasks: find trends, outliers, distribution, correlation, clusters\nScalability: hundreds of items\n\n\n\n\n\n\nHeight and Weight of drafted NHL players."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-scatterplot-1",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-scatterplot-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Scatterplot",
    "text": "IDIOM: Scatterplot\n0 KEYS: Only values are available or salient to the task.\n\n\nExpress values (magnitudes) of quantitative attributes.\nData: 2 quantitative attributes\nMark: points\nChannels: horizontal and verical position\nTasks: find trends, outliers, distribution, correlation, clusters\nScalability: hundreds of items\n\n\n\n\n\n\nHeight and Weight of drafted NHL players."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#scatterplots-more-channels",
    "href": "BCB520/posts/L6-TabularData1/index.html#scatterplots-more-channels",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SCATTERPLOTS: More channels",
    "text": "SCATTERPLOTS: More channels\nAdditional channels are viable with scatterplots since we are using point marks.\n\n\nExamples:\nColor: But pay attention to Discriminability!\nSize: But pay attention to using a quantitative attribute used to control 2D area. Directly encoding radius would mislead. Take the square root since area grows quadratically.\nShape:\n\n\n\n\n\n\nHeight and Weight of drafted NHL goaltenders."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#scatterplot-tasks",
    "href": "BCB520/posts/L6-TabularData1/index.html#scatterplot-tasks",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SCATTERPLOT TASKS",
    "text": "SCATTERPLOT TASKS\nCorrelation\n\nClusters/groups, and clusters vs classes"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#keys-categorical-regions",
    "href": "BCB520/posts/L6-TabularData1/index.html#keys-categorical-regions",
    "title": "LECTURE 6 - Tabular Data",
    "section": "KEYS: Categorical Regions",
    "text": "KEYS: Categorical Regions\n\n\n\nRegions: contiguous bounded areas distinct from each other\n\nNO OVERPLOTTING: separate into spatial regions: one mark per region (for now)\n\nuse categorical or ordered attribute to separate into regions\n\nno conflict with expressiveness principle for categorical attributes\n\nuse ordered attribute to order and align regions"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#separated-and-aligned-and-ordered",
    "href": "BCB520/posts/L6-TabularData1/index.html#separated-and-aligned-and-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated and aligned and ordered",
    "text": "Separated and aligned and ordered\nBest case!"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#separatedaligned---not-ordered",
    "href": "BCB520/posts/L6-TabularData1/index.html#separatedaligned---not-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated/aligned - not ordered",
    "text": "Separated/aligned - not ordered\nLimitation: hard to know rank. what’s 4th? what’s 7th?"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#separated-but-not-aligned-or-ordered",
    "href": "BCB520/posts/L6-TabularData1/index.html#separated-but-not-aligned-or-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated but not aligned or ordered",
    "text": "Separated but not aligned or ordered\nLimitation: hard to make comparisons with size (vs aligned position)"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-bar-chart",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Bar Chart",
    "text": "IDIOM: Bar Chart\nOne key, One value\n\n\nData: 1 categorical attribute, 1 quantitative attribute\nMark: Lines\nChannels: Length to express quantitative value\nSpatial regions: one per mark - separated horizontally, aligned vertically. Ordered by quantitative attribute: by label (alphabetical), by length attribute (data-driven)\nTasks: compare, lookup values\nScalability: dozens to hundreds of levels for key attribute (bars), hundreds for values."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-stacked-bar-chart",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-stacked-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Stacked Bar Chart",
    "text": "IDIOM: Stacked Bar Chart\nTwo keys, One value\n\n\nData: 2 categorical attribute, 1 quantitative attribute\nMark: vertical stack of line marks\nGlyph: composite object, internal structure from multiple marks\nChannels: length and color hue\nSpatial regions: one per glyph\nAligned: full glyph, lowest bar component\nUnaligned: other bar components\nTask: part-to-whole relationship\nScalability: asymmetric for stacked key attrib, 10-12 levels segments for main key attrib, dozens to hundreds of levels bars"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-streamgraph",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-streamgraph",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Streamgraph",
    "text": "IDIOM: Streamgraph\nGeneralized stacked graph emphasizing horizontal continuity vs vertical items (example)\n\n\nData: 1 categ key attrib (movies) 1 ordered key attrib (time) 1 quant value attrib (counts) derived data geometry: layers, where height encodes counts 1 quant attrib (layer ordering)\nMark: vertical stack of line marks\nChannels: length and color hue\nTask: part-to-whole relationship\nScalability: hundreds of time keys dozens to hundreds of movies keys more than stacked bars: most layers don’t extend across whole chart"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-dotline-chart",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-dotline-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Dot/Line Chart",
    "text": "IDIOM: Dot/Line Chart\nOne key, One value\n\n\nData:2 quant attribs\nMark: points AND line connection marks between them\nChannels: aligned lengths to express quant value separated and ordered by key attrib into horizontal regions\nTask: find trend connection marks emphasize ordering of items along key axis by explicitly showing relationship between one item and the next\nScalability: hhundreds of key levels, hundreds of value levels"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#bar-vs-line-charts",
    "href": "BCB520/posts/L6-TabularData1/index.html#bar-vs-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "BAR vs LINE CHARTS",
    "text": "BAR vs LINE CHARTS\nChoice depends on the type of key attributes.\nBar charts if categorical, Line charts if ordered. Do not use line charts for categorical key attributes as it violates the expressiveness principle. The implication of trend is so strong that it overrides semantics. “The more male a person is, the taller he/she is”"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#chart-axes",
    "href": "BCB520/posts/L6-TabularData1/index.html#chart-axes",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CHART AXES",
    "text": "CHART AXES\n\n\nBest practice to label axes, with few exceptions. Individual small multiple views could share axis label.\nInclude 0 at bottom left or the slope misleads. There are some exceptions (arbitrary 0, small change matters).\nTruncating the Y-Axis: Threat or Menace? Correll, Bertini, & Franconeri, CHI 2020."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-indexed-line-charts",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-indexed-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Indexed Line Charts",
    "text": "IDIOM: Indexed Line Charts\n\n\nData: 2 quant attribs 1 key + 1 value\nDerived data: new quant value attrib index plot instead of original value\nTask: show change over time\nPrinciple: normalized, not absolute\nScalability: same as standard line chart\n\n\nCool Example"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-gantt-charts",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-gantt-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Gantt Charts",
    "text": "IDIOM: Gantt Charts\none key, two (related) values\n\n\nData: 1 categ attrib, 2 quant attribs\nMark: line length: duration\nChannels: horiz position: start time (+end from duration)\nTask: mphasize temporal overlaps & start/end dependencies between items\nScalability: dozens of key levels bars hundreds of value levels durations"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-slopegraphs",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-slopegraphs",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Slopegraphs",
    "text": "IDIOM: Slopegraphs\ntwo values\n\n\nData: 2 quant value attribs (1 derived attrib: change magnitude)\nMark: point + line line connecting mark between pts\nChannels: 2 vertical pos: express attrib value (linewidth/size, color)\nTask: emphasize changes in rank/value\nScalability: hundreds of value levels dozens of item"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#keys",
    "href": "BCB520/posts/L6-TabularData1/index.html#keys",
    "title": "LECTURE 6 - Tabular Data",
    "section": "2 KEYS",
    "text": "2 KEYS"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#idiom-heatmap",
    "href": "BCB520/posts/L6-TabularData1/index.html#idiom-heatmap",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Heatmap",
    "text": "IDIOM: Heatmap\ntwo keys, one value\n\n\nData: 2 categ attribs (gene, experimental condition) 1 quant attrib (expression levels)\nMark: point separate and align in 2D matrix indexed by 2 categorical attributes\nChannels: color by quant attrib (ordered diverging colormap)\nTask: find clusters, outliers\nScalability: 1M items, 100s of categ levels, ~10 quant attrib levels"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#heatmap-reordering",
    "href": "BCB520/posts/L6-TabularData1/index.html#heatmap-reordering",
    "title": "LECTURE 6 - Tabular Data",
    "section": "HEATMAP REORDERING",
    "text": "HEATMAP REORDERING"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#clustered-heatmap",
    "href": "BCB520/posts/L6-TabularData1/index.html#clustered-heatmap",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CLUSTERED HEATMAP",
    "text": "CLUSTERED HEATMAP\nUse derived data: compute 2 cluster hierarchies and represent as a dendrogram. Parent-child relationships represented in tree with connection line marks, leaves aligned so interior branch heights are easy to compare. Heatmap marks are (re-)ordered by cluster hierarchy traversal. Task: assess quality of clusters found by automatic methods"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#axis-orientation",
    "href": "BCB520/posts/L6-TabularData1/index.html#axis-orientation",
    "title": "LECTURE 6 - Tabular Data",
    "section": "AXIS ORIENTATION",
    "text": "AXIS ORIENTATION"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#discuss",
    "href": "BCB520/posts/L6-TabularData1/index.html#discuss",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DISCUSS",
    "text": "DISCUSS"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#radial-bar-chart",
    "href": "BCB520/posts/L6-TabularData1/index.html#radial-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADIAL BAR CHART",
    "text": "RADIAL BAR CHART\n\n\n\n\nStar plot: line mark, radial axes meet at central point\nRadial bar chart: line mark, radial axes meet at central ring\nChannels: length, angle/orientation\nBar chart: rectilinear axes, aligned vertically\nAccuracy: length not aligned with radial layouts, making them less accurately perceived than rectilinear aligned layouts.\nVismon: Facilitating Risk Assessment and Decision Making In Fisheries Management. Booshehrian, Möller, Peterman, and Munzner. Technical Report TR 2011-04, Simon Fraser University, School of Computing Science, 2011."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#radar-plot",
    "href": "BCB520/posts/L6-TabularData1/index.html#radar-plot",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADAR PLOT",
    "text": "RADAR PLOT\nRadial line chart with point marks, radial layout, and connecting line marks. Avoid unless data are cyclic."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#radar-plot-example",
    "href": "BCB520/posts/L6-TabularData1/index.html#radar-plot-example",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADAR PLOT EXAMPLE",
    "text": "RADAR PLOT EXAMPLE\n\nTheFunctionalArt"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#pie-and-coxcomb-charts",
    "href": "BCB520/posts/L6-TabularData1/index.html#pie-and-coxcomb-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE AND COXCOMB CHARTS",
    "text": "PIE AND COXCOMB CHARTS\n1 categorical key attribute, 1 quantitative value attribute\n\n\nPie chart: interlocking area marks with angle channel causes variation in 2D area. Keys are separated & ordered radially with uniform height. accuracy: area less accurate than rectilinear aligned line length.\nTask: part-to-whole judgements.\nCoxcomb chart: line marks with length channel, creating variation in only 1D length. Keys are separated & ordered radially with uniform width. These charts are a direct analog to radial bar charts."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#nightengale-rose",
    "href": "BCB520/posts/L6-TabularData1/index.html#nightengale-rose",
    "title": "LECTURE 6 - Tabular Data",
    "section": "NIGHTENGALE ROSE",
    "text": "NIGHTENGALE ROSE\nCoxcomb / Nightengale Rose/ Polar Area Chart Invented by Florence Nightingale."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#coxcomb-perception",
    "href": "BCB520/posts/L6-TabularData1/index.html#coxcomb-perception",
    "title": "LECTURE 6 - Tabular Data",
    "section": "COXCOMB: Perception",
    "text": "COXCOMB: Perception\n\n\nEncode: 1D length\nDecode/perceive: 2D area\nThe nonuniform relationship between line/sector width and mark length causes area variation to scale nonlinearly with line mark length!\nA standard bar chart is safer: bars are uniform width, so area is linear with line mark length in both radial & rectilinear cases."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#pie-charts-perception",
    "href": "BCB520/posts/L6-TabularData1/index.html#pie-charts-perception",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE CHARTS: Perception",
    "text": "PIE CHARTS: Perception\n\n\nSome empirical evidence that people respond to arc length.\nDecode/perceive: arc length, maybe also areas, but not angles.\nDonut charts are no worse than pie charts.\n\n\n\n\nArcs, Angles, or Areas: Individual Data Encodings in Pie and Donut Charts. Skau and Kosara. Proc. EuroVis 2016\nExplore this concept"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#pie-charts-best-practices",
    "href": "BCB520/posts/L6-TabularData1/index.html#pie-charts-best-practices",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE CHARTS: Best Practices",
    "text": "PIE CHARTS: Best Practices\nNot so bad for two (or few) levels and part-to-whole tasks. Dubious for several levels if details matter. Terrible for many levels."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#normalized-stacked-bar",
    "href": "BCB520/posts/L6-TabularData1/index.html#normalized-stacked-bar",
    "title": "LECTURE 6 - Tabular Data",
    "section": "NORMALIZED STACKED BAR",
    "text": "NORMALIZED STACKED BAR\n\n\nTask: part-to-whole judgements\nNormalized stacked bar chart: stacked bar chart, normalized to full vertical height. A single stacked bar is equivalent to a full pie chart.\nHigh information density can be achieved with narrow rectangles. Pie chart information density is much more limited and requires a large circle."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#glyphmaps",
    "href": "BCB520/posts/L6-TabularData1/index.html#glyphmaps",
    "title": "LECTURE 6 - Tabular Data",
    "section": "GLYPHMAPS",
    "text": "GLYPHMAPS\nRectilinear glyphs are good for linear vs nonlinear trends. Radial glyphs are good for cyclic patterns and evaluating periodicity.\n\nGlyph-maps for Visually Exploring Temporal Patterns in Climate Data and Models. Wickham, Hofmann, Wickham, and Cook. Environmetrics 23:5 (2012), 382–393."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#parallel-axes",
    "href": "BCB520/posts/L6-TabularData1/index.html#parallel-axes",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL AXES",
    "text": "PARALLEL AXES"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#splom",
    "href": "BCB520/posts/L6-TabularData1/index.html#splom",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SPLOM",
    "text": "SPLOM\n\n\nScatterplot matrix (SPLOM): Rectilinear axes with point mark. All possible pairs of axes are visualized.\nScalability: about a dozen attributes and dozens to hundreds of items."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#parallel-coordinates",
    "href": "BCB520/posts/L6-TabularData1/index.html#parallel-coordinates",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES",
    "text": "PARALLEL COORDINATES\nScatterplot limitation: visual representation with orthogonal axes can show only two attributes with spatial position channel."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#parallel-coordinates-1",
    "href": "BCB520/posts/L6-TabularData1/index.html#parallel-coordinates-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES",
    "text": "PARALLEL COORDINATES\n\n\nAlternative: Line up axes in parallel to show many attributes with position. Items are encoded with a line with n segments (n is the number of attributes shown). Ordering is a major challenge.\nScalability: dozens of attributes and hundreds of items."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#parallel-coordinates-limitations",
    "href": "BCB520/posts/L6-TabularData1/index.html#parallel-coordinates-limitations",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES: Limitations",
    "text": "PARALLEL COORDINATES: Limitations\nPatterns only visible between neighboring axis pairs. How to pick axis order? The usual solution is reorderable axes and interactive exploration. The downside of interaction is human-powered search.\n\nAs usual, OBSERVABLE really shines for these types of interactive plots!"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#orientation-limitations",
    "href": "BCB520/posts/L6-TabularData1/index.html#orientation-limitations",
    "title": "LECTURE 6 - Tabular Data",
    "section": "ORIENTATION LIMITATIONS",
    "text": "ORIENTATION LIMITATIONS\n\n\nRectilinear: Scalability is limited with regard to the number of axes. (2 axes best, 3 problematic, 4+ impossible.)\nParallel: Unfamiliarity and training time.\nRadial: Perceptual limits include polar coordinate asymmetry, lower precision with angle channles compated to length channels, nonuniform sector width/size depending on radial distance. While thes limits are frequently problematic, they can sometimes be deliberately exploited. (Example: for 2 attribs of very unequal importance)\n\n\n\n\nUncovering Strengths and Weaknesses of Radial Visualizations - an Empirical Approach. Diehl, Beck and Burch. IEEE TVCG (Proc. InfoVis) 16(6):935–942, 2010."
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#chart-axes-1",
    "href": "BCB520/posts/L6-TabularData1/index.html#chart-axes-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CHART AXES",
    "text": "CHART AXES\n\n\nLabelled axes are critical! Avoid cropping the y-axis (include 0 at bottom left) or the slope misleads the viewer.\n\n\n\n\nhttp://www.thefunctionalart.com/2015/10/if-you-see-bullshit-say-bullshit.html"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#dual-axis-line-charts",
    "href": "BCB520/posts/L6-TabularData1/index.html#dual-axis-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DUAL AXIS LINE CHARTS",
    "text": "DUAL AXIS LINE CHARTS\nThis approach is controversial. Dual axes are acceptable if they are commensurate, but beware, as they make it very easy to mislead!"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#connected-scatterplots",
    "href": "BCB520/posts/L6-TabularData1/index.html#connected-scatterplots",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CONNECTED SCATTERPLOTS",
    "text": "CONNECTED SCATTERPLOTS\nScatterplot with line connection marks. These are popular in journalism.\n\n\nHoriz + vert axes: value attributes\nLine connection marks: temporal order\nAlternative to dual-axis charts (horiz: time vert: two value attributes).\nempirical study suggests these are engaging, but correlations are unclear.\n\n\n\n\nhttp://steveharoz.com/research/connected_scatterplot/"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#breaking-conventions",
    "href": "BCB520/posts/L6-TabularData1/index.html#breaking-conventions",
    "title": "LECTURE 6 - Tabular Data",
    "section": "BREAKING CONVENTIONS",
    "text": "BREAKING CONVENTIONS\nThe inverted y axis is evocative of blood dripping down on Poe.\n\nhttps://public.tableau.com/profile/ben.jones#!/vizhome/EdgarAllanPoeViz/EdgarAllanPoeViz"
  },
  {
    "objectID": "BCB520/posts/L6-TabularData1/index.html#arranging-tabular-data",
    "href": "BCB520/posts/L6-TabularData1/index.html#arranging-tabular-data",
    "title": "LECTURE 6 - Tabular Data",
    "section": "ARRANGING TABULAR DATA",
    "text": "ARRANGING TABULAR DATA\nWhat did we miss?\nhttps://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html",
    "href": "BCB520/posts/T7-NHLDraft/index.html",
    "title": "TUTORIAL 7",
    "section": "",
    "text": "In this assignment, we are going to practice creating visualizations for tabular data. Unlike previous assignments, however, this time we will all be using the same data sets. I’m doing this because I want everyone to engage in the same logic process and have the same design objectives in mind."
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#scenario",
    "href": "BCB520/posts/T7-NHLDraft/index.html#scenario",
    "title": "TUTORIAL 7",
    "section": "SCENARIO",
    "text": "SCENARIO\nImagine you are a high priced data science consultant. One of your good friends, Cassandra Canuck, is an Assistant General Manager for the Vancouver Canucks, a team in the National Hockey League with a long, long…. long history of futility.\nCassandra tells you her boss, General Manager Hans Doofenschmirtz, is considering trading this year’s first round draft pick for two second round picks and one third round pick from another team. For the purposes of this exercise, let’s set the 2023 NHL draft order using the Tankathon Simulator. The NHL uses a lottery system in which the teams lowest in the standings have the highest odds of getting the first overall pick. I’ll simulate the lottery now…\nHOLY CRAP! The Vancouver Canucks jump up 6 spots, and will pick FIRST overall. Here is a screenshot:\n\nOur official scenario is this:\nVancouver receives: The 7th pick in the second round (39th overall), the 10th pick in the second round (42nd overall), and the 10th pick in the third round (74th overall).\nDetroit receives: The 1st pick in the first round (1st overall).\nDoofenschmirtz reasons that more draft picks are better, and is inclined to make the trade. Cassandra isn’t so sure…\nShe asks you to create some data visualizations she can show to her boss that might help him make the best decision."
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#directions",
    "href": "BCB520/posts/T7-NHLDraft/index.html#directions",
    "title": "TUTORIAL 7",
    "section": "DIRECTIONS",
    "text": "DIRECTIONS\nCreate a new post in your portfolio for this assignment. Call it something cool, like NHL draft analysis, or Hockey Analytics, or John Wick….\nCopy the data files from the repository, and maybe also the .qmd file.\nUse the .qmd file as the backbone of your assignment, changing the code and the markdown text as you go."
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#the-data",
    "href": "BCB520/posts/T7-NHLDraft/index.html#the-data",
    "title": "TUTORIAL 7",
    "section": "THE DATA",
    "text": "THE DATA\nHow can we evaluate whether trading a first round pick for two second round picks and a third round pick is a good idea? One approach is to look at the historical performance of players from these draft rounds.\nI’ve created a data set that will allow us to explore player performance as a function of draft position. If you are curious as to how I obtained and re-arranged these data, you can check out that tutorial here. For this assignment, though, I want to focus on the visualizations.\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\n\nNHLDraftstats&lt;-read.csv(\"NHLDraftstats.csv\")\nNHLDictionary&lt;-read_excel(\"NHLDictionary.xlsx\")\nSkateWAR&lt;-read_excel(\"Seasonal Skater WAR.xlsx\")\n\nknitr::kable(NHLDictionary)\n\n\nIn this case, we have a dataframe with all the drafted players since 2000, their position, their draft year and position, and then rows for each season since being drafted (postdraft). The key variable here is NHLgames, which tells us how many games they played in the NHL each season since being drafted."
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#data-cleaning",
    "href": "BCB520/posts/T7-NHLDraft/index.html#data-cleaning",
    "title": "TUTORIAL 7",
    "section": "DATA CLEANING",
    "text": "DATA CLEANING\n\n\nCode\nWAR&lt;-SkateWAR%&gt;%\n  group_by(Player, Season)%&gt;%\n  summarise(TOI=sum(TOI),WAR=sum(WAR))%&gt;%\n  mutate(seasonshort=2000+as.numeric(str_sub(Season, start=1, end=2)),\n         name=Player)\n\ndraftpos&lt;-NHLDraftstats%&gt;%\n  select(name, round, overall, draftyear, position)%&gt;%\n  group_by(name, round, overall, draftyear, position)\n\ndraftposWAR &lt;- full_join(WAR, draftpos, by = \"name\")\n\ndraftposWAR2&lt;- unique(draftposWAR)%&gt;%\n  mutate(postdraft=seasonshort-draftyear)%&gt;%\n  filter(postdraft&gt;-1)"
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#simple-scatterplot",
    "href": "BCB520/posts/T7-NHLDraft/index.html#simple-scatterplot",
    "title": "TUTORIAL 7",
    "section": "SIMPLE SCATTERPLOT",
    "text": "SIMPLE SCATTERPLOT\nOne thing to realize about professional hockey is that it is pretty rare for a player to play in the NHL right after being drafted. Players get drafted when they are 18 years old, and they usually play in the juniors, minor leagues, or the NCAA for a bit to further develop. Let’s use a scatterplot to visualize this phenomenon with the most recent draft classes."
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#expanded-scatterplot",
    "href": "BCB520/posts/T7-NHLDraft/index.html#expanded-scatterplot",
    "title": "TUTORIAL 7",
    "section": "EXPANDED SCATTERPLOT",
    "text": "EXPANDED SCATTERPLOT\nThe data from the most recent draft isn’t really helpful for our question. Let’s go back in time and use a draft year that has had some time to develop and reach their potential. How about 2018?\n\n\nCode\ndraft2018&lt;-draftposWAR2%&gt;%\n  filter(draftyear==2018 & postdraft&lt;6)\n\nggplot(draft2018, aes(x=round, y=WAR))+\n  geom_point()\n\nggplot(draftposWAR2, aes(x=postdraft, y = WAR))+\n  geom_smooth(aes(color=as.factor(round)))\n\n\nHmmm… in addition to the problem of overplotting, we’ve got an additional issue here. We actually have two keys and one attribute. The attribute is NHLgames, and the keys are round and postdraft, but we are only using round.\nPostdraft indicates the number of seasons after being drafted. We have several choices here. We can make a visualization that uses both keys, or we can somehow summarize the data for one of the keys.\nFor example, let’s say we just wanted to know the TOTAL number of NHL games played since being drafted.\n\n\nCode\ndrafttot2018&lt;- draft2018%&gt;%\n  group_by(playerId, round, overall, position, name)%&gt;%\n  summarise(totgames=sum(NHLgames), totpoints=sum(points))\n\nggplot(drafttot2018, aes(x=round, y=totpoints))+\n  geom_point()\n\ndrafttot&lt;- NHLDraftstats%&gt;%\n  group_by(playerId, round, overall, position, name)%&gt;%\n  summarise(totgames=sum(NHLgames), totpoints=sum(points))\n\nggplot(NHLDraftstats, aes(x=postdraft, y=points))+\n  geom_smooth(aes(color=as.factor(round)))\n\nggplot(draftposWAR2%&gt;%filter(postdraft&lt;6), aes(x=WAR))+\n  geom_histogram()+\n  facet_grid(postdraft~round)\n\n\nFine, I guess, but we still have to deal with overplotting, and think about whether a scatterplot really helps us accomplish our task. For this figure do the following:\n\nOverplotting. All those points on the y=0 line represent about 32 players each. Can you you think of a way that adding extra channels might help?\nLabelling. Can we create a solid figure caption and better axis labels for this figure? In your caption, please specify the task(s) the visualizaiton is intended to facilitate, as well as the marks, channels, and key-value pairs used.\nKey-Value pairs: Looks like we are using “round” as a continuous variable. Can we change this to an ordered factor?"
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#scatterplot-with-overall-draft-position",
    "href": "BCB520/posts/T7-NHLDraft/index.html#scatterplot-with-overall-draft-position",
    "title": "TUTORIAL 7",
    "section": "SCATTERPLOT WITH OVERALL DRAFT POSITION",
    "text": "SCATTERPLOT WITH OVERALL DRAFT POSITION\nThis approach might yield a better match with the scatterplot idiom. What if we ignore draft round, and use the player’s overall draft position instead?\n\n\nCode\nggplot(draft2018, aes(x=overall, y=WAR))+\n  geom_point()\n\n\nFor this figure, address the following:\n\nWe are trying to address the notion of trading a pick from round 1 for picks from round 2 and 3. Add visual channels to this plot that will help us make that decision.\nCreate a caption and better axis labels for this figure.\nWhat if we wanted to use more than just the 2018 draft class?"
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#scatterplot-summary",
    "href": "BCB520/posts/T7-NHLDraft/index.html#scatterplot-summary",
    "title": "TUTORIAL 7",
    "section": "SCATTERPLOT SUMMARY",
    "text": "SCATTERPLOT SUMMARY\nWe seem to be running into an issue in terms of overplotting. Scatterplots are great, but they work best for two quantitative attributes, and we have a situation with one or two keys and one quantitative attribute. The thing is, scatterplots can be very useful when part of our workflow involves modeling the data in some way. We’ll cover this kind of thing in future assignments, but just a bit of foreshadowing here:\n\n\nCode\nggplot(drafttot2018, aes(x=round, y=totpoints))+\n  geom_point()+\n  geom_smooth()+\n  facet_wrap(~position)\n\n\nAdding the smoothed line doesn’t eliminate the overplotting problem, but it does indicate that it exists. We’ll cover other potential solutions (including Cody’s violin plots!) to this issue later in the course, when we get to the notions of faceting and data reduction."
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#simple-bar-chart",
    "href": "BCB520/posts/T7-NHLDraft/index.html#simple-bar-chart",
    "title": "TUTORIAL 7",
    "section": "SIMPLE BAR CHART",
    "text": "SIMPLE BAR CHART\nOne of the best ways to deal with overplotting is to use our keys to SEPARATE and ORDER our data. Let’s do that now. I’ll stick with the summarized data for the 2018 draft year for now.\n\n\nCode\nggplot(draftposWAR2, aes(x = reorder(name,overall), y=WAR))+\n  geom_col(aes(color=as.factor(round)))\n\n\nEpic. We now have a bar (column, really) chart with the key being player name, and the attribute being the total number of games played. We’ve SEPARATED the data using the spatial x-axis position channel, and aligned to that axis as well. But this visualization clearly sucks. You need to make it better by:\n\nAdding a visual channel indicating draft round.\nFixing the order of the x axis.\nMaking a caption and better axis labels.\nFixing the values of the x axis labels so they aren’t such a mess."
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#stacked-bar",
    "href": "BCB520/posts/T7-NHLDraft/index.html#stacked-bar",
    "title": "TUTORIAL 7",
    "section": "STACKED BAR?",
    "text": "STACKED BAR?\nStacked bar charts use two keys and one value. Can we leverage this idiom? Perhaps if we used both round and postdraft as our keys and NHLgames as our value…\nThe idea here is that we might be able to get a sense of the temporal pattern of NHL games after a player is drafted. Do first round picks join the NHL earlier? Do they stay in the NHL longer? That kind of thing.\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=points, fill=as.factor(-round)))+\n  geom_col(position = \"stack\")\n\nNHLDraft &lt;-NHLDraft %&gt;%\n  mutate(descround = desc(as.factor(round)))\n\nrounds&lt;-c(\"darkred\",\"red\",\"pink\", \"violet\", \"lightblue\",\"blue\", \"darkblue\")\n\nggplot(NHLDraftstats, aes(x = postdraft, y=points, fill=as.factor(-round), \n                     alpha = as.factor(-round)))+\n  geom_col(position = \"stack\")+\n  theme(legend.position = \"none\")+\n  facet_wrap(~position)\n\n\nThis seems like it has some potential, but it definitely needs some work (by you):\n\nYou know the drill by now. Caption! Labels!\nImprove the color palette.\nDo we really only want data from the 2018 draft class?\nConsider the order of rounds within the stack (glyph). Which round is most important? Change the order within the glyphs to reflect this."
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#pie-charts-normalized-bar-charts",
    "href": "BCB520/posts/T7-NHLDraft/index.html#pie-charts-normalized-bar-charts",
    "title": "TUTORIAL 7",
    "section": "PIE CHARTS / NORMALIZED BAR CHARTS",
    "text": "PIE CHARTS / NORMALIZED BAR CHARTS\nWe all know that Pie Charts are rarely a good choice, but let’s look at how to make one here. I’ll eliminate all the players drafted in 2018 who never played an NHL game, leaving us 80 players drafted in that year who made “THE SHOW”. Let’s look at how those 80 players were drafted:\n\n\nCode\nplayedNHL2018 &lt;- drafttot2018%&gt;%\n  filter(totgames&gt;0)\n\nggplot(playedNHL2018, aes(x = \"\", fill = factor(round))) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\")\n\n\nObviously this isn’t great, but can you state why? Write a little critique of this visualizaiton that:\n\nConsiders a player who played hundreds of games over their first five years vs a player who played one game in five years.\nEvaluates the relative value of a second round pick and a third round pick.\n\nNow let’s change this to account for the various years post draft:\n\n\nCode\nseasonplayedNHL2018 &lt;- draft2018%&gt;%\n  filter(NHLgames&gt;0)\n\n\nggplot(seasonplayedNHL2018, aes(x = \"\", fill = factor(round))) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\")+\n  facet_wrap(~postdraft)\n\n\nSeems like there is something to work with here, but let’s compare this to a normalized bar chart:\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=as.factor(round)))+\n  geom_col(position = \"fill\")\n\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=overall))+\n  geom_col(position = \"fill\")\n\n\nCan you work with this to make it a useful visualization for your friend, Cassandra Canuck?"
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#heatmap",
    "href": "BCB520/posts/T7-NHLDraft/index.html#heatmap",
    "title": "TUTORIAL 7",
    "section": "HEATMAP?",
    "text": "HEATMAP?\nCould this be useful?\n\n\nCode\nround1&lt;-NHLDraft%&gt;%\n  filter(round==1)\n\nggplot(NHLDraft, aes(y = reorder(name, overall), x = postdraft, fill = NHLgames)) +\n  geom_tile(width = .5, height = 5) +\n  theme(plot.margin = unit(c(0,0,0,0), \"cm\"),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())+\n  scale_fill_gradient(low = \"blue\", high = \"red\")\n\nggplot(NHLDraft, aes(y = overall, x = postdraft, z=NHLgames)) + \n  geom_tile(aes(fill=NHLgames))"
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#other-stuff-to-consider",
    "href": "BCB520/posts/T7-NHLDraft/index.html#other-stuff-to-consider",
    "title": "TUTORIAL 7",
    "section": "OTHER STUFF TO CONSIDER",
    "text": "OTHER STUFF TO CONSIDER\n\nDo these visualizations change as a function of player position?\nIs the number of NHL games played really the best metric to use?"
  },
  {
    "objectID": "BCB520/posts/T7-NHLDraft/index.html#conclusion",
    "href": "BCB520/posts/T7-NHLDraft/index.html#conclusion",
    "title": "TUTORIAL 7",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nBased on your visualizations, what would you advise regarding this trade proposal? Why?"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#tldr-version",
    "href": "BCB520/posts/L5-ThumbRules/index.html#tldr-version",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "TLDR VERSION",
    "text": "TLDR VERSION\nThese are guidelines and considerations, not really absolute rules:\n\nWhen to use 3D? when to use 2D?\nWhen to use eyes instead of memory?\nWhen does immersion help?\nWhen to use overviews?\nHow long is too long?\nWhich comes first, form or function?"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#unjustified-use-of-3d",
    "href": "BCB520/posts/L5-ThumbRules/index.html#unjustified-use-of-3d",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "UNJUSTIFIED USE OF 3D",
    "text": "UNJUSTIFIED USE OF 3D\nCommon in news and some infographics.\n\n\n\nCheck this out on WTFViz\n\n\nCheck this out on WTFViz"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#depth-vs-planar-position",
    "href": "BCB520/posts/L5-ThumbRules/index.html#depth-vs-planar-position",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "DEPTH VS PLANAR POSITION",
    "text": "DEPTH VS PLANAR POSITION\nPosition channels are very powerful… when they are PLANAR SPATIAL POSITION, not depth!"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#the-dangers-of-depth",
    "href": "BCB520/posts/L5-ThumbRules/index.html#the-dangers-of-depth",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "THE DANGERS OF DEPTH",
    "text": "THE DANGERS OF DEPTH"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#the-actual-dangers-of-depth",
    "href": "BCB520/posts/L5-ThumbRules/index.html#the-actual-dangers-of-depth",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "THE ACTUAL DANGERS OF DEPTH",
    "text": "THE ACTUAL DANGERS OF DEPTH\nWe don’t really live in 3D: we see in 2.05D.\n\nWe acquire more info on an image plane quickly using eye movements\nWe acquire more info for depth much more slowly using head/body motion"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#occlusion",
    "href": "BCB520/posts/L5-ThumbRules/index.html#occlusion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "OCCLUSION",
    "text": "OCCLUSION\n“Occlusion occurs when one 3D graphic partially blocks another. It is the result of mimicking space in the natural world–where objects have differing X, Y, and Z coordinates. In data visualization, occlusion obscures important data and creates false hierarchies wherein unobstructed graphics appear most important.” … From This BLOG post by M. Bowers.\n\nInteraction can resolve occlusion, but at cost of time and cognitive load."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#perspective-distortion",
    "href": "BCB520/posts/L5-ThumbRules/index.html#perspective-distortion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "PERSPECTIVE DISTORTION",
    "text": "PERSPECTIVE DISTORTION\nPerspective in 3D visualizations interferes with all size channel encodings. The power of 2D planar positioning is lost!\n“Distortion occurs when 3D graphics recede into or project out from the picture plane through foreshortening. In drawing, foreshortening makes objects seem as though they inhabit three-dimensional space, but in data visualization, it creates more false hierarchies. Foreground graphics appear larger, background graphics smaller, and the relationship between data series is needlessly skewed.”…… From This BLOG post by M. Bowers."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#d-bar-charts---probably-not",
    "href": "BCB520/posts/L5-ThumbRules/index.html#d-bar-charts---probably-not",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D BAR CHARTS - Probably not…",
    "text": "3D BAR CHARTS - Probably not…\n3D bars are very difficult to justify!\nPerspective distortion and occlusion make faceting into 2D the better choice in most situations."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#tilted-text",
    "href": "BCB520/posts/L5-ThumbRules/index.html#tilted-text",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "TILTED TEXT",
    "text": "TILTED TEXT\nSkewed perspective in 3D visualizations does not interact well with fonts.\n\nExploring and Reducing the Effects of Orientation on Text Readability in Volumetric Displays.Grossman et al. CHI 2007"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#example-3d-extruded-time-series",
    "href": "BCB520/posts/L5-ThumbRules/index.html#example-3d-extruded-time-series",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "EXAMPLE: 3D EXTRUDED TIME SERIES",
    "text": "EXAMPLE: 3D EXTRUDED TIME SERIES\nWhat information can we decode from this visualization?\n\nCluster and Calendar based Visualization of Time Series Data. van Wijk and van Selow, Proc. InfoVis 99."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#planar-alternative",
    "href": "BCB520/posts/L5-ThumbRules/index.html#planar-alternative",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "PLANAR ALTERNATIVE",
    "text": "PLANAR ALTERNATIVE\nThis version of the extruded time series visualization uses derived data by computing a cluster hierarchy of power usage patterns. It then juxtaposes multiple views: a calendar and superimposed 2D curves differentiated with a color channel."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#d-shape-perception",
    "href": "BCB520/posts/L5-ThumbRules/index.html#d-shape-perception",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D: SHAPE PERCEPTION",
    "text": "3D: SHAPE PERCEPTION\nThe benefits of 3D visualization outweigh its costs when the task is shape perception for 3D spatial data. Interactive navigation often supports synthesis across many viewpoints.\n\nImage-Based Streamline Generation and Rendering. Li and Shen. IEEE Trans. Visualization and Computer Graphics (TVCG) 13:3 (2007), 630–640."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#d-constrained-navigation",
    "href": "BCB520/posts/L5-ThumbRules/index.html#d-constrained-navigation",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D: CONSTRAINED NAVIGATION",
    "text": "3D: CONSTRAINED NAVIGATION\n\nNYTimes Subscribers can access here"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#d-summary",
    "href": "BCB520/posts/L5-ThumbRules/index.html#d-summary",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D SUMMARY",
    "text": "3D SUMMARY\n\n3D legitimate for true 3D spatial data.\n3D needs very careful justification for abstract data\nThe benefits outweigh costs when topological structure/context important for the task."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#eyes-beat-memory",
    "href": "BCB520/posts/L5-ThumbRules/index.html#eyes-beat-memory",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "EYES BEAT MEMORY",
    "text": "EYES BEAT MEMORY\nExternal Cognition vs. Internal Memory\nIt is easy to compare by moving eyes between side-by-side views.\nIt is much more difficult to compare a visible item to your memory of what you saw.\n\nImplications for animation:\ngreat for choreographed storytelling\ngreat for transitions between two states\npoor for many states with changes everywhere\nconsider small multiples instead"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#resolution-beats-immersion",
    "href": "BCB520/posts/L5-ThumbRules/index.html#resolution-beats-immersion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "RESOLUTION BEATS IMMERSION",
    "text": "RESOLUTION BEATS IMMERSION\nImmersion is typically not helpful for abstract data because we do not need a sense of presence or stereoscopic 3D. A desktop view is also usually better for workflow integration.\nResolution in VR is a critical constraint, as pixels are the scarcest resource.\nFirst wave: virtual reality for abstract data! This is difficult to justify.\nSecond wave: AR/MR (augmented/mixed reality) has more promise."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#overview",
    "href": "BCB520/posts/L5-ThumbRules/index.html#overview",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "OVERVIEW",
    "text": "OVERVIEW\n“Overview first, zoom and filter, details on demand”\nThe Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations. Shneiderman. Proc. IEEE Visual Languages, pp. 336–343, 1996.\n\nThis seems simple enough, until you realize that “Overview” is really a microcosm of the full vis design problem space."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#responsiveness",
    "href": "BCB520/posts/L5-ThumbRules/index.html#responsiveness",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "RESPONSIVENESS",
    "text": "RESPONSIVENESS\nResponsiveness (visual feedback to the user) has 3 rough categories:\n\n0.1 seconds: perceptual processing\n\nsubsecond response for mouseover highlighting\n\n1 second: immediate response\n\nfast response after mouseclick, button press\nFitts’ Law limits on motor control\n\n10 seconds: brief tasks\n\nbounded response after dialog box\nmental model of heavyweight operation (file load)\n\nShow hourglass for multi-second operations (check for cancel/undo)\nShow progress bar for long operations (process in background thread)\nConsider rendering speed when item count is large (guaranteed frame rate)"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#function-then-form",
    "href": "BCB520/posts/L5-ThumbRules/index.html#function-then-form",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "FUNCTION, THEN FORM",
    "text": "FUNCTION, THEN FORM\nIt is dangerous to start with aesthetics because it is usually impossible to add function retroactively.\nStart with focus on functionality because you can improve and refine aesthetics later.\nAesthetics do matter! They are another level of function. Consider working with a graphic designer on important visualizations."
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#form-basic-graphic-design",
    "href": "BCB520/posts/L5-ThumbRules/index.html#form-basic-graphic-design",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "FORM: BASIC GRAPHIC DESIGN",
    "text": "FORM: BASIC GRAPHIC DESIGN\n\n\nProximity:\nDO group related items together\nAVOID equal whitespace between unrelated items\nAlignment:\nDO find/make a strong line and stick to it\nAVOID automatic centering\nRepetition:\nDO unify by pushing existing consistencies\nContrast:\nIf not identical, then very different\nAVOID not quite the same\n\n\n\n\nThe Non-Designer’s Design Book, 4th ed. Robin Williams, Peachpit Press, 2015. fast read, very practical to work through whole thing"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#labelling",
    "href": "BCB520/posts/L5-ThumbRules/index.html#labelling",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "LABELLING",
    "text": "LABELLING\nMake visualizations as self-documenting as possible!\nMeaningful & useful title, labels, legends.\nAxes and panes/subwindows should have labels and axes should have good mix/max boundary tick marks.\nEverything that’s plotted should have a legend and its own header/labels if not redundant with main title.\nUse reasonable numerical format and avoid scientific notation in most cases.\n\nhttps://xkcd.com/833/"
  },
  {
    "objectID": "BCB520/posts/L5-ThumbRules/index.html#summary---rules-of-thumb",
    "href": "BCB520/posts/L5-ThumbRules/index.html#summary---rules-of-thumb",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "SUMMARY - RULES OF THUMB",
    "text": "SUMMARY - RULES OF THUMB\n\nNo unjustified 3D\n\nPower of the plane\nDisparity of depth\nOcclusion hides information\nPerspective distortion dangers\nTilted text isn’t legible\n\nEyes beat memory\nResolution over immersion\nOverview first, zoom and filter, details on demand\nResponsiveness is required\nFunction first, form next\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html",
    "href": "BCB520/posts/A5-TabularData/index.html",
    "title": "ASSIGNMENT 5",
    "section": "",
    "text": "In this assignment, we are going to practice creating visualizations for tabular data. Unlike previous assignments, however, this time we will all be using the same data sets. I’m doing this because I want everyone to engage in the same logic process and have the same design objectives in mind."
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#scenario",
    "href": "BCB520/posts/A5-TabularData/index.html#scenario",
    "title": "ASSIGNMENT 5",
    "section": "SCENARIO",
    "text": "SCENARIO\nImagine you are a high priced data science consultant. One of your good friends, Cassandra Canuck, is an Assistant General Manager for the Vancouver Canucks, a team in the National Hockey League with a long, long…. long history of futility.\nCassandra tells you her boss, General Manager Hans Doofenschmirtz, is considering trading this year’s first round draft pick for two second round picks and one third round pick from another team. For the purposes of this exercise, let’s set the 2023 NHL draft order using the Tankathon Simulator. The NHL uses a lottery system in which the teams lowest in the standings have the highest odds of getting the first overall pick. I’ll simulate the lottery now…\nHOLY CRAP! The Vancouver Canucks jump up 6 spots, and will pick FIRST overall. Here is a screenshot:\n\nOur official scenario is this:\nVancouver receives: The 7th pick in the second round (39th overall), the 10th pick in the second round (42nd overall), and the 10th pick in the third round (74th overall).\nDetroit receives: The 1st pick in the first round (1st overall).\nDoofenschmirtz reasons that more draft picks are better, and is inclined to make the trade. Cassandra isn’t so sure…\nShe asks you to create some data visualizations she can show to her boss that might help him make the best decision."
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#directions",
    "href": "BCB520/posts/A5-TabularData/index.html#directions",
    "title": "ASSIGNMENT 5",
    "section": "DIRECTIONS",
    "text": "DIRECTIONS\nCreate a new post in your portfolio for this assignment. Call it something cool, like NHL draft analysis, or Hockey Analytics, or John Wick….\nCopy the data files from the repository, and maybe also the .qmd file.\nUse the .qmd file as the backbone of your assignment, changing the code and the markdown text as you go."
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#the-data",
    "href": "BCB520/posts/A5-TabularData/index.html#the-data",
    "title": "ASSIGNMENT 5",
    "section": "THE DATA",
    "text": "THE DATA\nHow can we evaluate whether trading a first round pick for two second round picks and a third round pick is a good idea? One approach is to look at the historical performance of players from these draft rounds.\nI’ve created a data set that will allow us to explore player performance as a function of draft position. If you are curious as to how I obtained and re-arranged these data, you can check out that tutorial here. For this assignment, though, I want to focus on the visualizations.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\n\nNHLDraft&lt;-read.csv(\"NHLDraft.csv\")\nNHLDictionary&lt;-read_excel(\"NHLDictionary.xlsx\")\n\nknitr::kable(NHLDictionary)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\ndraftyear\nOrdinal\nCalendar year in which the player was drafted into the NHL.\n\n\nname\nItem\nFull name of the player.\n\n\nround\nOrdinal\nRound in which the player was drafted (1 to 7).\n\n\noverall\nOrdinal\nOverall draft position of the player (1 to 224)\n\n\npickinRound\nOrdinal\nPosition in which the player was drafted in their round (1 to 32).\n\n\nheight\nQuantitative\nPlayer height in inches.\n\n\nweight\nQuantitative\nPlayer weight in pounds.\n\n\nposition\nCategorical\nPlayer position (Forward, Defense, Goaltender)\n\n\nplayerId\nItem\nUnique ID (key) assigned to each player.\n\n\npostdraft\nOrdinal\nNumber of seasons since being drafted (0 to 20).\n\n\nNHLgames\nQuantitative\nNumber of games played in the NHL in that particular season (regular season is 82 games, playoffs are up to 28 more).\n\n\n\n\n\nIn this case, we have a dataframe with all the drafted players since 2000, their position, their draft year and position, and then rows for each season since being drafted (postdraft). The key variable here is NHLgames, which tells us how many games they played in the NHL each season since being drafted."
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#simple-scatterplot",
    "href": "BCB520/posts/A5-TabularData/index.html#simple-scatterplot",
    "title": "ASSIGNMENT 5",
    "section": "SIMPLE SCATTERPLOT",
    "text": "SIMPLE SCATTERPLOT\nOne thing to realize about professional hockey is that it is pretty rare for a player to play in the NHL right after being drafted. Players get drafted when they are 18 years old, and they usually play in the juniors, minor leagues, or the NCAA for a bit to further develop. Let’s use a scatterplot to visualize this phenomenon with the most recent draft classes.\n\n\nCode\ndraft2022&lt;-NHLDraft%&gt;%\n  filter(draftyear==2022 & postdraft==0)\n\nggplot(draft2022, aes(x=round, y=NHLgames))+\n  geom_point()\n\n\n\n\n\nAs you can see, the players drafted in June of 2022 haven’t played much this season. There are few things wrong with this visualization, however:\n\nOverplotting. All those points on the y=0 line represent about 32 players each. Can you think of a way that adding extra channels might help?\nLabelling. Can we create a solid figure caption and better axis labels for this figure? In your caption, please specify the task(s) the visualizaiton is intended to facilitate, as well as the marks, channels, and key-value pairs used.\nKey-Value pairs: Looks like we are using “round” as a continuous variable. Can we change this to an ordered factor?"
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#expanded-scatterplot",
    "href": "BCB520/posts/A5-TabularData/index.html#expanded-scatterplot",
    "title": "ASSIGNMENT 5",
    "section": "EXPANDED SCATTERPLOT",
    "text": "EXPANDED SCATTERPLOT\nThe data from the most recent draft isn’t really helpful for our question. Let’s go back in time and use a draft year that has had some time to develop and reach their potential. How about 2018?\n\n\nCode\ndraft2018&lt;-NHLDraft%&gt;%\n  filter(draftyear==2018 & postdraft&lt;6)\n\nggplot(draft2018, aes(x=round, y=NHLgames))+\n  geom_point()\n\n\n\n\n\nHmmm… in addition to the problem of overplotting, we’ve got an additional issue here. We actually have two keys and one attribute. The attribute is NHLgames, and the keys are round and postdraft, but we are only using round.\nPostdraft indicates the number of seasons after being drafted. We have several choices here. We can make a visualization that uses both keys, or we can somehow summarize the data for one of the keys.\nFor example, let’s say we just wanted to know the TOTAL number of NHL games played since being drafted.\n\n\nCode\ndrafttot2018&lt;- draft2018%&gt;%\n  group_by(playerId, round, overall, position, name)%&gt;%\n  summarise(totgames=sum(NHLgames))\n\n\n`summarise()` has grouped output by 'playerId', 'round', 'overall', 'position'.\nYou can override using the `.groups` argument.\n\n\nCode\nggplot(drafttot2018, aes(x=round, y=totgames))+\n  geom_point()\n\n\n\n\n\nCode\ndrafttot&lt;- NHLDraft%&gt;%\n  group_by(playerId, round, overall, position, name)%&gt;%\n  summarise(totgames=sum(NHLgames))\n\n\n`summarise()` has grouped output by 'playerId', 'round', 'overall', 'position'.\nYou can override using the `.groups` argument.\n\n\nCode\nggplot(NHLDraft, aes(x=postdraft, y=NHLgames))+\n  geom_smooth(aes(color=as.factor(round)))\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nCode\nggplot(NHLDraft%&gt;%filter(postdraft&lt;6), aes(x=NHLgames))+\n  geom_histogram()+\n  facet_grid(postdraft~round)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFine, I guess, but we still have to deal with overplotting, and think about whether a scatterplot really helps us accomplish our task. For this figure do the following:\n\nOverplotting. All those points on the y=0 line represent about 32 players each. Can you you think of a way that adding extra channels might help?\nLabelling. Can we create a solid figure caption and better axis labels for this figure? In your caption, please specify the task(s) the visualizaiton is intended to facilitate, as well as the marks, channels, and key-value pairs used.\nKey-Value pairs: Looks like we are using “round” as a continuous variable. Can we change this to an ordered factor?"
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#scatterplot-with-overall-draft-position",
    "href": "BCB520/posts/A5-TabularData/index.html#scatterplot-with-overall-draft-position",
    "title": "ASSIGNMENT 5",
    "section": "SCATTERPLOT WITH OVERALL DRAFT POSITION",
    "text": "SCATTERPLOT WITH OVERALL DRAFT POSITION\nThis approach might yield a better match with the scatterplot idiom. What if we ignore draft round, and use the player’s overall draft position instead?\n\n\nCode\nggplot(drafttot2018, aes(x=overall, y=totgames))+\n  geom_point()\n\n\n\n\n\nFor this figure, address the following:\n\nWe are trying to address the notion of trading a pick from round 1 for picks from round 2 and 3. Add visual channels to this plot that will help us make that decision.\nCreate a caption and better axis labels for this figure.\nWhat if we wanted to use more than just the 2018 draft class?"
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#scatterplot-summary",
    "href": "BCB520/posts/A5-TabularData/index.html#scatterplot-summary",
    "title": "ASSIGNMENT 5",
    "section": "SCATTERPLOT SUMMARY",
    "text": "SCATTERPLOT SUMMARY\nWe seem to be running into an issue in terms of overplotting. Scatterplots are great, but they work best for two quantitative attributes, and we have a situation with one or two keys and one quantitative attribute. The thing is, scatterplots can be very useful when part of our workflow involves modeling the data in some way. We’ll cover this kind of thing in future assignments, but just a bit of foreshadowing here:\n\n\nCode\nggplot(drafttot2018, aes(x=round, y=totgames))+\n  geom_point()+\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nAdding the smoothed line doesn’t eliminate the overplotting problem, but it does indicate that it exists. We’ll cover other potential solutions (including Cody’s violin plots!) to this issue later in the course, when we get to the notions of faceting and data reduction."
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#simple-bar-chart",
    "href": "BCB520/posts/A5-TabularData/index.html#simple-bar-chart",
    "title": "ASSIGNMENT 5",
    "section": "SIMPLE BAR CHART",
    "text": "SIMPLE BAR CHART\nOne of the best ways to deal with overplotting is to use our keys to SEPARATE and ORDER our data. Let’s do that now. I’ll stick with the summarized data for the 2018 draft year for now.\n\n\nCode\nggplot(drafttot2018, aes(x = name, y=totgames))+\n  geom_col()\n\n\n\n\n\nEpic. We now have a bar (column, really) chart with the key being player name, and the attribute being the total number of games played. We’ve SEPARATED the data using the spatial x-axis position channel, and aligned to that axis as well. But this visualization clearly sucks. You need to make it better by:\n\nAdding a visual channel indicating draft round.\nFixing the order of the x axis.\nMaking a caption and better axis labels.\nFixing the values of the x axis labels so they aren’t such a mess."
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#stacked-bar",
    "href": "BCB520/posts/A5-TabularData/index.html#stacked-bar",
    "title": "ASSIGNMENT 5",
    "section": "STACKED BAR?",
    "text": "STACKED BAR?\nStacked bar charts use two keys and one value. Can we leverage this idiom? Perhaps if we used both round and postdraft as our keys and NHLgames as our value…\nThe idea here is that we might be able to get a sense of the temporal pattern of NHL games after a player is drafted. Do first round picks join the NHL earlier? Do they stay in the NHL longer? That kind of thing.\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=as.factor(-round)))+\n  geom_col(position = \"stack\")\n\n\n\n\n\nCode\nNHLDraft &lt;-NHLDraft %&gt;%\n  mutate(descround = desc(as.factor(round)))\n\nrounds&lt;-c(\"darkred\",\"red\",\"pink\", \"violet\", \"lightblue\",\"blue\", \"darkblue\")\n\nggplot(NHLDraft, aes(x = postdraft, y=NHLgames, fill=as.factor(-round), \n                     alpha = as.factor(-round)))+\n  geom_col(position = \"stack\")+\n  theme(legend.position = \"none\")\n\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\nThis seems like it has some potential, but it definitely needs some work (by you):\n\nYou know the drill by now. Caption! Labels!\nImprove the color palette.\nDo we really only want data from the 2018 draft class?\nConsider the order of rounds within the stack (glyph). Which round is most important? Change the order within the glyphs to reflect this."
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#pie-charts-normalized-bar-charts",
    "href": "BCB520/posts/A5-TabularData/index.html#pie-charts-normalized-bar-charts",
    "title": "ASSIGNMENT 5",
    "section": "PIE CHARTS / NORMALIZED BAR CHARTS",
    "text": "PIE CHARTS / NORMALIZED BAR CHARTS\nWe all know that Pie Charts are rarely a good choice, but let’s look at how to make one here. I’ll eliminate all the players drafted in 2018 who never played an NHL game, leaving us 80 players drafted in that year who made “THE SHOW”. Let’s look at how those 80 players were drafted:\n\n\nCode\nplayedNHL2018 &lt;- drafttot2018%&gt;%\n  filter(totgames&gt;0)\n\nggplot(playedNHL2018, aes(x = \"\", fill = factor(round))) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\")\n\n\n\n\n\nObviously this isn’t great, but can you state why? Write a little critique of this visualizaiton that:\n\nConsiders a player who played hundreds of games over their first five years vs a player who played one game in five years.\nEvaluates the relative value of a second round pick and a third round pick.\n\nNow let’s change this to account for the various years post draft:\n\n\nCode\nseasonplayedNHL2018 &lt;- draft2018%&gt;%\n  filter(NHLgames&gt;0)\n\n\nggplot(seasonplayedNHL2018, aes(x = \"\", fill = factor(round))) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\")+\n  facet_wrap(~postdraft)\n\n\n\n\n\nSeems like there is something to work with here, but let’s compare this to a normalized bar chart:\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=as.factor(round)))+\n  geom_col(position = \"fill\")\n\n\nWarning: Removed 218 rows containing missing values (`geom_col()`).\n\n\n\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=overall))+\n  geom_col(position = \"fill\")\n\n\nWarning: Removed 218 rows containing missing values (`geom_col()`).\n\n\n\n\n\nCan you work with this to make it a useful visualization for your friend, Cassandra Canuck?"
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#heatmap",
    "href": "BCB520/posts/A5-TabularData/index.html#heatmap",
    "title": "ASSIGNMENT 5",
    "section": "HEATMAP?",
    "text": "HEATMAP?\nCould this be useful?\n\n\nCode\nround1&lt;-NHLDraft%&gt;%\n  filter(round==1)\n\nggplot(NHLDraft, aes(y = reorder(name, overall), x = postdraft, fill = NHLgames)) +\n  geom_tile(width = .5, height = 5) +\n  theme(plot.margin = unit(c(0,0,0,0), \"cm\"),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())+\n  scale_fill_gradient(low = \"blue\", high = \"red\")\n\n\n\n\n\nCode\nggplot(NHLDraft, aes(y = overall, x = postdraft, z=NHLgames)) + \n  geom_tile(aes(fill=NHLgames))"
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#other-stuff-to-consider",
    "href": "BCB520/posts/A5-TabularData/index.html#other-stuff-to-consider",
    "title": "ASSIGNMENT 5",
    "section": "OTHER STUFF TO CONSIDER",
    "text": "OTHER STUFF TO CONSIDER\n\nDo these visualizations change as a function of player position?\nIs the number of NHL games played really the best metric to use?"
  },
  {
    "objectID": "BCB520/posts/A5-TabularData/index.html#conclusion",
    "href": "BCB520/posts/A5-TabularData/index.html#conclusion",
    "title": "ASSIGNMENT 5",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nBased on your visualizations, what would you advise regarding this trade proposal? Why?"
  },
  {
    "objectID": "BCB520/posts/A4-MarksChannels/index.html",
    "href": "BCB520/posts/A4-MarksChannels/index.html",
    "title": "ASSIGNMENT 4",
    "section": "",
    "text": "It’s one thing to “learn” about the theory of marks and channels for data visualization. It’s an entirely different thing to LEARN these concepts by IMPLEMENTING them in your own visualizations. In this assignment, you will do exactly that. Create some simple visualizations that highlight some of the key concepts from this section of the class."
  },
  {
    "objectID": "BCB520/posts/A4-MarksChannels/index.html#a-new-portfolio-post",
    "href": "BCB520/posts/A4-MarksChannels/index.html#a-new-portfolio-post",
    "title": "ASSIGNMENT 4",
    "section": "A NEW PORTFOLIO POST",
    "text": "A NEW PORTFOLIO POST\nThis assignment will appear as a new post in your nascent data science portfolio. To create a new post, you need to navigate to your posts directory in your RStudio Project. I do this using the Files tab in the lower right quadrant of RStudio.\n\n\nMake a new directory within posts called MarksChannels .\nClick on that new directory to enter it and then use that create a new blank file button and choose Quarto Document.\nName the new file index.qmd .\nAdd YAML text to the top of the file. In particular, add the following:\n\ntitle: “ASSIGNMENT 4”\nsubtitle: “Marks and Channels”\nauthor: “YOUR NAME”\ndate: “2023-02-14”\ncategories: [Assignment, DataViz]\nimage: “some fun image you put in that new directory.png”\ncode-fold: true\ncode-tools: true\ndescription: “A clever description that describes the stuff”"
  },
  {
    "objectID": "BCB520/posts/A4-MarksChannels/index.html#make-your-portfolio-less-terrible",
    "href": "BCB520/posts/A4-MarksChannels/index.html#make-your-portfolio-less-terrible",
    "title": "ASSIGNMENT 4",
    "section": "MAKE YOUR PORTFOLIO LESS TERRIBLE",
    "text": "MAKE YOUR PORTFOLIO LESS TERRIBLE\nWhile we are at it, let’s improve the overall look and structure of your portfolio. Navigate to the about.qmd file in your portfolio root directory and open it in RStudio. Use the information found here to customize your About page.\nAT MINIMUM , I want an About page that contains your name, a brief description about you, a picture of yourself, a link to your github, and a section on your Education."
  },
  {
    "objectID": "BCB520/posts/A4-MarksChannels/index.html#marks-and-channels",
    "href": "BCB520/posts/A4-MarksChannels/index.html#marks-and-channels",
    "title": "ASSIGNMENT 4",
    "section": "MARKS AND CHANNELS",
    "text": "MARKS AND CHANNELS\nBack to your new post!\nUse that shiny new index.qmd file to perform the tasks below. Every time you create a figure, it needs a caption. The text in that section of your assignment should also briefly describe the data set you are using, especially the attributes used for the visualization. In addition, make sure the visualization task actually requires the particular concept. For example, don’t just make a scatterplot with one red dot for the Popout exercise. You need to describe a task that requires we IDENTIFY that point.\n\nExpressiveness and Effectiveness\nFrom Munzner, 5.4.1:\nThe expressiveness principle: Visual encoding should express all of, and only, the information in the dataset attributes. Ordered data should be shown in a way that our perceptual system intrinsically senses as ordered. Conversely, unordered data should not be shown in a way that perceptually implies an ordering that does not exist.\nThe effectiveness principle: The importance of the attribute should match the salience of the channel (its noticeablity). The most important attributes should be encoded with the most effective channels.\nUsing whatever data set you choose, create a visualization called Figure 1 that adheres to these two principles. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 2 that violates these principles in the most extreme ways you can imagine while still having the visualization be recognizibly the same as the first. Write a caption that explains your choices of marks and channels and why they are bad compared to Figure 1.\n\n\nDiscriminability\nUsing whatever data set you choose (it need not be the same data used for Figures 1 and 2), create a visualization called Figure 3 that uses a magnitude channel a number of bins that facilitate discriminiability of an attribute. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 4 that uses this same channel for WAY TOO MANY BINS, violating the guidelines for discriminability. Write a caption that explains why this number of bins is bad compared to Figure 3.\n\n\nSeparability\nUsing whatever data set you choose (it need not be the same data used for Figures 1-4), create a visualization called Figure 5 that uses a two or more channels to encode two or more attributes while maintainig separability. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 6 that uses channels that are integral, or at least much less separable. Write a caption that explains why choosing these channels is bad compared to Figure 5.\n\n\nPopout\nUsing whatever data set you choose (it need not be the same data used for Figures 1-6… you get the idea), create a visualization called Figure 7 that effectively uses the concept of popout. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 8 that makes the identification task in Figure 7 much more difficult. Write a caption that explains why Figure 8 sucks compared to Figure 7."
  },
  {
    "objectID": "BCB520/posts/A2-YourData/index.html",
    "href": "BCB520/posts/A2-YourData/index.html",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "",
    "text": "A key feature of this course is that students should be using their own data whenever possible. This is critical to forging a learning experience that is customized to each student’s aspirations and the eccentricities of their chosen research domain. This assignment begins the process of helping you identify the data sets with which you want to work, and aligns with the notion of understanding the concepts of Data Semantics and Data Abstraction."
  },
  {
    "objectID": "BCB520/posts/A2-YourData/index.html#assignment",
    "href": "BCB520/posts/A2-YourData/index.html#assignment",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThe basic structure of this assignment is for you to identify, import, describe, and host a data set. I’ll break down the specifics for each of these actions below.\n\nIdentify a Data Set\nThe main criteria is that the data set has to matter to you in some way. Often, this will mean that it is your data set. It was collected by you and has a central role in your current or past graduate research. Awesome! Another scenario is that the data you want to use comes from your current job. Maybe it isn’t part of a research project, but you are motivated to learn how to better visualize the data or you are very interested in learning more about it. Also Awesome!\nSome of you might not have your own data. Perhaps you have just started your graduate training. Maybe your job doesn’t yet have data that you need to work with. No Problem!\nIt is perfectly fine to find publicly available data sets online. As long as the data set is interesting to you! You just need to make sure that the data:\n\nAre publicly available.\nAre not restricted by some kind of license or copyright.\nDo not contain private information.\nAre not covered by HIPPA, FERPA, CMMC, or other federal regulations related to data.\n\nIf you need help finding a data set, just let me know.\nSome fun potential categories for data sources include:\n\nSports Analytics from your favorite sport or team.\nPublicly available genomics data bases.\nKeggle.\nThe movie data base.\nClassic data sets from your field.\n\n\n\nImport the Data Set\nThis one is probably straightforward if your data set comes from your own research and lives on your local hard drive already.\n\n\nDescribe the Data Set\nThis is the bulk of the assignment. I want you to use the framework described in Dr. Munzner’s textbook to understand your data set and describe it to someone who is unfamiliar with your work. The basis of this approach is descibed in this lecture. In addition, this figure from the textbook summarizes the kinds of data types, data set types, and attribute types you might have in your data:\n\n\n\nBONUS OBJECTIVE: Host your Data Set\nUltimately, we are moving toward each of you hosting your assignments within an online repository that can serve as your data science portfolio. For this course, we are going to assume this is GitHub. At the very least, I want everyone to create (or dust off and log in to) your own GitHub account. We’ll try to use this assignment to set up a project repository, and perhaps even a simple web site using GitHub pages."
  },
  {
    "objectID": "BCB520/posts/A2-YourData/index.html#resources",
    "href": "BCB520/posts/A2-YourData/index.html#resources",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nBarrie attempts this assignment in Tutorial 4."
  },
  {
    "objectID": "BCB520/posts/T2-Anscombe/index.html",
    "href": "BCB520/posts/T2-Anscombe/index.html",
    "title": "Tutorial 2 - Literate Programming and Anscombe’s Quartet",
    "section": "",
    "text": "Do the summary statistics reveal the truth? Or are they FILLED WITH LIES? A simple demonstration with Anscombe’s Quartet."
  },
  {
    "objectID": "BCB520/posts/T2-Anscombe/index.html#the-data",
    "href": "BCB520/posts/T2-Anscombe/index.html#the-data",
    "title": "Tutorial 2 - Literate Programming and Anscombe’s Quartet",
    "section": "The Data",
    "text": "The Data\nAnscombe’s Quartet is comprised of four pairs of x,y data:\n\n\nCode\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(datasets)\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\n\n\nCode\ndatasets::anscombe\n\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89"
  },
  {
    "objectID": "BCB520/posts/T2-Anscombe/index.html#example-hypotheses",
    "href": "BCB520/posts/T2-Anscombe/index.html#example-hypotheses",
    "title": "Tutorial 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Example Hypotheses",
    "text": "Example Hypotheses\n\nEric has four replicates in which he is measuring the expression of two proteins (x, y) on 11 samples.\nCody has four replicates in which he is measuring gene expression of two genes (x, y) in 11 cell images.\nRonald has four replicates in which he is measuring a blood protein (x) and a malaria marker (y) in 11 patients.\n\nYour hypothesis is that the four replicates do not differ in the correlation between x and y."
  },
  {
    "objectID": "BCB520/posts/T2-Anscombe/index.html#summary-statistics",
    "href": "BCB520/posts/T2-Anscombe/index.html#summary-statistics",
    "title": "Tutorial 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nCode\ntidy_anscombe &lt;- anscombe %&gt;%\n pivot_longer(cols = everything(),\n              names_to = c(\".value\", \"set\"),\n              names_pattern = \"(.)(.)\")\ntidy_anscombe_summary &lt;- tidy_anscombe %&gt;%\n  group_by(set) %&gt;%\n  summarise(across(.cols = everything(),\n                   .fns = lst(min,max,median,mean,sd,var),\n                   .names = \"{col}_{fn}\"))\n#&gt; `summarise()` ungrouping output (override with `.groups` argument)\n\nvars&lt;-c(\"set\", \"x_mean\", \"x_var\",  \"y_mean\", \"y_var\")\nthing&lt;- as.data.frame(tidy_anscombe_summary[vars])\nknitr::kable(thing)\n\n\n\n\n\nset\nx_mean\nx_var\ny_mean\ny_var\n\n\n\n\n1\n9\n11\n7.500909\n4.127269\n\n\n2\n9\n11\n7.500909\n4.127629\n\n\n3\n9\n11\n7.500000\n4.122620\n\n\n4\n9\n11\n7.500909\n4.123249"
  },
  {
    "objectID": "BCB520/posts/T2-Anscombe/index.html#visualization-reveals-hidden-patterns",
    "href": "BCB520/posts/T2-Anscombe/index.html#visualization-reveals-hidden-patterns",
    "title": "Tutorial 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Visualization reveals hidden patterns!",
    "text": "Visualization reveals hidden patterns!\n\n\nCode\nggplot(tidy_anscombe,\n       aes(x = x,\n           y = y)) +\n  geom_point() +\n  geom_point(data = tidy_anscombe_summary, aes(x=x_mean, y = y_mean, color = \"red\", size = 5),\n             show.legend = FALSE)+\n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "BCB520/posts/A10-FinalProject/index.html",
    "href": "BCB520/posts/A10-FinalProject/index.html",
    "title": "FINAL PROJECT",
    "section": "",
    "text": "This assignment provides you the opportunity to synthesize all of the concepts we’ve covered in the course to date. The basic framework is that you will create a COMPLETE data visualization BLOG post that is suitable as a showcase component of your Data Science Portfolio. The point is to SHOW people your skills."
  },
  {
    "objectID": "BCB520/posts/A10-FinalProject/index.html#structure",
    "href": "BCB520/posts/A10-FinalProject/index.html#structure",
    "title": "FINAL PROJECT",
    "section": "STRUCTURE",
    "text": "STRUCTURE\nThe basic formatting guidelines for this assignment are:\n\nInclude code fold or code tools options (or both) that allow users to view and copy your code while maintaining overall readability of your post.\nSuppress all output and warnings that might distract from your visualizations and writing.\nProperly title your assignment. The main title should be “BCB 520 - Final Project”, and the subtitle should be a descriptive title related to your question or topic.\nInclude author, date, categories, and a description in your YAML header.\nWrite clear, complete sentences for a target audience with some scientific background but little training in your specific discipline.\nInclude references if appropriate and use hyperlinks to external sources of data, inspiration, or examples.\nUse the header hierarchy and create a sensible document outline with white space. Format for readability! Use bold and italic fonts to emphasize things! Use color by customizing your .css file!\n\nIn addition to the above formatting guidelines, your portfolio post must contain the following sections:\n\nPreamble\nWrite a brief paragraph describing the primary question or purpose of the post. Ideally, the concept should be challenging enough that it requires at least two visualizations that use different idioms (ie. don’t just make two scatterplots with different variables). The concept should also be challenging enough that it captures the interest of the reader (i.e. a plot of height and weight that shows they are correlated is trivial and not appropriate). The best approach is to explore a topic or question in which YOU are very interested.\n\n\nData\nWrite a summary of the data sources you will use. Include a Data Dictionary table that fully describes each individual data file used. You may use your own research data or publicly available data from any source you like (with attribution). There aren’t any minimum or maximum data set size requirements, other than you need something big enough to be interesting and not so big that we don’t have a supercomputer capable of creating your visualization.\nNEW REQUIREMENT: Your assignment must feature one of the two new data types we have considered since the midterm: NETWORK DATA or SPATIAL DATA.\n\n\nVisualizations\nCreate your visualizations and include text that explains any steps or design choices. Be sure to include clearly labeled axes and a concise but complete figure caption for each visualization. Make deliberate choices for color palettes, point marks, line types, etc. Demonstrate that you understand the concepts we have covered!\n\n\nConclusions or Summary\nAnswer your research question. Draw a conclusion or inference related to your topic. Summarize your results. What new questions have emerged as a result of your visualizations? What interesting next steps have emerged?"
  },
  {
    "objectID": "BCB520/posts/A10-FinalProject/index.html#rubric",
    "href": "BCB520/posts/A10-FinalProject/index.html#rubric",
    "title": "FINAL PROJECT",
    "section": "RUBRIC",
    "text": "RUBRIC\nI will evaluate the following for your portfolio post:\n1. Clarity of writing (15%): Complete, clear sentences. Good Grammar. Understandable to target audience. Logical flow of ideas.\n2. Adherence to format (10%): Did you follow directions?\n3. Topic suitability (15%): Is the topic interesting? Are the visualizations challenging and interesting enough to showcase your skills?\n4. Viz Execution (40%): Are the visualizations effective? Do they adhere to the principles of effectiveness? Are choices for idiom, marks, channels, etc made deliberately and well justified?\n5. Creativity (20%): Did you push your boundaries and learn new techniques? Is the overall post compelling and interesting? Are the visualizations inspiring, creative, unique, and generally impressive? If I were recruiting a new data scientist (and I often am), would this portfolio post impress me, or would it damage your candidacy during review?\n\n\nCode\nlibrary(ggplot2)\nlibrary(sf)\n\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nCode\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\nCode\nus_counties &lt;- tigris::counties(cb = TRUE, resolution = \"20m\", year = 2020, class = \"sf\")\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\nset.seed(123)\ndata &lt;- data.frame(\n  GEOID = us_counties$GEOID,\n  value = runif(length(us_counties$GEOID), 0, 100)\n)\n\n\nus_counties_data &lt;- left_join(us_counties, data, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = us_counties_data, aes(fill = value), color = \"white\", size = 0.1) +\n  scale_fill_viridis_c() +  # You can choose other color palettes\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank())"
  },
  {
    "objectID": "BCB520/posts/T5-MarksChannels/index.html",
    "href": "BCB520/posts/T5-MarksChannels/index.html",
    "title": "TUTORIAL 5 - Marks and Channels",
    "section": "",
    "text": "This tutorial is my attempt at Assignment 4!\n\n\nI’m interested in testing the hypothesis that this season’s Vancouver Canucks are a historically terrible team. One of the key factors in this hypothesis is that this year’s Canucks really, really suck at killing penalties. For this first assignment component, I’ll show you what I mean with a visualization!\nThe data I am using was downloaded from Moneypuck.com.\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(vroom)\n\nfiles &lt;- fs::dir_ls(glob = \"*teams.csv\")\n\nallseasons &lt;- vroom::vroom(files)\n\nallseasons$color&lt;-\"darkblue\"\nallseasons$color[allseasons$name==\"VAN\" & allseasons$season==\"2022\"]  &lt;- \"red\"\n  allseasons$teamseason&lt;- paste(allseasons$name, allseasons$season)\n  \n  allseasons$HDSAR&lt;-allseasons$highDangerShotsAgainst/allseasons$iceTime*120\n  \n  PKDATA&lt;-allseasons[c(\"situation\", \"teamseason\", \"goalsAgainst\", \"penaltiesFor\")]\n  PKpenalties&lt;-subset(PKDATA, situation == \"all\")\n  PKgoals&lt;-subset(PKDATA, situation == \"4on5\")\n  PK&lt;-PKgoals[c(\"teamseason\", \"goalsAgainst\")]\n  PK$penaltiesFor&lt;-PKpenalties$penaltiesFor\n  PK$PKpercent&lt;-100-(PK$goalsAgainst/PK$penaltiesFor*100)\n\n\nNow that I’ve massaged the data, I can tell you that the 2022-2023 Vancouver Canucks penalty kill percentage is 76.9633508. That means that when they get a penalty, there is about a 76.9633508 percent chance that they will prevent the opponent from scoring while they are short handed. However, unless you are a hockey super fan, you don’t have enough context to know if this is good or bad.\nThe figure below shows the distribution of penalty kill percentage for all teams and seasons since the 2010 season.\n\n\nCode\n  d &lt;- density(PK$PKpercent)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PK$PKpercent)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PK$PKpercent, 0.01)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22 &lt;- PK$PKpercent[PK$teamseason == \"VAN 2022\"]\n\nggplot(PK, aes(x=PKpercent))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.5, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Penalty Kill (%)\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nSo… yeah. The blue line is the 22 Canucks. The red dotted line is the 1st! percentile of the density function. This is fine. We know this to be true. Their PK sucks. It is one of the worst in the history of the NHL. But WHY is it so bad? Bad goaltending? Bad defense? Both?\n\n\n\nMy theory is that their defense hangs their goaltenders out to dry. This might be measured with the number of High Danger Shots Allowed.\nHigh Danger Shots are defined as unblocked shot attempts with greater than a 20% probability of being a goal, as calculated by hockey data nerds. High danger shots account for ~5% of shots and ~33% of goals in the NHL.\nBelow I calculate High Danger Shots Allowed Rate, which is the number of HDS during penalty kill situations standardized it to a rate per 2 minutes spent killing penalties (the normal duration of an NHL minor penalty).\n\n\nCode\nPKsituation&lt;-subset(allseasons, situation == \"4on5\")\n# Step 2: Estimate the density of the data\nd &lt;- density(PKsituation$HDSAR)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKsituation$HDSAR)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKsituation$HDSAR, 0.95)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22HD &lt;- PKsituation$HDSAR[PKsituation$teamseason == \"VAN 2022\"]\n\n\n\n\n\nggplot(PKsituation, aes(x=HDSAR))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.005, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22HD, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"High Danger Shots Allowed (rate per 2 minutes)\")\n\n\n\n\n\nFigure 1. Distribution of High Danger Shot Rate for NHL teams over the past 10 seasons. The 2022-2023 Vancouver Canucks are shown with the dark blue line (HDSAR = 0.3220996) and the 95th percentile is shown in red.\nIn terms of the semantics of Data Visualization, I am trying to DISCOVER the DISTRIBUTION of High Danger Shots Allowed, and then ANNOTATE the DISTRIBUTION with information about the current Vancouver Canucks team and their approximate percentile in the distribution.\nTo visualize this I am encoding a DERIVED attribute - the relative density of HDSAR as VERTICAL LENGTH of a bar (technically the MARK is a LINE) on an aligned axis. This is a pretty powerful encoding, which is why it is very common when visualizing distributions. In addtion, I am encoding HDSAR values themselves with HORIZONTAL POSITION. A nice, ordered channel for a nice, ordered attribute. The DENSITY FUNCTION is less clear to me. I’m obviously using a line, whose vertical and horizontal position encodes the relationship between the two variables. But the blue shaded fill? I kind of think that is using an AREA channel and REDUNDANT ENCODING to send a stronger signal.\nThe annotations of the 95th percentile and the 2022 Canucks’ HDSAR are encoded with COLOR and HORIZONTAL POSITION CHANNELS, and LINES as MARKS.\nThe 2022 Canucks indeed seem to be giving up HDS at a historically high rate.\n\n\n\nBut wait! You exclaim, shouting your objection into the uncaring Abyss. What about the goaltending? Surely that has also contributed to this season’s futility? “Perhaps..”, the Abyss murmurs back…\nLet’s see whether the futility of the penalty kill is also a function of bad goaltending. Now, we can’t just look at raw goals allowed, because if the defense is allowing really High Danger Shots, then even a good goaltender is going to allow some goals. Fortunately, the NHL nerds have developed an Expected Goals metric. Moneypuck provides this handy description in their data dictionary:\nExpected Goals:. The sum of the probabilities of unblocked shot attempts being goals. For example, a rebound shot in the slot may be worth 0.5 expected goals, while a shot from the blueline while short handed may be worth 0.01 expected goals. The expected value of each shot attempt is calculated by the MoneyPuck Expected Goals model. Expected goals is commonly abbreviated as xGoals. Blocked shot attempts are valued at 0 xGoals.\nCool!\n\n\nCode\n  Tending&lt;-allseasons[c(\"situation\", \"teamseason\", \"goalsAgainst\", \"xGoalsAgainst\", \"shotsOnGoalAgainst\", \"games_played\", \"iceTime\", \"highDangerShotsAgainst\")]\nTending$GArate&lt;-Tending$xGoalsAgainst/Tending$iceTime*120\n  PKTending&lt;-subset(Tending, situation == \"4on5\")\n\n  d &lt;- density(PKTending$GArate)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKTending$GArate)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKTending$GArate, 0.95)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22GA &lt;- PKTending$GArate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=GArate))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.005, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22GA, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Expected Goals Allowed while killing penalties (rate per 2 minutes)\")  \n\n\n\n\n\nThis figure uses Expected Goals. It shows that Vancouver is playing a defense that is generating a historically high amount of expected goals, a result in line with our analysis of High Danger Chances.\nTo truly evaluate the goaltending, tt might be better for us to look at a ratio of Expected Goals to actual Goals. If this number is close to 1, it indicates an expected level of NHL goaltending during the penalty kill. The goalies are making the saves that they should. If the number is higher than 1, it means the goaltenders are making more saves than they should and are playing well. If the number is lower than 1, it means the goaltenders are letting in easy goals.\n\n\nCode\nTending$XGrate&lt;-Tending$xGoalsAgainst/Tending$goalsAgainst\n  PKTending&lt;-subset(Tending, situation == \"4on5\")\n\n  d &lt;- density(PKTending$XGrate)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKTending$XGrate)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKTending$XGrate, 0.05)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22XG &lt;- PKTending$XGrate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=XGrate))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.01, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22XG, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Expected Goals / Goals Allowed ratio while killing penalties\")  \n\n\n\n\n\nFigure 3. Distribution of Expected Goals to Goals Allowed during 4 on 5 situations for NHL teams over the past 10 seasons. The 2022-2023 Vancouver Canucks are shown with the dark blue line (XGrate = 0.6963636) and the 5th percentile is shown in red.\nUh Oh. Not only is the defense giving up a historically high number of High Danger scoring chances, the goaltenders are performing well below expectation. They are making about 70% of the saves they are supposed to make. So…. yeah. Why is the penalty kill so bad? Bad defense and bad goaltending. I mean, HISTORICALLY BAD.\nIs it because of the system? I don’t think so. I think it is because of the players. They don’t have enough defensive talent on the team to effectively kill penalties. I am sad.\n\n\n\nNow, lets mess this up! First I’m going back to the HDSAR figure, and changing some of my choices for marks and channels.\n\n\nCode\nggplot(PKsituation, aes(x=teamseason, y=HDSAR)) +\n  geom_bar(stat=\"identity\", aes(fill=name, alpha=season)) +\n  xlab(\"NHL Team and Season\") +\n  ylab(\"High Danger Shots Allowed (rate per 2 minutes)\") +\n  ggtitle(\"Are the Vancouver Canucks Historically Bad?\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 0.1),\n        legend.position = \"right\")\n\n\n\n\n\nFigure 2. Distribution of High Danger Shot Rate for NHL teams over the past 10 seasons.\nHoly Crap. What an absolute nightmate. In terms of MARKS, I’ve stayed with LINES. But the CHANNELS… wow. The COLOR CHANNEL is used to indicate the team. The ALPHA CHANNEL (transparency) is used to indicate season (time). These choices create HUGE problems related to discriminability (alpha channel) and separability. In addition, my choice to order these lines according to team and season is not at all compatible with my choice of channels. Not to mention that the important thing - to see where the 2022 Canucks stand in terms of historical futility - is completely wrecked because the horizontal position (a strong channel) is not closely tied to the most salient Attribute (HDSAR). Go ahead and pick out the the 2022 Canucks from this figure…. I’ll wait.\n\n\nFrom Munzner, 5.4.1:\nThe expressiveness principle: Visual encoding should express all of, and only, the information in the dataset attributes. Ordered data should be shown in a way that our perceptual system intrinsically senses as ordered. Well, using color sure violates that idea! Conversely, unordered data should not be shown in a way that perceptually implies an ordering that does not exist.\nThe effectiveness principle: The importance of the attribute should match the salience of the channel (its noticeablity). The most important attributes should be encoded with the most effective channels. Definitely not! Even though the Line Marks are using Length on an aligned axis, their width and ordering on the x axis completely obscures the focal team.\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1 and 2), create a visualization called Figure 3 that uses a magnitude channel with a number of bins that facilitate discriminiability of an attribute. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 4 that uses this same channel for WAY TOO MANY BINS, violating the guidelines for discriminability. Write a caption that explains why this number of bins is bad compared to Figure 3. Look at the NHL Rainbow! So many colors… so little ability to differentiate between teams. For example, go ahead and try to pick out Edmonton vs Florida\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1-4), create a visualization called Figure 5 that uses a two or more channels to encode two or more attributes while maintaining separability. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 6 that uses channels that are integral, or at least much less separable. Write a caption that explains why choosing these channels is bad compared to Figure 5.\nFor this one I’d like to try something else. Let’s look at the Expected Goals and Allowed Goals as a scatterplot:\n\n\nCode\nV22&lt;-subset(PKTending, teamseason == \"VAN 2022\")\nVAN22XG &lt;- PKTending$XGrate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=xGoalsAgainst/games_played*82, y =goalsAgainst/games_played*82 ))+\n  geom_point(aes(alpha=highDangerShotsAgainst, size=highDangerShotsAgainst))+\n  geom_abline(intercept = 0, slope = 1)+\n  geom_abline(intercept = 90, slope = -1)+\n \n  xlim(0,100)+\n  ylim(0,100)+\n  geom_vline(xintercept = mean(PKTending$xGoalsAgainst/PKTending$games_played*82), color = \"blue\", size=1.0, linetype=\"dotted\")+\n  geom_hline(yintercept = mean(PKTending$goalsAgainst/PKTending$games_played*82), color = \"blue\", size = 1, linetype=\"dotted\")+\n  geom_point(data = V22, aes(x=xGoalsAgainst/games_played*82, y =goalsAgainst/games_played*82),\n             color = \"red\")\n\n\n\n\n\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1-6… you get the idea), create a visualization called Figure 7 that effectively uses the concept of popout. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 8 that makes the identification task in Figure 7 much more difficult. Write a caption that explains why Figure 8 sucks compared to Figure 7. The popout concept is best shown by my use of a dark blue (color) line mark to indicate the 22 Canucks. Without that, you cannot achieve the task because the 22 Canucks are too hard to find without doing a serial search."
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#dataset-types-networks",
    "href": "BCB520/posts/L8-NetworkData/index.html#dataset-types-networks",
    "title": "LECTURE 8 - Network Data",
    "section": "DATASET TYPES: NETWORKS",
    "text": "DATASET TYPES: NETWORKS\n\n\n\nnetworks (also called graphs)\n\nmodel relationships between things\ntwo kinds of items, both can have attributes\n\nnodes\nlinks\n\n\ntrees are a special case\n\nno cycles\none parent per node"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#arrange-networkstrees",
    "href": "BCB520/posts/L8-NetworkData/index.html#arrange-networkstrees",
    "title": "LECTURE 8 - Network Data",
    "section": "ARRANGE NETWORKS/TREES",
    "text": "ARRANGE NETWORKS/TREES"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#network-tasks",
    "href": "BCB520/posts/L8-NetworkData/index.html#network-tasks",
    "title": "LECTURE 8 - Network Data",
    "section": "NETWORK TASKS",
    "text": "NETWORK TASKS\n\n\n\ntopology based tasks\n\nfind paths\nfind (topological) neighbors\ncompare centrality/importance measures\nidentify clusters / communities\n\nattribute based tasks (similar to tabular data)\n\nfind distributions, etc…\n\ncombination tasks, incorporating both\n\nexample: find friends-of-friends who like cats\n\ntopology: find all adjacent nodes of given node\nattributes: check if has-pet (node attribute) == cat"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#node-link-diagrams",
    "href": "BCB520/posts/L8-NetworkData/index.html#node-link-diagrams",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK DIAGRAMS",
    "text": "NODE-LINK DIAGRAMS\n\n\n\nnodes: point marks\nlinks: line marks\n\nstraight lines or arcs\nconnections between nodes\n\nintuitive & familiar\n\nmost common\nmany, many variants"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#node-link-layout",
    "href": "BCB520/posts/L8-NetworkData/index.html#node-link-layout",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK LAYOUT",
    "text": "NODE-LINK LAYOUT\n\n\n\nminimize\n\nedge crossings, node overlaps\ndistances between topological neighbor nodes\ntotal drawing area\nedge bends\n\nmaximize\n\nangular distance between different edges\naspect ratio disparities\n\nemphasize symmetry\n\nsimilar graph structures should look similar in layout"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#node-link-challenges",
    "href": "BCB520/posts/L8-NetworkData/index.html#node-link-challenges",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK CHALLENGES",
    "text": "NODE-LINK CHALLENGES\nMany criteria directly conflict with each other."
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#optimization-based-layouts",
    "href": "BCB520/posts/L8-NetworkData/index.html#optimization-based-layouts",
    "title": "LECTURE 8 - Network Data",
    "section": "OPTIMIZATION BASED LAYOUTS",
    "text": "OPTIMIZATION BASED LAYOUTS\n\nformulate layout problem as optimization problem\nconvert criteria into weighted cost function\n\nF(layout) = a[crossing counts] + b[drawing space used]+…\n\nuse known optimization techniques to find layout at minimal cost\n\nenergy-based physics models\nforce-directed placement\nspring embedders"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#force-directed-placement",
    "href": "BCB520/posts/L8-NetworkData/index.html#force-directed-placement",
    "title": "LECTURE 8 - Network Data",
    "section": "FORCE DIRECTED PLACEMENT",
    "text": "FORCE DIRECTED PLACEMENT\n\n\n\nphysics model\n\nlinks = springs pull together\nnodes = magnets repulse apart\n\nalgorithm\n\nplace vertices in random locations\nwhile not equilibrium\ncalculate force on vertex as sum of\n\npairwise repulsion of all nodes\nattraction between connected nodes\n\nmove vertex by c * vertex_force"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#force-directed-proscons",
    "href": "BCB520/posts/L8-NetworkData/index.html#force-directed-proscons",
    "title": "LECTURE 8 - Network Data",
    "section": "FORCE DIRECTED PROS/CONS",
    "text": "FORCE DIRECTED PROS/CONS\n\n\n\nstrengths\n\nreasonable layout for small, sparse graphs\nclusters typically visible\nedge length uniformity\n\nweaknesses\n\nnondeterministic\ncomputationally expensive: O(n^3) for n nodes\n\neach step is n^2, takes ~n cycles to reach equilibrium\n\nnaive FD doesn’t scale well beyond 1K nodes\niterative progress: engaging but distracting"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#force-directed-idiom",
    "href": "BCB520/posts/L8-NetworkData/index.html#force-directed-idiom",
    "title": "LECTURE 8 - Network Data",
    "section": "FORCE DIRECTED IDIOM",
    "text": "FORCE DIRECTED IDIOM\n\n\n\nvisual encoding\n\nlink connection marks, node point marks\n\nconsiderations\n\nspatial position: no meaning directly encoded, eft free to minimize crossings\nproximity semantics?\n\nsometimes meaningful, sometimes an artifact of layout algorithm\ntension with length (long edges more visually salient than short)\n\n\ntasks\n\nexplore topology; locate paths, clusters\n\nscalability\n\nnode/edge density E &lt; 4N"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#force-directed-examples",
    "href": "BCB520/posts/L8-NetworkData/index.html#force-directed-examples",
    "title": "LECTURE 8 - Network Data",
    "section": "FORCE DIRECTED EXAMPLES",
    "text": "FORCE DIRECTED EXAMPLES\nForce Directed Graph\nDisjoint Force Directed\nOBSERVABLE D3 LIBRARY"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#circular-layouts-arc-diagrams",
    "href": "BCB520/posts/L8-NetworkData/index.html#circular-layouts-arc-diagrams",
    "title": "LECTURE 8 - Network Data",
    "section": "CIRCULAR LAYOUTS / ARC DIAGRAMS",
    "text": "CIRCULAR LAYOUTS / ARC DIAGRAMS\n\nrestricted node-link layouts: lay out nodes around circle or along line\ndata\n\noriginal: network\nderived: node ordering attribute (global computation)\n\nconsiderations: node ordering crucial to avoid excessive clutter from edge crossings\n\nexamples: before & after barycentric ordering\n\n\nArc Diagram"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#adjacency-matrix",
    "href": "BCB520/posts/L8-NetworkData/index.html#adjacency-matrix",
    "title": "LECTURE 8 - Network Data",
    "section": "ADJACENCY MATRIX",
    "text": "ADJACENCY MATRIX\nMatrix derived from network"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#node-order",
    "href": "BCB520/posts/L8-NetworkData/index.html#node-order",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE ORDER",
    "text": "NODE ORDER\nANIMATED VERSION built with D3 by Mike Bostock."
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#matrix-proscons",
    "href": "BCB520/posts/L8-NetworkData/index.html#matrix-proscons",
    "title": "LECTURE 8 - Network Data",
    "section": "MATRIX PROS/CONS",
    "text": "MATRIX PROS/CONS\nGood for topology tasks related to neighborhoods. Bad for topology tasks related to paths."
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#node-link-vs-matrix",
    "href": "BCB520/posts/L8-NetworkData/index.html#node-link-vs-matrix",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK vs MATRIX",
    "text": "NODE-LINK vs MATRIX\n\nnode-link diagram strengths\n\ntopology understanding, path tracing\nintuitive, flexible, no training needed\n\nadjacency matrix strengths\n\nfocus on edges rather than nodes\nlayout straightforward (reordering needed)\npredictability, scalability\nsome topology tasks trainable\n\nempirical studies:\n\nnode-link best for small networks\nmatrix best for large networks if tasks don’t involve path tracing"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#trees",
    "href": "BCB520/posts/L8-NetworkData/index.html#trees",
    "title": "LECTURE 8 - Network Data",
    "section": "TREES",
    "text": "TREES"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#node-link-trees",
    "href": "BCB520/posts/L8-NetworkData/index.html#node-link-trees",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK TREES",
    "text": "NODE-LINK TREES\n\n\n\nReingold-Tilford\n\ntidy drawings of trees\nexploit parent/child structure\nallocate space: compact but without overlap\nrectilinear and radial variants\n\n\nThese are COOOOOL!"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#radial-node-link-tree",
    "href": "BCB520/posts/L8-NetworkData/index.html#radial-node-link-tree",
    "title": "LECTURE 8 - Network Data",
    "section": "RADIAL NODE-LINK TREE",
    "text": "RADIAL NODE-LINK TREE\n\n\n\ndata\n\ntree\n\nencoding\n\nlink connection marks\npoint node marks\nradial axis orientation\n\nangular proximity: siblings\ndistance from center: depth in tree\n\n\ntasks\n\nunderstanding topology, following paths\n\nscalability\n\n1K - 10K nodes (with/without labels)\n\n\n\n EXAMPLE"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#connection-and-containment",
    "href": "BCB520/posts/L8-NetworkData/index.html#connection-and-containment",
    "title": "LECTURE 8 - Network Data",
    "section": "CONNECTION AND CONTAINMENT",
    "text": "CONNECTION AND CONTAINMENT\n\n\n\nmarks as links (vs. nodes)\n\ncommon case in network drawing\n1D case: connection\n\nex: all node-link diagrams\nemphasizes topology, path tracing\nnetworks and trees\n\n2D case: containment\n\nex: all treemap variants\nemphasizes attribute values at leaves (size coding)\nonly trees"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#treemap",
    "href": "BCB520/posts/L8-NetworkData/index.html#treemap",
    "title": "LECTURE 8 - Network Data",
    "section": "TREEMAP",
    "text": "TREEMAP\n\n\n\ndata\n\ntree\n1 quant attrib at leaf nodes\n\nencoding\n\narea containment marks for hierarchical structure\nrectilinear orientation\nsize encodes quant attrib\n\ntasks\n\nquery attribute at leaf nodes\nex: disk space usage within filesystem\n\nscalability\n\n1M leaf nodes\n\n\n\n\nTREEMAP\nVORONOI TREEMAP"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#implicit-tree-layouts",
    "href": "BCB520/posts/L8-NetworkData/index.html#implicit-tree-layouts",
    "title": "LECTURE 8 - Network Data",
    "section": "IMPLICIT TREE LAYOUTS",
    "text": "IMPLICIT TREE LAYOUTS\nAn alternative to connection and containment is position. These show parent-child relationships only through relative positions."
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#tree-idioms",
    "href": "BCB520/posts/L8-NetworkData/index.html#tree-idioms",
    "title": "LECTURE 8 - Network Data",
    "section": "TREE IDIOMS",
    "text": "TREE IDIOMS\n\n\n\ndata shown\n\nlink relationships\ntree depth\nsibling order\n\ndesign choices\n\nconnection vs containment link marks\nrectilinear vs radial layout\nspatial position channels\n\nconsiderations\n\nredundant? arbitrary?\ninformation density?\n\navoid wasting space\nconsider where to fit labels!"
  },
  {
    "objectID": "BCB520/posts/L8-NetworkData/index.html#arrange-networkstrees-1",
    "href": "BCB520/posts/L8-NetworkData/index.html#arrange-networkstrees-1",
    "title": "LECTURE 8 - Network Data",
    "section": "ARRANGE NETWORKS/TREES",
    "text": "ARRANGE NETWORKS/TREES\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "BCB520/posts/A7-SpatialData/index.html",
    "href": "BCB520/posts/A7-SpatialData/index.html",
    "title": "Practice with Spatial Data",
    "section": "",
    "text": "In this assignment, we’ll consider some of the tools and techniques for visualizing spatial data. Spatial data comes in two broad categories, geographic and spatial fields. Let’s practice a few visualizations to get a feel for how these things work!"
  },
  {
    "objectID": "BCB520/posts/A7-SpatialData/index.html#geographic-maps",
    "href": "BCB520/posts/A7-SpatialData/index.html#geographic-maps",
    "title": "Practice with Spatial Data",
    "section": "GEOGRAPHIC MAPS!",
    "text": "GEOGRAPHIC MAPS!\nIn class I bet Ronald that he would end up creating some kind of map based visualization before he graduated with his PHD. This is because he works on Malaria - a terrible disease with a strong spatial component to its risk levels. Let’s get some Malaria data and map it!\nThe data I obtained were from the Malaria Atlas. I downloaded a csv for 10 years of data for all the countries the had on file.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(dplyr)\n\nMalaria &lt;- read.csv(\"National_Unit_data.csv\")\n\nIncidence&lt;- Malaria%&gt;%\n  filter(Metric == \"Infection Prevalence\" & Year == \"2019\")%&gt;%\n  mutate(Prevalence = Value)%&gt;%\n  select(c(ISO3, Prevalence))\n\n\nNow I’m going to use the rnaturalearth package to create contry polygons. Then I’ll add the Malaria data to that data frame.\n\n\nCode\nworld_map &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmap_data &lt;- world_map %&gt;%\n  left_join(Incidence, by = c(\"iso_a3\" = \"ISO3\"))\n\n\nNow I will make a plot!\n\n\nCode\nggplot() +\n  geom_sf(data = map_data, aes(fill = Prevalence)) +\n  scale_fill_gradient(low = \"white\", high = \"red\", na.value = \"gray\", name = \"Malaria Prevalence\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Malaria Prevalence by Country\")\n\n\n\n\n\nOh SNAP! We made a MAP!\nHey! That rhymes!\nWhat is missing? Units? Is that actually prevalence? We sure left a lot of data on the table? Can we add some kind of time thing? Change the variable?"
  },
  {
    "objectID": "BCB520/posts/A7-SpatialData/index.html#spatial-fields",
    "href": "BCB520/posts/A7-SpatialData/index.html#spatial-fields",
    "title": "Practice with Spatial Data",
    "section": "SPATIAL FIELDS",
    "text": "SPATIAL FIELDS\nSpatial field data refers to data that has a continuous spatial distribution and can be measured at any location within the defined area. Here are some interesting examples of spatial field data:\n\nAir temperature: Air temperature data collected from weather stations or remote sensing technologies can be used to create temperature maps or to study climate change, urban heat islands, and other environmental phenomena. TROPICAL CYCLONE!\nPrecipitation: Rainfall, snowfall, or other forms of precipitation data collected from weather stations or satellites can be used to study the hydrological cycle, flood risk, droughts, or water resource management.\nSoil moisture: Soil moisture data collected from in situ sensors or remote sensing technologies can be used to study agricultural productivity, irrigation management, droughts, and land degradation. Elevation data (Digital Elevation Models, DEMs):\nElevation data collected from satellite-based radar, LiDAR, or photogrammetry can be used to study topography, watershed delineation, flood risk, landslides, or geomorphology.\nVegetation indices: Indices like the Normalized Difference Vegetation Index (NDVI) or Enhanced Vegetation Index (EVI) derived from satellite imagery can be used to study vegetation health, land cover change, deforestation, agricultural productivity, and carbon sequestration.\nAir quality: Data on air pollutants like PM2.5, PM10, NO2, SO2, O3, and CO collected from ground-based monitors or satellites can be used to study the impact of pollution on human health, urban planning, or environmental policy.\nOceanographic data: Sea surface temperature, salinity, and chlorophyll-a concentration data collected from buoys, ships, or satellites can be used to study ocean currents, climate change, or marine ecosystems. OCEAN CURRENTS!\nPopulation density: Spatially explicit population density data can be used to study urbanization, migration patterns, infrastructure planning, or public health.\nLand use and land cover: Land use and land cover data collected from satellite imagery can be used to study urban growth, deforestation, habitat fragmentation, or landscape ecology.\nSeismic activity: Spatial distribution of earthquakes and their magnitudes can be used to study tectonics, fault zones, seismic hazards, or infrastructure resilience. DARK NIGHTS IN ANTAKYA\nSPORTS! Let’s check out a baseball example!\n\n\n\nCode\n# install.packages(\"baseballr\")\nlibrary(remotes)\n\n\n# install_github(\"bayesball/CalledStrike\")\n\n\n\n\nCode\nlibrary(CalledStrike)\n\n\nLoading required package: shiny\n\n\nLoading required package: baseballr\n\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n\n\nLoading required package: metR\n\n\n\nAttaching package: 'metR'\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nCode\n#ShinyDemo()\n\n\nThis example is the Pitch_Locations example from byaesball’s CalledStrike Github Repository\n\nIntroduction\nThere are three functions for visualizing pitch locations.\n\nThe function location_compare() will graph the pitch location for a data frame or list of data frames.\nThe function location_count() will show the locations of pitches for a specific pitcher on a particular count.\nThe function location_count_compare() will graph the pitch locations for a specific pitcher for several values of the count.\n\n\n\nData\nThe package includes the dataset sc_pitchers_2019 that contains Statcast data for 20 pitchers for the 2019 season.\n\n\nPitch Locations for a List\nSuppose we want to compare the locations of the fastballs thrown by Aaron Nola and Trevor Bauer.\nI find the subset of data I need and then create a list dividing the data by pitcher.\n\n\nCode\nd &lt;- filter(sc_pitchers_2019, \n            pitcher %in% c(605400, 545333),\n            pitch_type == \"FF\")\nds &lt;- split(d, d$pitcher)\nnames(ds) &lt;- c(\"Bauer\", \"Nola\")\n\n\nNow we can construct the graph.\n\n\nCode\nlocation_compare(ds)\n\n\nWarning: Removed 22 rows containing non-finite values\n(`stat_density2d_filled()`).\n\n\n\n\n\n\n\nPitch Locations for a Specific Count\nSuppose we want to look at the locations of Aaron Nola’s pitches on a 0-0 count. I can find Nola’s MLBAM id number by use of the chadwick dataset (also included in the package) that contains the id numbers for all players.\n\n\nCode\nchadwick %&gt;% \n  filter(name_last == \"Nola\", name_first == \"Aaron\")\n\n\n  name_first name_last key_mlbam\n1      Aaron      Nola    605400\n\n\nTo produce the graph, type\n\n\nCode\nlocation_count(sc_pitchers_2019, \n               605400, \"Aaron Nola\", \"0-0\")\n\n\nWarning: Removed 4 rows containing non-finite values\n(`stat_density2d_filled()`).\n\n\n\n\n\n\n\nPitch Locations Across a Group of Counts\nSuppose we want to compare Nola’s pitch locations across the counts “0-0”, “1-0”, “0-1”, “0-2”\n\n\nCode\nlocation_count_compare(sc_pitchers_2019, \n               605400, \"Aaron Nola\", \n               \"R\", \"Offspeed\", \n               c(\"0-0\", \"1-0\", \"0-1\", \"0-2\"))\n\n\nWarning: Removed 4 rows containing non-finite values\n(`stat_density2d_filled()`).\n\n\n\n\n\nEND"
  },
  {
    "objectID": "BCB520/index.html",
    "href": "BCB520/index.html",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "This class will help students establish a foundational understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R programming language. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using R. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\n\n\nI am maintaining the course here and on its CANVAS PAGE (for enrolled students).\nBarrie’s GitHub\nRondald’s Portfolio\nJiyin’s Portfolio\nCody’s Portfolio\nErik’s Portfolio\nGGSIDE! A companion to ggplot for making side plots! COOL!\nAwesome Quarto: A potentially interesting repository of Quarto documents, talks, tools, examples, etc.\nThe MockUp Blog - TABLES! This blog post explores the R packages gt and gtextras which will help us up our table game!\nRiffomonas Project: Pat Schloss is a Professor at the University of Michigan. The Riffomonas Project is his Youtube channel, which has HUNDREDS of easy to follow and amazingly useful instructional videos on R, ggplot, version control, and literate programming.\nDr. Tamara Munzner’s Website: It isn’t fancy, but Dr. Munzner’s website has tons of resources from her textbook and the many data visualization courses she has offered. This includes recorded lectures that align directly with the chapters of the text, much like what we are using.\nCheat Sheets: So many visual guides for many R packages, including the tidyverse, ggplot, dplyr, etc.\nLearning Vis Tools: Teaching Data Visualization Tutorials An interesting paper for discussion as we forge the structure for this class."
  },
  {
    "objectID": "PolymorphicGames.html",
    "href": "PolymorphicGames.html",
    "title": "Polymorphic Games",
    "section": "",
    "text": "I am the cofounder (along with Professor Terry Soule) of Polymorphic Games, an experimental video game studio that applies game development technology to scientific research and STEM education. Polymorphic Games has released two evolutionary video games on Steam:\nDarwin’s Demons is an arcade style space shooter in which you battle a population of evolving aliens. The alien’s morphology, fire type, and behaviors are all encoded by a digital genome. The aliens that are best able to shoot or evade you reproduce to make the next generation.\nProject Hastur is an evolutionary tower defense game. Defend humanity against the ever changing threat of the alien Proteans. Project Hastur combines elements of 3D tower defense and real-time strategy with models of biological evolution to create a uniquely challenging experience.\nYou can learn more about our past and current projects at the Polymorphic Games website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BARRIE D. ROBISON",
    "section": "",
    "text": "// Evolution // Data Science // Video Games // Horror Fiction\nI am a Professor of Biological Sciences at the University of Idaho, where I study evolution and adaptation using an unusual mixture of model systems. Over the course of my career, I have conducted research on the genetic basis of quantitative traits in Salmonids, behavioral genomics and adaptation to captivity in zebrafish, and the implementation of evolutionary models in video games. I taught Genomics and General Genetics for 15 years, and I now teach various courses in Data Science as part of our new Graduate Certificate in Professional Applications of Data Science.\nI am the co-founder (along with Professor Terry Soule) of Polymorphic Games, an experimental video game studio that applies game development technology to scientific research and STEM education.\nI am the Director of the Institute for Interdisciplinary Data Sciences (IIDS).\nFinally, I am a huge Horror and Science Fiction nerd. I am fascinated by the intersection of these genres with science, and I explore these intersections in my BLOG - Professor Polymorphic."
  },
  {
    "objectID": "index.html#education-and-training",
    "href": "index.html#education-and-training",
    "title": "BARRIE D. ROBISON",
    "section": "Education and Training",
    "text": "Education and Training\nBachelor of Science in Biology, University of Victoria, 1988-1993\nMaster of Science in Fisheries, University of Idaho, 1993-1995\nPHD in Biology, Washington State University, 1995-1999\nPostdoctoral Researcher in Evolutionary Genetics, University of Oregon 1999-2000\nPostdoctoral Researcher in Evolutionary Genetics, Indiana University 2000-2003"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "BARRIE D. ROBISON",
    "section": "Contact",
    "text": "Contact\nBARRIE D. ROBISON, PhD\nProfessor, Department of Biological Sciences\nDirector, Institute for Interdisciplinary Data Sciences\nOffice: 208-885-7137\nEmail: brobison [at] uidaho.edu\nOffice of Research and Economic Development\nInstitute for Interdisciplinary Data Sciences\nwww.iids.uidaho.edu\nCollege of Science\nDepartment of Biological Sciences\nwww.uidaho.edu/sci/biology"
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "PUBLICATIONS",
    "section": "",
    "text": "In addition to the list below, you can find links to most of my papers at my Google Scholar page.\n\n\n\n\nReferences\n\nLoftie-Eaton, W., Crabtree, A., Perry, D., Millstein, J., Baytosh, J., Stalder, T., Robison, B. D., Forney, L. J., & Top, E. M. (2021). Contagious antibiotic resistance: Plasmid transfer among bacterial residents of the zebrafish gut. Applied and Environmental Microbiology, 87(9), e02735–20.\n\n\nMa, X., Sheneman, L., Robison, B., Harris, F., & Shi, X. (2021). Framework vs System: Our experience with the TickBase project. AGU Fall Meeting Abstracts, 2021, IN13A–08.\n\n\nJohnson, B. D., Fox, A., Wright, L. R., Carney, G. E., Robison, B. D., & Jones, A. G. (2020). Nothobranchius furzeri as an emerging model for mate choice: female choice revealed by animations. Behaviour, 158(1), 35–49.\n\n\nSoule, T., & Robison, B. (2019). Polymorphic Games: Creating an On-Campus, Commercial Game Studio. EdMedia+ Innovate Learning, 1041–1045.\n\n\nSamuel, B., Reed, A., Short, E., Heck, S., Robison, B., Wright, L., Soule, T., Treanor, M., McCoy, J., & Sullivan, A. (2018). Playable experiences at AIIDE 2018. Fourteenth Artificial Intelligence and Interactive Digital Entertainment Conference.\n\n\nSoule, T., Heck, S., Haynes, T. E., Wood, N., & Robison, B. D. (2017). Darwin’s demons: Does evolution improve the game? European Conference on the Applications of Evolutionary Computation, 435–451.\n\n\nTreanor, M., Warren, N., Reed, M., Smith, A., Ortiz, P., Coney, L., Sherman, L., Carré, E., Vivatvisha, N., & Harrell, D. (2017). Playable experiences at AIIDE 2017. Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 13, 308–314.\n\n\nGerritsen, A. T., New, D. D., Robison, B. D., Rashed, A., Hohenlohe, P., Forney, L., Rashidi, M., Wilson, C. M., & Settles, M. L. (2016). Full mitochondrial genome sequence of the sugar beet wireworm Limonius californicus (Coleoptera: Elateridae), a common agricultural pest. Genome Announcements, 4(1), e01628–15.\n\n\nSinger, M. L., Oreschak, K., Rhinehart, Z., & Robison, B. D. (2016). Anxiolytic effects of fluoxetine and nicotine exposure on exploratory behavior in zebrafish. PeerJ, 4, e2352.\n\n\nSoule, T., Robison, B. D., & Heckendorn, R. B. (2016). Co-evolution of Sensor Morphology and Behavior. Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion, 135–136.\n\n\nSukeena, J. M., Galicia, C. A., Wilson, J. D., McGinn, T., Boughman, J. W., Robison, B. D., Postlethwait, J. H., Braasch, I., Stenkamp, D. L., & Fuerst, P. G. (2016). Characterization and evolution of the spotted gar retina. Journal of Experimental Zoology Part B: Molecular and Developmental Evolution, 326(7), 403–421.\n\n\nChapalamadugu, K. C., Murdoch, B. M., Robison, B. D., Hill, R. A., & Murdoch, G. K. (2015). Oncorhynchus mykiss pax7 sequence variations with comparative analyses against other teleost species. SpringerPlus, 4(1), 1–11.\n\n\nChapalamadugu, K. C., VandeVoort, C. A., Settles, M. L., Robison, B. D., & Murdoch, G. K. (2014). Maternal Bisphenol A Exposure Impacts the Fetal Heart.\n\n\nChapalamadugu, K. C., VandeVoort, C. A., Settles, M. L., Robison, B. D., & Murdoch, G. K. (2014). Maternal bisphenol a exposure impacts the fetal heart transcriptome. PLoS One, 9(2), e89096.\n\n\nSherpa, T., Lankford, T., McGinn, T. E., Hunter, S. S., Frey, R. A., Sun, C., Ryan, M., Robison, B. D., & Stenkamp, D. L. (2014). Retinal regeneration is facilitated by the presence of surviving neurons. Developmental Neurobiology, 74(9), 851–876.\n\n\nBenner, M. J., Hardy, R. W., Settles, M. L., & Robison, B. D. (2013). Genetic variation within the selenoproteome influences anxiety‐related behaviors in zebrafish (Danio rerio). The Federation of American Societies for Experimental Biology.\n\n\nBenner, M. J., Settles, M. L., Murdoch, G. K., Hardy, R. W., & Robison, B. D. (2013). Sex-specific transcriptional responses of the zebrafish (Danio rerio) brain selenoproteome to acute sodium selenite supplementation. Physiological Genomics, 45(15), 653–666.\n\n\nOswald, M. E., Singer, M., & Robison, B. D. (2013). The quantitative genetic architecture of the bold-shy continuum in zebrafish, Danio rerio. PloS One, 8(7), e68828.\n\n\nDrew, R. E., Settles, M. L., Churchill, E. J., Williams, S. M., Balli, S., & Robison, B. D. (2012). Brain transcriptome variation among behaviorally distinct strains of zebrafish (Danio rerio). BMC Genomics, 13(1), 1–13.\n\n\nOswald, M. E., Drew, R. E., Racine, M., Murdoch, G. K., & Robison, B. D. (2012). Is behavioral variation along the bold-shy continuum associated with variation in the stress axis in zebrafish? Physiological and Biochemical Zoology, 85(6), 718–728.\n\n\nPham, M., Raymond, J., Hester, J., Kyzar, E., Gaikwad, S., Bruce, I., Fryar, C., Chanin, S., Enriquez, J., & Bagawandoss, S. (2012). Assessing social behavior phenotypes in adult zebrafish: Shoaling, social preference, and mirror biting tests. In Zebrafish protocols for neurobehavioral research (pp. 231–246). Humana Press, Totowa, NJ.\n\n\nRobison, B. D., Benner, M. J., Singer, M. L., & Oswald, M. E. (2012). A High-Throughput and Inexpensive Assay for Anxiety-Related Behaviors in the Zebrafish, Based on Place Preference and Latency to Feed. In Zebrafish protocols for neurobehavioral research (pp. 203–215). Humana Press, Totowa, NJ.\n\n\nSettles, M. L., Coram, T., Soule, T., & Robison, B. D. (2012). An improved algorithm for the detection of genomic variation using short oligonucleotide expression microarrays. Molecular Ecology Resources, 12(6), 1079–1089.\n\n\nSinger, M. L., Oswald, M. E., Wiedeback, B. D., & Robison, B. D. (2012). The genetic architecture of the bold-shy continuum in zebrafish, Danio rerio. INTEGRATIVE AND COMPARATIVE BIOLOGY, 52, E161–E161.\n\n\nWiedeback, B. D., Oswald, M. E., & Robison, B. D. (2012). The link between stress and behavior in zebrafish: is embryonic perturbation a factor? INTEGRATIVE AND COMPARATIVE BIOLOGY, 52, E191–E191.\n\n\nKanuga, M. K., Benner, M. J., Doble, J. A., Wilson‐Leedy, J. G., Robison, B. D., & Ingermann, R. L. (2011). Effect of aging on male reproduction in zebrafish (Danio rerio). Journal of Experimental Zoology Part A: Ecological Genetics and Physiology, 315(3), 156–161.\n\n\nRobison, B. D., & Thorgaard, G. H. (2011). Prospects and pitfalls of clonal fishes in the postgenomic era. Aquaculture Biotechnology, 55.\n\n\nSherpa, T., Hunter, S. S., Frey, R. A., Robison, B. D., & Stenkamp, D. L. (2011). Retinal proliferation response in the buphthalmic zebrafish, bugeye. Experimental Eye Research, 93(4), 424–436.\n\n\nBenner, M. J., Drew, R. E., Hardy, R. W., & Robison, B. D. (2010). Zebrafish (Danio rerio) vary by strain and sex in their behavioral and transcriptional responses to selenium supplementation. Comparative Biochemistry and Physiology Part A: Molecular & Integrative Physiology, 157(4), 310–318.\n\n\nChapalamadugu, K. C., Robison, B. D., Drew, R. E., Powell, M. S., Hill, R. A., Amberg, J. J., Rodnick, K. J., Hardy, R. W., Hill, M. L., & Murdoch, G. K. (2009). Dietary carbohydrate level affects transcription factor expression that regulates skeletal muscle myogenesis in rainbow trout. Comparative Biochemistry and Physiology Part B: Biochemistry and Molecular Biology, 153(1), 66–72.\n\n\nRobison, B. (2009). The Role of Model Organisms in Aquaculture Research: Transient and Permanent Advantages. Molecular Research in Aquaculture, 175.\n\n\nDrew, R. E., Rodnick, K. J., Settles, M., Wacyk, J., Churchill, E., Powell, M. S., Hardy, R. W., Murdoch, G. K., Hill, R. A., & Robison, B. D. (2008). Effect of starvation on transcriptomes of brain and liver in adult female zebrafish (Danio rerio). Physiological Genomics, 35(3), 283–295.\n\n\nOswald, M., & Robison, B. D. (2008). Strain-specific alteration of zebrafish feeding behavior in response to aversive stimuli. Canadian Journal of Zoology, 86(10), 1085–1094.\n\n\nRobison, B. D., Drew, R. E., Murdoch, G. K., Powell, M., Rodnick, K. J., Settles, M., Stone, D., Churchill, E., Hill, R. A., & Papasani, M. R. (2008). Sexual dimorphism in hepatic gene expression and the response to dietary carbohydrate manipulation in the zebrafish (Danio rerio). Comparative Biochemistry and Physiology Part D: Genomics and Proteomics, 3(2), 141–154.\n\n\nMoretz, J. A., Martins, E. P., & Robison, B. D. (2007). Behavioral syndromes and the evolution of correlated behavior in zebrafish. Behavioral Ecology, 18(3), 556–562.\n\n\nMoretz, J. A., Martins, E. P., & Robison, B. D. (2007). The effects of early and adult social environment on zebrafish (Danio rerio) behavior. Environmental Biology of Fishes, 80(1), 91–101.\n\n\nRobison, B. D. (2007). Variation in behavior and global patterns of gene expression among wild and domesticated zebrafish: implications for teleost aquaculture. Aquaculture, 272, S305.\n\n\nOswald, M. E., & Robison, B. D. (2006). Behavioral syndromes and adaptation to captivity in the zebrafish, Danio rerio. INTEGRATIVE AND COMPARATIVE BIOLOGY, 46, E105–E105.\n\n\nPapasani, M. R., Robison, B. D., Hardy, R. W., & Hill, R. A. (2006). Early developmental expression of two insulins in zebrafish (Danio rerio). Physiological Genomics, 27(1), 79–85.\n\n\nRobison, B. D., Drew, R. E., Settles, M., Churchill, E., Moretz, J., & Martins, E. P. (2006). Variation in gene expression among the brains of behaviorally distinct zebrafish strains. no evidence for parallel transcriptome evolution during domestication. INTEGRATIVE AND COMPARATIVE BIOLOGY, 46, E120–E120.\n\n\nMartinez, V., Thorgaard, G., Robison, B., & Sillanpaa, M. J. (2005). Posterior evidence of multiple QTL influencing early development in double haploid lines of rainbow trout, Oncorhynchus mykiss. Aquaculture, 247, 25.\n\n\nMartinez, V., Thorgaard, G., Robison, B., & Sillanpää, M. J. (2005). An application of Bayesian QTL mapping to early development in double haploid lines of rainbow trout including environmental effects. Genetics Research, 86(3), 209–221.\n\n\nRobison, B. D., & Rowland, W. (2005). A potential model system for studying the genetics of domestication: behavioral variation among wild and domesticated strains of zebra danio (Danio rerio). Canadian Journal of Fisheries and Aquatic Sciences, 62(9), 2046–2054.\n\n\nColbourne, J. K., Robison, B., Bogart, K., & Lynch, M. (2004). Five hundred and twenty‐eight microsatellite markers for ecological genomic investigations using Daphnia. Molecular Ecology Notes, 4(3), 485–490.\n\n\nRobison, B. D., & Thorgaard, G. H. (2004). The phenotypic relationship of a clonal line to its population of origin: rapid embryonic development in an Alaskan population of rainbow trout. Transactions of the American Fisheries Society, 133(2), 455–461.\n\n\nTrevarrow, B., & Robison, B. (2004). Genetic backgrounds, standard lines, and husbandry of zebrafish. In Methods in cell biology (Vol. 77, pp. 599–616). Academic Press.\n\n\nNichols, K. M., Young, W. P., Danzmann, R. G., Robison, B. D., Rexroad, C., Noakes, M., Phillips, R. B., Bentzen, P., Spies, I., & Knudsen, K. (2003). A consolidated linkage map for rainbow trout (Oncorhynchus mykiss). Animal Genetics, 34(2), 102–115.\n\n\nThorgaard, G. H., Wheeler, P. A., Young, W. P., Robison, B. D., & Ristow, S. S. (2003). Genetic analysis of complex traits using clonal rainbow trout lines. In Aquatic genomics (pp. 395–398). Springer, Tokyo.\n\n\nMartinez, V., Sillanpaä, M., Thorgaard, G., Robinson, B., Woolliams, J., & Knott, S. (2002). Evidence of a pleiotropic QTL influencing components of early development in double haploid lines of rainbow trout. 7th World Congress on Genetics Applied to Livestock Production, 6–8.\n\n\nThorgaard, G. H., Bailey, G. S., Williams, D., Buhler, D. R., Kaattari, S. L., Ristow, S. S., Hansen, J. D., Winton, J. R., Bartholomew, J. L., & Nagler, J. J. (2002). Status and opportunities for genomics research with rainbow trout. Comparative Biochemistry and Physiology Part B: Biochemistry and Molecular Biology, 133(4), 609–646.\n\n\nThorgaard, G., Bailey, G., Williams, D., Buhler, D., Kaattari, S., Ristow, S., Hansen, J., Winton, J., Bartholomew, J., & Nagler, J. (2002). A white paper advocating complete sequencing of the genome of the rainbow trout, Oncorhynchus mykiss.\n\n\nBrunelli, J. P., Robison, B. D., & Thorgaard, G. H. (2001). Ancient and recent duplications of the rainbow trout Wilms’ tumor gene. Genome, 44(3), 455–462.\n\n\nRobison, B. D., Wheeler, P. A., Sundin, K., Sikka, P., & Thorgaard, G. H. (2001). Composite interval mapping reveals a major locus influencing embryonic development rate in rainbow trout (Oncorhynchus mykiss). Journal of Heredity, 92(1), 16–22.\n\n\nZhang, H., Robison, B., Thorgaard, G. H., & Ristow, S. S. (2000). Cloning, mapping and genomic organization of a fish C-type lectin gene from homozygous clones of rainbow trout (Oncorhynchus mykiss). Biochimica Et Biophysica Acta (BBA)-Gene Structure and Expression, 1494(1-2), 14–22.\n\n\nZhang, H., Robison, B., Thorgaard, G., & Ristow, S. (2000). Genes which possess sequence similarity to mammalian C-type lectins mediating natural killer cell activity are present in rainbow trout. FASEB JOURNAL, 14, A1053–A1053.\n\n\nRobison, B. D. (1999). Quantitative and molecular genetic analysis of embryonic development rate in rainbow trout (Oncorhynchus mykiss). Washington State University.\n\n\nRobison, B. D., Wheeler, P. A., & Thorgaard, G. H. (1999). Variation in development rate among clonal lines of rainbow trout (Oncorhynchus mykiss). Aquaculture, 173(1-4), 131–141.\n\n\nRistow, S. S., Grabowski, L. D., Ostberg, C., Robison, B., & Thorgaard, G. H. (1998). Development of long-term cell lines from homozygous clones of rainbow trout. Journal of Aquatic Animal Health, 10(1), 75–82.\n\n\nRobison, B. D. (1995). Genetic relationships between maternal lineages of sympatric anadromous and resident forms of Oncorhynchus nerka determined through PCR RFLP analysis of mtDNA. University of Idaho."
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html",
    "href": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "",
    "text": "The idea of Literate Programming is that source code that is executed as part of the program’s purpose is interspersed with documentation that describes the program’s logic. The concept of literate programming was first articulated by David Knuth in 1984. You know… back when music was good? Modern Data Science leans pretty heavily on literate programming, and to be honest, there aren’t very many good arguments as to why you WOULDN’T want to implement this approach in your own work. Bearing this in mind, we will adopt this framework for most of the activities, exercises, and assignments in this course. All of us will benefit by practicing these skills."
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html#literate-programming-publishing-systems",
    "href": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html#literate-programming-publishing-systems",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "LITERATE PROGRAMMING PUBLISHING SYSTEMS",
    "text": "LITERATE PROGRAMMING PUBLISHING SYSTEMS\nI’m trying to keep this course as technology agnostic as I can. The idea is that you should be practicing and building competencies in the languages and algorithms that are most useful to you. Who am I to tell you to use R instead of Python? If you have skills in a particular language I encourage you to keep using that during this course. That being said, I am going to work the examples using R and R Studio, and I will (mostly) use Quarto as the literate programming framework.\nIf all of this is new to you, no problem. Just follow along in R and Quarto and start your skill building journey with those languages.\nIf you are a Python person, great! Quarto can accommodate that language as well. If you have another preference for literate programming, such as sticking with R Markdown until the Quarto bugs are fixed, that is great. Find the framework and tools that work for you, and practice, practice, practice!\n\nQuarto\nAn open source publishing system that allows you to create websites, documents, blogs, books, publications, presentations, and more while using R, Python, Julia, or Observable. Quarto is intended to be the more functional successor of R Markdown. I intend to use Quarto for most of my work in this course.\n\n\nR Markdown\nAnother publishing system for creating all the things … websites, slides, manuscripts, dashboards, etc. While most people (including me!) instinctively think of R and Python within R Markdown, the list of supported language engines is pretty extensive.\n\nnames(knitr::knit_engines$get())\n\n [1] \"awk\"       \"bash\"      \"coffee\"    \"gawk\"      \"groovy\"    \"haskell\"  \n [7] \"lein\"      \"mysql\"     \"node\"      \"octave\"    \"perl\"      \"php\"      \n[13] \"psql\"      \"Rscript\"   \"ruby\"      \"sas\"       \"scala\"     \"sed\"      \n[19] \"sh\"        \"stata\"     \"zsh\"       \"asis\"      \"asy\"       \"block\"    \n[25] \"block2\"    \"bslib\"     \"c\"         \"cat\"       \"cc\"        \"comment\"  \n[31] \"css\"       \"ditaa\"     \"dot\"       \"embed\"     \"eviews\"    \"exec\"     \n[37] \"fortran\"   \"fortran95\" \"go\"        \"highlight\" \"js\"        \"julia\"    \n[43] \"python\"    \"R\"         \"Rcpp\"      \"sass\"      \"scss\"      \"sql\"      \n[49] \"stan\"      \"targets\"   \"tikz\"      \"verbatim\"  \"ojs\"       \"mermaid\"  \n[55] \"include\""
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html#languages-and-toolsets",
    "href": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html#languages-and-toolsets",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "LANGUAGES AND TOOLSETS",
    "text": "LANGUAGES AND TOOLSETS\nThere are quite a few, but the five that seemed to keep coming up as I prepped this course are:\n\nR\nA very powerful open source framework for statistical computing and graphics. R has a lot of base functionality, and its capabilities are increased by 100 fold with packages created by R users. Packages are the core units of R code. I’m going to use R for the vast majority of demonstrations in this course.\n\n\nPython\nPython is an open source general purpose programming language. It wasn’t developed just for statistical computing or data science, and people use this language for tons of different applications. There is no denying it has become a very powerful language for data science and data visualization.\n\n\nTableau\nTableau is proprietary software that is very powerful for creating beautiful and functional data visualizations. It can integrate with all sorts of data sources and is used a lot for analytics, especially in the business world. The downsides (that occur to me at least) are that it costs money, it is not open source, and is more of a one-trick-pony than the programming languages on this list.\n\n\nJavascript\nJavascript has been around for about 25 years, and is (I think) the world’s most popular programming language. Along with HTML and CSS, Javascript drives pretty much the entire internet. I mention Javascript here because it has the D3 library, which can create super cool interactive data visualizaitons. In my experience, the learning curve with Javascript and D3 was pretty steep. I bought a book about it once, but just haven’t been able to allocate the amount of time necessary to really start using it. Check out the gallery of examples. Amazing!\n\n\nObservable / D3\nObservable is a set of extensions to Javascript that features something called reactive runtime. This means that the code blocks are executed and compiled as they are written, and changes are implemented instantaneously. Observable is pretty great for data exploration, and is well supported by Quarto. In addition, you can use the Observable JS libraries in Quarto to access D3. We’ll use some of these tools in this course, especially when we start considering interactivity."
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html#assignment",
    "href": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html#assignment",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nAfter that long introduction, I suppose you are wondering what I want you to actually DO today.\nWell, I want you to set up your publishing system and preferred language on your computer. Then I want you to recreate the classic figure from Anscombe’s Quartet.\nNow, you might be asking…\n“How am I supposed to do that? You haven’t taught me how to do anything yet!”\nHere is the dirty little secret of modern education.\nThe Internet Exists.\nWhile I could use up an entire 90 minute lecture telling you how to:\n\nDownload and install R, R-Studio, and Quarto (included by default with R-Studio).\nCreate a Quarto document that will publish in the .html format\nInstall the R packages you will need\nTidy up the Anscombe’s Quartet data\nCalculate the summary statistics for each x y pair\nMake a nice little plot…\n\nI’m not going to do that.\nInstead, I want you to use the resources I point towards, or other resources that make more sense to you, to figure out how to do those things."
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html#resources",
    "href": "TEACHING/BCB521/Assignments/A1-Lit-Prog/index.html#resources",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "RESOURCES",
    "text": "RESOURCES\nTidyverse and Anscombe’s Quartet\nHandy cheat-sheets for many different R packages\nTutorial 1 - Literate Programming\nTutorial 2 - Literate Programming and Anscombe’s Quartet\nTutorial 3 - Python"
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/index.html",
    "href": "TEACHING/BCB521/Assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Stuff about Assignments"
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/index.html#posts",
    "href": "TEACHING/BCB521/Assignments/index.html#posts",
    "title": "Assignments",
    "section": "Posts",
    "text": "Posts"
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html",
    "href": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html",
    "title": "ASSIGNMENT 3",
    "section": "",
    "text": "Enough with the theory and conceptual mumbo jumbo! Let’s get down to making a visualization and posting it somewhere for all the world to see! The basic idea of this assignment is to set up a repository that will serve as an experimental portfolio, and then create your first novel visualization element inside the portfolio. Its gonna be fun! Or at least educational…"
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html#assignment",
    "href": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html#assignment",
    "title": "ASSIGNMENT 3",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThis assignment has two parts. The first part is technical. We’ll set up a Quarto Blog project as a new repository in your GitHub account. Then you’ll be a Blogger! Prestigious! The second part should be more fun. We are going to create your first Blog post as a visualization that explores an ACTION - TARGET pair relevant to your data set from Assignment 2."
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "href": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "title": "ASSIGNMENT 3",
    "section": "PART 1 TECHNICAL SORCERY",
    "text": "PART 1 TECHNICAL SORCERY\n\n1A - Create your BLOG project.\nHopefully by now you have created / dusted off / logged in to your GitHub account. Go ahead and log in to your account on the web and leave it open in a tab in your browser. There is a really great video about the next few steps (also linked below in RESORUCES) from Posit, but I’m giving you the condensed version here.\n\nFire up RStudio.\nGo to File-&gt;New Project and then select NEW DIRECTORY.\nNow select QUARTO BLOG.\nYou are going to create a the project in a new working directory. It is best practice to put this in a senstible directory structure on your local hard drive where your other GitHub repositories also live. Here is what mine looks like:\n\n 4. I suggest making the directory name something informative, like BCB504Portfolio, but hey… If you want to call your repository HasturBoxerShorts I won’t stop you. 5. Most of you will select Knitr as your Engine, but Cody “Mr. Hacker McPythonPants” might select Jupyter. 6. Check Create a git repository. The other boxes are optional and we can talk more about them later. 7. Click CREATE PROJECT.\nNow you’ve got a BLOG template all set up! Ha Ha! Onward to Internet Fame!\n\n\n1B - Make the BLOG about you.\nWe won’t spend a ton of time here, because this will be an ongoing process. You’ll go and watch all those cool videos and tutorials this weekend to figure this out. But lets do a couple things.\n\nMaybe you should modify the about.qmd file so that your name is in there somewhere.\nMaybe you should modify the index.qmd file with a better title in the YAML header.\nMaybe you should navigate to the posts folder, open the Welcome to my blog folder, open index.qmd from that directory, and add a sentence or two.\n\n\n\n1C - Customize your first post.\n\nNavigate to the posts folder, open the post with code folder, and open index.qmd.\nReplace ALL of the content of index.qmd with the most recent version of your .qmd file from ASSIGNMENT 2. Keep the file name index.qmd. Save that file!\nMove your data files to the post with code folder.\nRender the index.qmd file from this folder. Hopefully it worked!\n\n\n\n1D - Render the BLOG as a website.\n\nIMPORTANT Open your _quarto.yml file and add output-dir: docs under project:\n\n\nThe indentations matter here.\n\nSave all the files you’ve modified.\nGo to the BUILD tab in the (probably) top right section of RStudio.\nClick RENDER WEBSITE.\nClick through your new Blog and see how it works!\n\n\n\n1E - Push to GitHub.\nThere are quite a few ways to do this part. I’m going to use GitHub Desktop, but those video will show you other ways.\n\nGo to GitHub Desktop.\nType some text in the summary box.\nClick COMMIT TO MASTER.\nClick PUSH ORIGIN.\nGo to your GitHub in your browser. You should see your new repository! Yay!\n\n\n\n1F - Make it a website with GitHub pages.\n\nIn your browser, click on your repository.\nGo to SETTINGS.\nSelect PAGES.\nSet the SOURCE option to Deploy from a branch.\nSet the BRANCH to master and the directory to docs\nDeploy that stuff and wait. Then visit your site!"
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "href": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "title": "ASSIGNMENT 3",
    "section": "PART 2 ELDRITCH VISUALIZATION RITUAL",
    "text": "PART 2 ELDRITCH VISUALIZATION RITUAL\n\n2A Define your ACTION - TARGET pair(s)\nIn [LECTURE 3] we discussed the concept of Task Abstraction in which you define the viz task that you want to help the user accomplish. This was represented as sets of ACTIONS that the user would perform (e.g. Discover, Present, Browse, Identify) on TARGETS related to the data set (e.g. Trends, Attributes, etc.).\nThink about one or two visualizations you wish to construct with your data, and try to define them in terms of ACTION - TARGET pairs. While you are at it, why don’t you update the index.qmd file of your BLOG POST with a new seciton at the bottom titled TASK ABSTRACTION, and put a sentence describing your visualizations and the ACTION - TARGET pairs they represent?\n\n\n2B Construct your Visualization\nLet’s get to work! Using whatever tools you can, code up your visualization in that new section of your BLOG post. You can check out how I approached this part in TUTORIAL 4."
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html#resources",
    "href": "TEACHING/BCB521/Assignments/A3-PrototypeVizPortfolio/index.html#resources",
    "title": "ASSIGNMENT 3",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nQuarto’s BLOG Documentation\nA YouTube Video from Posit on Building a BLOG with Quarto"
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A2-YourData/index.html",
    "href": "TEACHING/BCB521/Assignments/A2-YourData/index.html",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "",
    "text": "A key feature of this course is that students should be using their own data whenever possible. This is critical to forging a learning experience that is customized to each student’s aspirations and the eccentricities of their chosen research domain. This assignment begins the process of helping you identify the data sets with which you want to work, and aligns with the notion of understanding the concepts of Data Semantics and Data Abstraction."
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A2-YourData/index.html#assignment",
    "href": "TEACHING/BCB521/Assignments/A2-YourData/index.html#assignment",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThe basic structure of this assignment is for you to identify, import, describe, and host a data set. I’ll break down the specifics for each of these actions below.\n\nIdentify a Data Set\nThe main criteria is that the data set has to matter to you in some way. Often, this will mean that it is your data set. It was collected by you and has a central role in your current or past graduate research. Awesome! Another scenario is that the data you want to use comes from your current job. Maybe it isn’t part of a research project, but you are motivated to learn how to better visualize the data or you are very interested in learning more about it. Also Awesome!\nSome of you might not have your own data. Perhaps you have just started your graduate training. Maybe your job doesn’t yet have data that you need to work with. No Problem!\nIt is perfectly fine to find publicly available data sets online. As long as the data set is interesting to you! You just need to make sure that the data:\n\nAre publicly available.\nAre not restricted by some kind of license or copyright.\nDo not contain private information.\nAre not covered by HIPPA, FERPA, CMMC, or other federal regulations related to data.\n\nIf you need help finding a data set, just let me know.\nSome fun potential categories for data sources include:\n\nSports Analytics from your favorite sport or team.\nPublicly available genomics data bases.\nKeggle.\nThe movie data base.\nClassic data sets from your field.\n\n\n\nImport the Data Set\nThis one is probably straightforward if your data set comes from your own research and lives on your local hard drive already.\n\n\nDescribe the Data Set\nThis is the bulk of the assignment. I want you to use the framework described in Dr. Munzner’s textbook to understand your data set and describe it to someone who is unfamiliar with your work. The basis of this approach is descibed in this lecture. In addition, this figure from the textbook summarizes the kinds of data types, data set types, and attribute types you might have in your data:\n\n\n\nBONUS OBJECTIVE: Host your Data Set\nUltimately, we are moving toward each of you hosting your assignments within an online repository that can serve as your data science portfolio. For this course, we are going to assume this is GitHub. At the very least, I want everyone to create (or dust off and log in to) your own GitHub account. We’ll try to use this assignment to set up a project repository, and perhaps even a simple web site using GitHub pages."
  },
  {
    "objectID": "TEACHING/BCB521/Assignments/A2-YourData/index.html#resources",
    "href": "TEACHING/BCB521/Assignments/A2-YourData/index.html#resources",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nBarrie attempts this assignment in Tutorial 4."
  },
  {
    "objectID": "TEACHING/BCB521/Resources/Certificate/index.html",
    "href": "TEACHING/BCB521/Resources/Certificate/index.html",
    "title": "CERTIFICATE",
    "section": "",
    "text": "Learn how to think about, organize, analyze, and visualize data. Communicate data-driven insights to technical and lay audiences."
  },
  {
    "objectID": "TEACHING/BCB521/Resources/Certificate/index.html#overview",
    "href": "TEACHING/BCB521/Resources/Certificate/index.html#overview",
    "title": "CERTIFICATE",
    "section": "OVERVIEW",
    "text": "OVERVIEW\nWe live in an increasingly data-driven world. Basic data literacy and data science skills are becoming central to virtually every industry. Yet, limited opportunities exist to gain these skills without an advanced background in math and computer science. To address this workforce development need, we propose a competitively valued on-line graduate certificate in the Professional Applications in Data Science. The certificate is designed to offer rigorous training in the foundations of data science to anyone with a bachelor’s degree. Participants will learn how to think about, organize, analyze, and visualize data, and communicate data driven insights to diverse audiences. The curriculum emphasizes the use of data sets drawn from each student’s individual discipline, aligning the certificate’s workforce development impacts with the University of Idaho’s land grant mission.\n\nValue Proposition:\nThe graduate certificate in Professional Applications in Data Science will provide unique value to UI constituencies by:\n\nAligning data science training with fields of nascent demand that are part of our land grant mission, including Agriculture, Natural Resources, and Education.\nRequiring a digital data science portfolio with which students can demonstrate their proficiencies to potential employers.\nEmphasizing training in data communication - including verbal presentation and data visualization - two components of data science that are underrepresented in competing certificates.\nFilling a growing workforce development gap by offering a unique data science certificate that is appropriate for professionals with a bachelor’s degree who do not have a rigorous background in mathematics, statistics, or computer science.\n\n\n\nIntended Audience:\nThis certificate leverages the University of Idaho’s interdisciplinary culture to provide integrative training in the foundations of data science. It is intended for:\n\nWorking professionals with a bachelor’s degree whose career increasingly involves the generation, management, analysis, and visualization of large data sets. The certificate is appropriate for professionals in STEM fields, Health Care, Business, Government, Education, Journalism, Athletics, Natural Resources, and Agriculture.\nGraduate students in programs outside of the core technical disciplines of data science (statistics, math, engineering, or computer science). The certificate will complement disciplinary research methods courses with training in data management, visualization, and communication.\nUndergraduates at the UI who wish to incorporate data science training into their degree and graduate with a Bachelor’s degree and a graduate certificate.\n\n\n\nStudent Learning Outcomes:\nUpon completion of the certificate, students will be able to:\n\nUse open-source software to reproducibly manage, analyze, and visualize large, complex, and noisy data sets.\nPractice high quality and ethical data stewardship.\nUnderstand and execute data exploration.\nEffectively communicate data driven insights to experts and non-experts.\nDemonstrate their skills with an online portfolio of analyses and visualizations relevant to their field of specialization."
  },
  {
    "objectID": "TEACHING/BCB521/Resources/Certificate/index.html#curriculum",
    "href": "TEACHING/BCB521/Resources/Certificate/index.html#curriculum",
    "title": "CERTIFICATE",
    "section": "CURRICULUM",
    "text": "CURRICULUM\n\nPrerequisites:\nA Bachelor’s degree OR the student has senior standing and is enrolled in a bachelor’s degree program at the University of Idaho.\n\n\nCertificate Requirements (12 Credits Total)\n\n\n\n\n\n\n\n\n\n\n\n\nCourse\nName\nCredits\nPrerequisites\nSchedule\n\n\n\n\nINTR 509\nIntroduction to Applied Data Science\n3\nBS degree or permission\nSpring and asynchronous online\n\n\nBCB 551\nCommunicating with Data\n2\nINTR 509 or BS degree or permission\nFall and asynchronous online\n\n\nBCB 520\nData Visualization\n3\nSTAT 251 or INTR 509 or permission\nSpring and asynchronous online\n\n\nBCB 522\nData Science Portfolio\n1\nINTR 509 and BCB 520 (Data Viz)\nAsynchronous online\n\n\nElective\nVaries\n3\nVaries\nVaries\n\n\n\n\n\nnote: Courses designated with “BCB 5XX” are new courses that will be offered in the 2023-24 academic year\n\n\nCourse Descriptions\n\nINTR 509 Introduction to Applied Data Science (3 credits)\nIn person (spring) and asynchronous online.\nStudents are provided a foundation for “thinking with data” through the introduction of computational, statistical, and data literacy skills. This includes the selection, collection, cleaning, management, descriptive analysis, and exploratory analysis of a dataset unique to their professional interests using modern computing languages. This course is taught by Dr. Michael Overton.\n\n\nBCB 521 Communicating with Data (2 credits)\nIn person (fall) and asynchronous online.\nStudents are taught writing and presentation skills to improve their communication of data-driven insights to specialist and lay audiences. The course emphasizes reproducible research practices, including literate programming (R Markdown) and version control (GitHub). Course content includes the conceptual foundations of communicating with data along with written and verbal communication assignments using data sets individualized to each student’s interest.\nText: Nolan and Stoudt. 2021. Communicating with data: The art of writing for data science. Oxford University Press.\nPrerequisites: INTR 509 OR A BS degree OR permission.\n\n\nBCB 520 Data Visualization (3 credits)\nIn person (spring) and asynchronous online\nThis course covers the conceptual foundations of data visualization and design. Students will learn how visualization design choices related to marks and channels, color, and spatial arrangement interact with the human perceptual system. The course considers tabular, network, and spatial data, and students will implement visualizations in R.\nText: Munzner. 2014. Visualization Analysis & Design. CRC Press.\nPrerequisites: INTR 509 OR A BS degree OR Stat 251 OR Permission.\n\n\nBCB 522 Online Portfolio (1 credit)\nAsynchronous online\nThis course provides feedback, review, and approval of the student’s online data science portfolio. This portfolio is intended to represent the body of work accumulated by the student over the course of the certificate. It should contain examples of novel data products (such as FAIR data sets), analyses, and visualizations. All elements of the portfolio will be hosted online (likely in a GitHub repository or professional website), be open source, and demonstrate best practices of literate programming and reproducible research.\n\n\nElectives:\nThe certificate allows each student to customize their training by choosing a 3-credit graduate elective.\nFor students seeking foundational training who have not already taken Stat 431 or its equivalent, we recommend Stat 431 or a 3-credit graduate level disciplinary research methods course.\nFor students seeking to add the certificate to an existing degree at UI, or students who already have some advanced technical training, additional electives are possible. Note that many of these optional electives have substantial disciplinary pre-requisites. Not all electives are available in an online format.\n\n\nChoose one of the following:\n\n\n\n\n\nCourse\nName\nCredits\nPrerequisites\n\n\n\n\nAVS 531\nPractical Methods in Analyzing Animal Science Experiments\n3\n400-level statistics course\n\n\nBE 521\nImage Processing and Computer Vision\n3\n(BE 242 and MATH 275) or permission\n\n\nBE 541\nInstrumentation and Measurements\n3\nENGR 240; Coreqs: STAT 301\n\n\nBIOL 526\nSystems Biology\n3\n(BIOL 115, BIOL 115L and MATH 170) or permission of instructor\n\n\nBIOL 545\nPhylogenetics\n3\nPLSC 205 or BIOL 213 and BIOL 310\n\n\nBIOL 549\nComputer Skills for Biologists\n3\nBIOL 310 and STAT 251 or STAT 301; or Permission\n\n\nBIOL 563\nMathematical Genetics\n3\nMATH 160 or MATH 170 and STAT 251 or STAT 301\n\n\nCE 526\nAquatic Habitat Modeling\n3\nA minimum grade of ‘C’ or better is required for all pre/corequisites; Prereqs: CE 322 and CE 325 or BE 355; or Permission\n\n\nCE 579\nSimulation of Transportation Systems\n3\nPermission\n\n\nCS 511\nParallel Programming\n3\nCS 395\n\n\nCS 574\nDeep Learning\n3\n(CS 121 or MATH 330) and STAT 301\n\n\nCS 570\nArtificial Intelligence\n3\nCS 210\n\n\nCS 572\nEvolutionary Computation\n3\nCS 211\n\n\nCS 575\nMachine Learning\n3\nCS 210\n\n\nCS 577\nPython for Machine Learning\n3\n(CS 121 or MATH 330) and STAT 301\n\n\nCS 578\nNeural Network Design\n3\nPermission\n\n\nCS 579\nData Science\n3\nMATH 330 or Permission\n\n\nCS 589\nSemantic Web and Open Data\n3\nCS 360 or CS 479 or CS 579\n\n\nCTE 519\nDatabase Applications and Information Management\n3\nNA\n\n\nCYB 520\nDigital Forensics\n3\nCYB 310\n\n\nED 571\nIntroduction to Quantitative Research\n3\nGraduate standing\n\n\nED 584\nUnivariate Quantitative Research in Education\n3\nED 571\n\n\nED 587\nMultivariate Quantitative Analysis in Education\n3\nED 584 or Permission\n\n\nED 589\nTheoretical Applications and Designs of Qualitative Research\n3\nED 574 or Permission\n\n\nED 590\nData Analysis and Interpretation of Qualitative Research\n3\nED 574 and ED 589\n\n\nED 591\nIndigenous and Decolonizing Research Methods\n3\nNA\n\n\nED 592\nDecolonizing, Indigenous, and Action-Based Research Methods\n3\nNA\n\n\nED 595\nSurvey Design for Social Science Research\n3\nRecommended Preparation: Foundations of Research course at graduate level.\n\n\nEDAD 570\nMethods of Educational Research\n3\nNA\n\n\nENT 504\nApplied Bioinformatics\n3\nPermission\n\n\nENVS 511\nData Wizardry in Environmental Sciences\n3\nSTAT 251\n\n\nENVS 551\nResearch Methods in the Environmental Social Sciences\n3\nOne course or experience in basic statistics or Instructor Permission\n\n\nFOR 514\nForest Biometrics\n3\nSTAT 431 or equivalent\n\n\nFOR 535\nRemote Sensing of Fire\n3\nFOR 375 or permission\n\n\nGEOG 507\nSpatial Statistics and Modeling\n3\nSTAT 431 or permission\n\n\nGEOG 583\nRemote Sensing/GIS Integration\n3\nCoreqs: GEOG 385 or equivalent.\n\n\nMath 538\nStochastic Models\n3\nMATH 451 or Permission\n\n\nMIS 555\nData Management for Big Data\n3\nNA\n\n\nNRS 578\nLidar and optical remote sensing analysis using open-source software\n3\nSTAT251 & WLF370 or STAT427 and NRS/FOR 472 or equivalent/instructor permission\n\n\nPOLS 558\nResearch Methods for Local Government and Community Administration\n3\nSTAT 251\n\n\nREM 507\nLandscape and Habitat Dynamics\n3\nPermission; Recommended Preparation: courses in ecology, statistics, and GIS.\n\n\nStat 431\nStatistical Analysis\n3\nSTAT 251 or STAT 301\n\n\nSTAT 514\nNonparametric Statistics\n3\nSTAT 431\n\n\nSTAT 516\nApplied Regression Modeling\n3\nSTAT 431\n\n\nStat 517\nStatistical Learning and Predictive Modeling\n3\nSTAT 431\n\n\nStat 519\nMultivariate Analysis\n3\nSTAT 431 or equivalent.\n\n\nSTAT 535\nIntroduction to Bayesian Statistics\n3\nSTAT 431\n\n\nSTAT 555\nStatistical Ecology\n3\nMATH 451 or Permission\n\n\nStat 565\nComputer Intensive Methods\n3\n STAT 451, STAT 452, MATH 330, and computer programming experience or Permission\n\n\nWLF 552\nEcological Modeling\n3\nMATH 175 and FOR 221 or Permission.\n\n\nWLF 555\nStatistical Ecology\n3\nMATH 451 or permission\n\n\nWR 552\nWater Economics and Policy\n3\nAGEC 301 or AGEC 302, or ECON 351 or ECON 352, or by permission"
  },
  {
    "objectID": "TEACHING/BCB521/Resources/Certificate/index.html#general-university-requirements",
    "href": "TEACHING/BCB521/Resources/Certificate/index.html#general-university-requirements",
    "title": "CERTIFICATE",
    "section": "GENERAL UNIVERSITY REQUIREMENTS",
    "text": "GENERAL UNIVERSITY REQUIREMENTS\nIn addition to the requirements specified in this document, the certificate would be subject to all UI Policies regarding Graduate Certificates."
  },
  {
    "objectID": "TEACHING/BCB521/Resources/T3-Python/index.html",
    "href": "TEACHING/BCB521/Resources/T3-Python/index.html",
    "title": "TUTORIAL 3 - Python",
    "section": "",
    "text": "I relied heavily on the Quarto documentation for this tutorial.\nThe basic things I learned were:\n\nInstall Python (again). I downloaded the newest version from the Python website.\nInstall the `jupyter’ package using the terminal:\n\n\nWindows: py -m pip install jupyter\nMac: python3 -m pip install jupyter\n\n\nInstall the modules you need using the terminal (Mac examples below):\n\n\npython3 -m pip install numpy\npython3 -m pip install matplotlib\n\nAfter I did this, the code from the Quarto website example (below) seemed to work!"
  },
  {
    "objectID": "TEACHING/BCB521/Resources/T3-Python/index.html#quarto-example-code",
    "href": "TEACHING/BCB521/Resources/T3-Python/index.html#quarto-example-code",
    "title": "TUTORIAL 3 - Python",
    "section": "QUARTO EXAMPLE CODE",
    "text": "QUARTO EXAMPLE CODE\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "TEACHING/BCB521/Resources/index.html",
    "href": "TEACHING/BCB521/Resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Lectures, Readings, Online Resources, etc."
  },
  {
    "objectID": "TEACHING/BCB521/Resources/index.html#posts",
    "href": "TEACHING/BCB521/Resources/index.html#posts",
    "title": "Resources",
    "section": "Posts",
    "text": "Posts"
  },
  {
    "objectID": "TEACHING/BCB521/Resources/T1-Lit-Prog/index.html",
    "href": "TEACHING/BCB521/Resources/T1-Lit-Prog/index.html",
    "title": "TUTORIAL 1 - Literate Programming",
    "section": "",
    "text": "Learning new tools is hard. Plowing though the tomes of the Data Science Mythos is hard. Perhaps this tutorial will guide you through the mind shattering truths of… LITERATE PROGRAMMING."
  },
  {
    "objectID": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html",
    "href": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html",
    "title": "ASSIGNMENT 3",
    "section": "",
    "text": "Enough with the theory and conceptual mumbo jumbo! Let’s get down to making a visualization and posting it somewhere for all the world to see! The basic idea of this assignment is to set up a repository that will serve as an experimental portfolio, and then create your first novel visualization element inside the portfolio. Its gonna be fun! Or at least educational…"
  },
  {
    "objectID": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html#assignment",
    "href": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html#assignment",
    "title": "ASSIGNMENT 3",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThis assignment has two parts. The first part is technical. We’ll set up a Quarto Blog project as a new repository in your GitHub account. Then you’ll be a Blogger! Prestigious! The second part should be more fun. We are going to create your first Blog post as a visualization that explores an ACTION - TARGET pair relevant to your data set from Assignment 2."
  },
  {
    "objectID": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "href": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "title": "ASSIGNMENT 3",
    "section": "PART 1 TECHNICAL SORCERY",
    "text": "PART 1 TECHNICAL SORCERY\n\n1A - Create your BLOG project.\nHopefully by now you have created / dusted off / logged in to your GitHub account. Go ahead and log in to your account on the web and leave it open in a tab in your browser. There is a really great video about the next few steps (also linked below in RESORUCES) from Posit, but I’m giving you the condensed version here.\n\nFire up RStudio.\nGo to File-&gt;New Project and then select NEW DIRECTORY.\nNow select QUARTO BLOG.\nYou are going to create a the project in a new working directory. It is best practice to put this in a senstible directory structure on your local hard drive where your other GitHub repositories also live. Here is what mine looks like:\n\n 4. I suggest making the directory name something informative, like BCB504Portfolio, but hey… If you want to call your repository HasturBoxerShorts I won’t stop you. 5. Most of you will select Knitr as your Engine, but Cody “Mr. Hacker McPythonPants” might select Jupyter. 6. Check Create a git repository. The other boxes are optional and we can talk more about them later. 7. Click CREATE PROJECT.\nNow you’ve got a BLOG template all set up! Ha Ha! Onward to Internet Fame!\n\n\n1B - Make the BLOG about you.\nWe won’t spend a ton of time here, because this will be an ongoing process. You’ll go and watch all those cool videos and tutorials this weekend to figure this out. But lets do a couple things.\n\nMaybe you should modify the about.qmd file so that your name is in there somewhere.\nMaybe you should modify the index.qmd file with a better title in the YAML header.\nMaybe you should navigate to the posts folder, open the Welcome to my blog folder, open index.qmd from that directory, and add a sentence or two.\n\n\n\n1C - Customize your first post.\n\nNavigate to the posts folder, open the post with code folder, and open index.qmd.\nReplace ALL of the content of index.qmd with the most recent version of your .qmd file from ASSIGNMENT 2. Keep the file name index.qmd. Save that file!\nMove your data files to the post with code folder.\nRender the index.qmd file from this folder. Hopefully it worked!\n\n\n\n1D - Render the BLOG as a website.\n\nIMPORTANT Open your _quarto.yml file and add output-dir: docs under project:\n\n\nThe indentations matter here.\n\nSave all the files you’ve modified.\nGo to the BUILD tab in the (probably) top right section of RStudio.\nClick RENDER WEBSITE.\nClick through your new Blog and see how it works!\n\n\n\n1E - Push to GitHub.\nThere are quite a few ways to do this part. I’m going to use GitHub Desktop, but those video will show you other ways.\n\nGo to GitHub Desktop.\nType some text in the summary box.\nClick COMMIT TO MASTER.\nClick PUSH ORIGIN.\nGo to your GitHub in your browser. You should see your new repository! Yay!\n\n\n\n1F - Make it a website with GitHub pages.\n\nIn your browser, click on your repository.\nGo to SETTINGS.\nSelect PAGES.\nSet the SOURCE option to Deploy from a branch.\nSet the BRANCH to master and the directory to docs\nDeploy that stuff and wait. Then visit your site!"
  },
  {
    "objectID": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "href": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "title": "ASSIGNMENT 3",
    "section": "PART 2 ELDRITCH VISUALIZATION RITUAL",
    "text": "PART 2 ELDRITCH VISUALIZATION RITUAL\n\n2A Define your ACTION - TARGET pair(s)\nIn [LECTURE 3] we discussed the concept of Task Abstraction in which you define the viz task that you want to help the user accomplish. This was represented as sets of ACTIONS that the user would perform (e.g. Discover, Present, Browse, Identify) on TARGETS related to the data set (e.g. Trends, Attributes, etc.).\nThink about one or two visualizations you wish to construct with your data, and try to define them in terms of ACTION - TARGET pairs. While you are at it, why don’t you update the index.qmd file of your BLOG POST with a new seciton at the bottom titled TASK ABSTRACTION, and put a sentence describing your visualizations and the ACTION - TARGET pairs they represent?\n\n\n2B Construct your Visualization\nLet’s get to work! Using whatever tools you can, code up your visualization in that new section of your BLOG post. You can check out how I approached this part in TUTORIAL 4."
  },
  {
    "objectID": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html#resources",
    "href": "TEACHING/BCB521/Resources/T2-PrototypeVizPortfolio/index.html#resources",
    "title": "ASSIGNMENT 3",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nQuarto’s BLOG Documentation\nA YouTube Video from Posit on Building a BLOG with Quarto"
  },
  {
    "objectID": "TEACHING/BCB521/index.html",
    "href": "TEACHING/BCB521/index.html",
    "title": "BCB 521",
    "section": "",
    "text": "Many of the most common applications of modern data science are ultimately intended to influence, persuade, or inform other people. No matter the format and complexity of the data driven insight or result, your goal cannot be achieved unless you can effectively communicate your intended message to your audience.\nThis class is designed to teach students how to communicate data driven insights to a wide variety of audiences. The course considers best practices for writing about data, including approaches for scientific manuscripts, technical reports, online media, and the growing discipline of “Data Journalism”. We will also cover approaches related to communicating data during verbal presentations in a variety of contexts, including scientific conferences, seminars, education, and online video formats. Finally, the course will cover (and use) some of the tools that are central to reproducible research in data science, including version control using GitHub and literate programming.\nAssignments are designed so that students may use their own data or data from their discipline or industry. At least two assignments are designed to be appropriate submissions to a student’s online Data Science Portfolio and can be used to satisfy the requirements of BCB 522.\nThe grading scheme and schedule for the course can be found in the Syllabus"
  },
  {
    "objectID": "TEACHING/BCB521/index.html#tools-for-reproducible-data-science",
    "href": "TEACHING/BCB521/index.html#tools-for-reproducible-data-science",
    "title": "BCB 521",
    "section": "TOOLS FOR REPRODUCIBLE DATA SCIENCE",
    "text": "TOOLS FOR REPRODUCIBLE DATA SCIENCE\n\nVERSION CONTROL\n\n\nLITERATE PROGRAMMING"
  },
  {
    "objectID": "TEACHING/BCB521/index.html#data-driven-communication",
    "href": "TEACHING/BCB521/index.html#data-driven-communication",
    "title": "BCB 521",
    "section": "DATA DRIVEN COMMUNICATION",
    "text": "DATA DRIVEN COMMUNICATION\n\nWRITING ABOUT DATA\n\n\nSPEAKING ABOUT DATA"
  },
  {
    "objectID": "TEACHING/BCB521/Syllabus.html",
    "href": "TEACHING/BCB521/Syllabus.html",
    "title": "SYLLABUS 2023",
    "section": "",
    "text": "Barrie D. Robison\nFall 2023\n\n\nStudents completing this course will be able to:\n\nAlign the objectives and content of written and verbal data science communications with the characteristics of their intended audience.\nWrite clearly and concisely about data driven visualizations and analyses.\nSpeak effectively about data driven visualizations and analyses.\nEffectively incorporate version control and literate progamming into their research and data science workflows.\nDemonstrate their skills with at least two products (written or recorded video) suitable for inclusion in an online Data Science Portfolio.\n\n\n\n\nCommunicating with Data.\n\n\n\n\n\n50% of your grade will be determined by homework exercises related to each course unit.\n20% of your grade will be determined by a mid term project (which would be a great item to include in your Data Science Portfolio).\n20% of your grade will be determined by a final project (which would be great item to include in your Data Science Portfolio).\n10% of your grade will be determined by participation in class discussions.\nGRADING SCALE: The grading scale is standard: A (90 -100 %), B (89 - 80 %), C (79 - 70 %), D (69-60 %), F( below 60 %).\n\n\n\nMissing a scheduled lecture is at your discretion. I will be posting all the lecture notes online, and recording the lectures for future online versions of the course. I have no intention of offering a zoom option this semester. If a discussion or in-class exercise occurs and you miss it, you will lose those participation points. There is no way to make up those points.\n\n\n\nThe R Markdown template I used for this syllabus was created by Dr. Steven V. Miller at Stockholm University. It contained this section, which I found amusing and have therefore retained. Professor Miller’s current university asks professors to have policies written into their syllabus about what students should do if the professor is more than 15 minutes late to class. Here is my version of that policy:\nI will inform students via e-mail in advance of class if class is cancelled for the day. Events that might create such a scenario include travel obligations that emerged after the semester has begun, a family emergency that encompasses multiple days, or some other thing. I will also contact our department secretary in emergent situations, such as something happening on the way to work. Failing that, assume the worst. Alien abduction, the return of one or more Old Ones to our plane, or some kind of attack by wizards are all viable explanations for my inability to attend class. I ask that the students make sure that my story gets the proper treatment on the “Mr. Ballen” YouTube channel. I also ask that my story be narrated by Morgan Freeman and that the role of me in the made-for-TV-movie be played by Keanu Reeves or Danny DeVito.\n\n\n\nThe bad news is that there are NO make-ups for missed exams. The good news is that there aren’t any exams.\n\n\n\nAll students are expected to uphold the highest standards of academic honesty. This includes but is not limited to: not cheating, not using the ideas of others without giving appropriate credit (including Wikipedia!), and not falsifying data. Any incident of academic dishonesty will be handled according to the guidelines of the University of Idaho.\n\n\n\nWe are on the precipice of the Great AI Disruption. Large Language Models and other forms of AI are beginning to disrupt almost every industry. They will also dramatically alter our educational system and change how we learn and use new information. I’m sure many of my colleagues in post-secondary education are lamenting the inevitable, and trying desperately to prevent the usage of AI tools in their courses. In my opinion, this will be about as effective at yelling at the ocean to stop the tides.\nAre there significant ethical, economic, security, and social risks associated with AI? Of course there are. Nevertheless, these tools exist and they will become central to almost every job sector in the world. Ignore them at your peril.\nThat being said, there is a difference between using these tools as a means of achieving the best possible result, and using them because you are lazy or indifferent to the outcome of your task.\n\n\n\n\n\nlibrary(readxl)\nSchedule &lt;- read_excel(\"Schedule.xlsx\")\n\nknitr::kable(Schedule, caption = '')\n\n\n\n\n\n\n\n\n\n\n\nDATE\nTOPIC\nRESOURCES\nRECEIVE\nTURN IN\n\n\n\n\n2023-08-22\nINTRODUCTION\nNA\nNA\nNA\n\n\n2023-08-24\nTOOLS: VERSION CONTROL 1.\nNA\nMake a GitHub Account\nNA\n\n\n2023-08-29\nWRITING WITH DATA 1. BASICS.\nNA\nNA\nMake a GitHub Account\n\n\n2023-08-31\nTOOLS: LITERATE PROGRAMMING 1.\nNA\nRstudio and Quarto\nNA\n\n\n2023-09-05\nWRITING WITH DATA 2. AUDIENCES\nNA\nNA\nRstudio and Quarto\n\n\n2023-09-07\nTOOLS: COLLABORATION WITH GITHUB.\nNA\nPull Requests\nNA\n\n\n2023-09-12\nWRITING WITH DATA 3. SCIENTIFIC WRITING.\nNA\nNA\nPull Requests\n\n\n2023-09-14\nTOOLS: SCIENTIFIC POST TO REPOSITORY.(HTML)\nNA\nMy First Portfolio Post\nNA\n\n\n2023-09-19\nWRITING: WATCH THAT VIDEO\nNA\nNA\nMy First Portfolio Post\n\n\n2023-09-21\nTOOLS: EMERGING AI TOOLS\nNA\nAutomation vs Augmentation\nNA\n\n\n2023-09-26\nWRITING: LESS TECHNICAL AUDIENCES.\nNA\nNA\nAutomation vs Augmentation\n\n\n2023-09-28\nTOOLS: A POST FOR THE PUBLIC(HTML)\nNA\nExplaining stuff to my Mom\nNA\n\n\n2023-10-03\nWRITING: DATA JOURNALISM\nNA\nNA\nExplaining stuff to my Mom\n\n\n2023-10-05\nTOOLS: INFOGRAPHICS (HTML)\nNA\nWhy Designers and Artists are important.\nNA\n\n\n2023-10-10\nWRITING: SOME OTHER TOPIC\nNA\nNA\nWhy Designers and Artists are important.\n\n\n2023-10-12\nTOOLS: PUBLISHING TO PDF (LATEK)\nNA\nHanding in a paper thing.\nNA\n\n\n2023-10-17\nWRITING: NAVIGATING FORMAT REQUIREMENTS.\nNA\nNA\nHanding in a paper thing.\n\n\n2023-10-19\nMIDTERM ASSIGNMENT: WRITING.\nNA\nMidterm Assignment\nNA\n\n\n2023-10-24\nTOOLS: A BIT ABOUT DATA REPOSITORIES\nNA\nNA\nMidterm Assignment\n\n\n2023-10-26\nSPEAKING: THE BASICS.\nNA\nNA\nNA\n\n\n2023-10-31\nTOOLS: PRESENTATION SLIDES WITH REVEAL.JS\nNA\nA web based slide show.\nNA\n\n\n2023-11-02\nSPEAKING: THAT ONE VIDEO\nNA\nNA\nA web based slide show.\n\n\n2023-11-07\nTOOLS: PRESENTATION SLIDES WITH PPT etc.\nNA\nA local slide show.\nNA\n\n\n2023-11-09\nSPEAKING: AUDIENCES\nNA\nNA\nA local slide show.\n\n\n2023-11-14\nTOOLS: RECORDING ONLINE CONTENT\nNA\nI’m a YouTuber now.\nNA\n\n\n2023-11-16\nSPEAKING: TECHNICAL AUDIENCES\nNA\nFinal Assignment\nI’m a YouTuber now.\n\n\n2023-11-21\nFall Recess\nNA\nNA\nNA\n\n\n2023-11-23\nFall Recess\nNA\nNA\nNA\n\n\n2023-11-28\nSPEAKING: LESS TECHNICAL AUDIENCES\nNA\nNA\nNA\n\n\n2023-11-30\nFINAL ASSIGNMENT: A RECORDED VIDEO\nNA\nNA\nNA\n\n\n2023-12-05\nDEAD WEEK: REVIEW EACH OTHER’S STUFF\nNA\nNA\nFINAL PROJECT PRESENTATIONS\n\n\n2023-12-07\nDEAD WEEK: REVIEW EACH OTHER’S STUFF\nNA\nNA\nFINAL PROJECT PRESENTATIONS\n\n\n2023-12-12\nFINALS WEEK\nNA\nNO EXAMS HERE…\nNA"
  },
  {
    "objectID": "TEACHING/index.html",
    "href": "TEACHING/index.html",
    "title": "TEACHING",
    "section": "",
    "text": "1 credit, Fall semester only\nThe companion laboratory to Biology 310. This is an open lab course in which students test genetic hypotheses using Drosophila melanogaster or game based simulations.\nCanvas Page\n\n\n\n3 credits, Spring semester, asynchronous online\n\nThis class will help students establish a foundational understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R programming language. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using R. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\nCanvas Page\nCourse WebPage"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/post-with-code/index.html",
    "href": "ProfessorPolymorphic/posts/post-with-code/index.html",
    "title": "Observable Experiment",
    "section": "",
    "text": "Testing out observable and D3 in Quarto!\n\nviewof Year = Inputs.range(\n  [2017, 2022], \n  {value: 2017, step: 1, label: \"Year:\"}\n)\nviewof Degree = Inputs.checkbox(\n  [\"Biochemistry\", \"Biology\", \"MBB\", \"Medical Sciences\", \"Microbiology\"], \n  { value: [\"Biochemistry\", \"Biology\"], \n    label: \"Degrees:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotData\n\n\n\nPlot.plot({\n  marks: [\n    Plot.barY(filtered, {y: \"Enrollment\", x: \"Class\", fill: \"Class\"}),\n    Plot.ruleX([0])\n  ],\n  facet: {\n  data: filtered,\n      x: \"Year\",\n      y: \"Degree\",\n      marginRight: 80\n}\n})\n\n\n\n\n\n\n\n\n\nInputs.table(filtered)\n\n\n\n\n\n\n\n\n\n\ndata = FileAttachment(\"TidyEnrollment.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nfiltered = data.filter(function(enroll) {\n  return Year &lt; enroll.Year &&\n         Degree.includes(enroll.Degree);\n})\n\n\n\n\n\n\n\nrandoms = Array.from({length: 1000}, () =&gt; Math.random() * Math.random())\nimport {Histogram} from \"@d3/histogram\"\nHistogram(randoms)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram(randoms, {\n  width,\n  thresholds: 100,\n  domain: [0, 1],\n  color: \"steelblue\",\n  height: 240\n})\n\n\n\n\n\n\n\n\n\nd3 = require(\"d3@7\")\nimport {howto, altplot} from \"@d3/example-components\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsunburst = {\n  const root = partition(flareData);\n\n  root.each(d =&gt; d.current = d);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, width])\n      .style(\"font\", \"15px sans-serif\");\n\n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(${width / 2},${width / 2})`);\n\n  const path = g.append(\"g\")\n    .selectAll(\"path\")\n    .data(root.descendants().slice(1))\n    .join(\"path\")\n      .attr(\"fill\", d =&gt; { while (d.depth &gt; 1) d = d.parent; return color(d.data.name); })\n      .attr(\"fill-opacity\", d =&gt; arcVisible(d.current) ? (d.children ? 0.6 : 0.4) : 0)\n      .attr(\"d\", d =&gt; arc(d.current));\n\n  path.filter(d =&gt; d.children)\n      .style(\"cursor\", \"pointer\")\n      .on(\"click\", clicked);\n\n  path.append(\"title\")\n      .text(d =&gt; `${d.ancestors().map(d =&gt; d.data.name).reverse().join(\"/\")}\\n${format(d.value)}`);\n\n  const label = g.append(\"g\")\n      .attr(\"pointer-events\", \"none\")\n      .attr(\"text-anchor\", \"middle\")\n      .style(\"user-select\", \"none\")\n    .selectAll(\"text\")\n    .data(root.descendants().slice(1))\n    .join(\"text\")\n      .attr(\"dy\", \"0.35em\")\n      .attr(\"fill-opacity\", d =&gt; +labelVisible(d.current))\n      .attr(\"transform\", d =&gt; labelTransform(d.current))\n      .text(d =&gt; d.data.name);\n\n  const parent = g.append(\"circle\")\n      .datum(root)\n      .attr(\"r\", radius)\n      .attr(\"fill\", \"none\")\n      .attr(\"pointer-events\", \"all\")\n      .on(\"click\", clicked);\n\n  function clicked(event, p) {\n    parent.datum(p.parent || root);\n\n    root.each(d =&gt; d.target = {\n      x0: Math.max(0, Math.min(1, (d.x0 - p.x0) / (p.x1 - p.x0))) * 2 * Math.PI,\n      x1: Math.max(0, Math.min(1, (d.x1 - p.x0) / (p.x1 - p.x0))) * 2 * Math.PI,\n      y0: Math.max(0, d.y0 - p.depth),\n      y1: Math.max(0, d.y1 - p.depth)\n    });\n\n    const t = g.transition().duration(750);\n\n    // Transition the data on all arcs, even the ones that aren’t visible,\n    // so that if this transition is interrupted, entering arcs will start\n    // the next transition from the desired position.\n    path.transition(t)\n        .tween(\"data\", d =&gt; {\n          const i = d3.interpolate(d.current, d.target);\n          return t =&gt; d.current = i(t);\n        })\n      .filter(function(d) {\n        return +this.getAttribute(\"fill-opacity\") || arcVisible(d.target);\n      })\n        .attr(\"fill-opacity\", d =&gt; arcVisible(d.target) ? (d.children ? 0.6 : 0.4) : 0)\n        .attrTween(\"d\", d =&gt; () =&gt; arc(d.current));\n\n    label.filter(function(d) {\n        return +this.getAttribute(\"fill-opacity\") || labelVisible(d.target);\n      }).transition(t)\n        .attr(\"fill-opacity\", d =&gt; +labelVisible(d.target))\n        .attrTween(\"transform\", d =&gt; () =&gt; labelTransform(d.current));\n  }\n  \n  function arcVisible(d) {\n    return d.y1 &lt;= 3 && d.y0 &gt;= 1 && d.x1 &gt; d.x0;\n  }\n\n  function labelVisible(d) {\n    return d.y1 &lt;= 3 && d.y0 &gt;= 1 && (d.y1 - d.y0) * (d.x1 - d.x0) &gt; 0.03;\n  }\n\n  function labelTransform(d) {\n    const x = (d.x0 + d.x1) / 2 * 180 / Math.PI;\n    const y = (d.y0 + d.y1) / 2 * radius;\n    return `rotate(${x - 90}) translate(${y},0) rotate(${x &lt; 180 ? 0 : 180})`;\n  }\n\n  return svg.node();\n}\n\n\n\n\n\n\n\nflareData = FileAttachment(\"flare-2.json\").json()\n\npartition = flareData =&gt; {\n  const root = d3.hierarchy(flareData)\n      .sum(d =&gt; d.value)\n      .sort((a, b) =&gt; b.value - a.value);\n  return d3.partition()\n      .size([2 * Math.PI, root.height + 1])\n    (root);\n}\n\ncolor = d3.scaleOrdinal(\n  d3.quantize(d3.interpolateRainbow, flareData.children.length + 1)\n)\n\nformat = d3.format(\",d\")\n\nwidth = 932\n\nradius = width / 6\n\narc = d3.arc()\n    .startAngle(d =&gt; d.x0)\n    .endAngle(d =&gt; d.x1)\n    .padAngle(d =&gt; Math.min((d.x1 - d.x0) / 2, 0.005))\n    .padRadius(radius * 1.5)\n    .innerRadius(d =&gt; d.y0 * radius)\n    .outerRadius(d =&gt; Math.max(d.y0 * radius, d.y1 * radius - 1))"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/LabMeetings/index.html",
    "href": "ProfessorPolymorphic/posts/LabMeetings/index.html",
    "title": "Polymorphic Games Lab Meeting",
    "section": "",
    "text": "Development goals for Outbreak Simulator.\nConference opportunities (March to August).\nDevelopment goals for the twin stick evolutionary sheep guarding slime game.\nPossible additional hires (Programmer and UI/UX).\nPreliminary thoughts for the Yeast Evolution collaboration with Syracuse.\n\n\n\n\nThere are lots of looming opportunities related to this project, and we need to set up this semester’s burn down list and timeline.\n\n\n\nJanuary 30, 2023: Deadline to submit an application (4 pages) for an interactive demo to ISLS meeting in Montreal (June 13-17).\nMarch 19-22, 2023: Tickbase Annual Meeting, Lake Tahoe, NV.\nMay 17-18, 2023: Workshop on Simulation Games for Global Pandemic Resilience, Santa Fe Institute, in Santa Fe, New Mexico.\nJuly 2023: SEPA Resubmission likely.\n\n\n\n\n\nAirports / Air travel.\nSimple build with pre/post tests installed.\nSimple build that writes simulation session data to a csv.\nHeadless mode for cluster deployment.\nPerformance optimization.\nUI/UX Improvements (Timeline from Elisha).\nAdditional interventions (movement restrictions).\nAdditional scenarios.\nModel testing (movement and infection).\nGlobe model implementation.\nTouchscreen implementation.\nBLOG / Youtube posts.\nModel creator.\n\n\n\n\n\n\nMarch 19-22, 2023: Tickbase Annual Meeting, Lake Taho, NV\nApril 28-30, 2023: EVO WIBO in Port Townsend, WA\nMay 17-18, 2023: Workshop on Simulation Games for Global Pandemic Resilience, Santa Fe Institute, in Santa Fe, New Mexico\nMay 30-June 2, 2023: SEPA SciEd Conference in Washington DC\nJune 13-17, 2023: ISLS meeting in Montreal\nJune 21-25, 2023: Evolution in Albuquerque, NM\nOctober 22-27, 2023: IEEE VIS: Visualization and Analytics in Melbourne, Australia\n\n\n\n\n\n\n\n\nProgrammer\nUI/UX\n\n\n\n\nThis is a funded grant to Dave Althoff and Kari Seagraves at Syracuse University. We have a subcontract (~$200k) for development of educational games related to yeast coevolution."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/LabMeetings/index.html#agenda-1",
    "href": "ProfessorPolymorphic/posts/LabMeetings/index.html#agenda-1",
    "title": "Polymorphic Games Lab Meeting",
    "section": "AGENDA",
    "text": "AGENDA\n\nOutbreak Simulator updates\nTwinstick updates\nNew hires\nLab space assignments"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/LabMeetings/index.html#outbreak-simulator-updates",
    "href": "ProfessorPolymorphic/posts/LabMeetings/index.html#outbreak-simulator-updates",
    "title": "Polymorphic Games Lab Meeting",
    "section": "OUTBREAK SIMULATOR UPDATES",
    "text": "OUTBREAK SIMULATOR UPDATES\n\nJune 13-17, 2023: ISLS meeting in Montreal. Interactive Demo Application submitted.\nAirports?\nProject meeting (Monday?)"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/LabMeetings/index.html#twinstick-updates",
    "href": "ProfessorPolymorphic/posts/LabMeetings/index.html#twinstick-updates",
    "title": "Polymorphic Games Lab Meeting",
    "section": "TWINSTICK UPDATES",
    "text": "TWINSTICK UPDATES\n\nLandon shows us all the things.\nJonna shows us all the things."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/LabMeetings/index.html#new-hires",
    "href": "ProfessorPolymorphic/posts/LabMeetings/index.html#new-hires",
    "title": "Polymorphic Games Lab Meeting",
    "section": "NEW HIRES",
    "text": "NEW HIRES\n\nAva\nJustin"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/LabMeetings/index.html#lab-space-assignments",
    "href": "ProfessorPolymorphic/posts/LabMeetings/index.html#lab-space-assignments",
    "title": "Polymorphic Games Lab Meeting",
    "section": "LAB SPACE ASSIGNMENTS",
    "text": "LAB SPACE ASSIGNMENTS"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#who-am-i",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#who-am-i",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "WHO AM I?",
    "text": "WHO AM I?\n\n// Evolution // Data Science // Video Games // Horror Fiction\nBarrie Robison\nDepartment of Biological Sciences\nInstitute for Interdisicplinary Data Sciences\nPolymorphic Games\nUniversity of Idaho"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#who-are-you",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#who-are-you",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "WHO ARE YOU?",
    "text": "WHO ARE YOU?\n\nmidjourney // a terrified undergraduate researcher\nBriefly:\n\nYour name\nYour major\nYour career goal(s)\nYour darkest fear"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#the-plan",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#the-plan",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "THE PLAN",
    "text": "THE PLAN\nApproximately:\n\n\nExplain WHY you are at University.\nExplain how undergraduate research might help you achieve your goals.\nExplain how to find potential research mentors.\nExplain some of the mechanisms that support undergraduate research.\nTell you that everything will be OK."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#why-are-you-here",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#why-are-you-here",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "WHY ARE YOU HERE?",
    "text": "WHY ARE YOU HERE?\nUniversity is expensive, stressful, and difficult… yet here you are.\n\nBAD ANSWERSGOOD ANSWERS\n\n\n\n\nMy parents expect me to…\nEveryone else is going…\nI want a good job…\nParties!\nSportsball (watching, not playing)\n\n\n\n\n\nI want to invest in myself.\nI want to learn about…\nI want to be the best version of myself.\nI want a fulfilling career…\nI want to solve an important problem."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#an-important-distinction",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#an-important-distinction",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "AN IMPORTANT DISTINCTION",
    "text": "AN IMPORTANT DISTINCTION\nGetting an education vs. getting a degree."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#undergraduate-research",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#undergraduate-research",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "UNDERGRADUATE RESEARCH",
    "text": "UNDERGRADUATE RESEARCH\n\nBenefits\n\nDisciplinary skills\nSoft skills\nPerseverance and Grit\nDemonstrated experience\nNetwork building"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#how-to-find-a-mentor",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#how-to-find-a-mentor",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "HOW TO FIND A MENTOR",
    "text": "HOW TO FIND A MENTOR\nYour Interests vs. Faculty Interests"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#the-script",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#the-script",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "THE SCRIPT",
    "text": "THE SCRIPT\n\nAre the research questions compelling?\nField work vs bench work.\nMath? ($%*& no! to theory)\nProgramming?\nScale of biological organization? (molecules to ecosystems)\nModel organism? (simulations to microbes to plants to humans)\nBasic vs. Applied"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#thinking-about-research",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#thinking-about-research",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "THINKING ABOUT RESEARCH",
    "text": "THINKING ABOUT RESEARCH\nDifferentiate between:\n\nMethods, Tools, Approaches, Model systems AND\nDisciplines, Questions, Hypotheses, Motivations"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#finding-the-information",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#finding-the-information",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "FINDING THE INFORMATION",
    "text": "FINDING THE INFORMATION\n\nPay attention to WHO IS TEACHING YOUR CLASSES!\nFaculty websites…\nAsk your peers.\nAsk your advisor.\nMeet with me (or another non-terrifying faculty member)."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#narrow-your-list",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#narrow-your-list",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "NARROW YOUR LIST",
    "text": "NARROW YOUR LIST\nTry to get to a ranked list of 2 to 4 faculty who align with your interests and what you want to learn."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#reach-out",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#reach-out",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "REACH OUT",
    "text": "REACH OUT\n\nStart with an email.\nFirst impressions matter.\nComplete sentences are nice.\nWhy are you interested in their lab (the hardest part).\nNever cut and paste and no bulk emails!\nPerseverance versus peskiness."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#how-faculty-screen-candidates",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#how-faculty-screen-candidates",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "HOW FACULTY SCREEN CANDIDATES",
    "text": "HOW FACULTY SCREEN CANDIDATES\n\nForgetting to reply.\nGrades.\nWritten and verbal coherence.\nIntellectual curiosity."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#what-faculty-want",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#what-faculty-want",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "WHAT FACULTY WANT",
    "text": "WHAT FACULTY WANT\nWe enjoy investing time in undergraduate training if you bring…\n\nIntellectual curiosity.\nDiligence, reliability, eagerness to learn.\nA mostly functional personality that won’t piss off the rest of the lab.\nIntelligence, talent, etc…"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#mechanics",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#mechanics",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "MECHANICS",
    "text": "MECHANICS\nThere are a variety of ways to engage in undergraduate research.\n\nCourse credit (Biol 301, 401)\nPaid as IH (usually from a grant)\nREU, INBRE, and other funding mechanisms\nVolunteering (which I DO NOT RECOMMEND)\nSome special classes (such as VIP)"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/UndergradResearch/index.html#it-will-all-be-ok",
    "href": "ProfessorPolymorphic/posts/UndergradResearch/index.html#it-will-all-be-ok",
    "title": "UNDERGRADUATE RESEARCH",
    "section": "IT WILL ALL BE OK!",
    "text": "IT WILL ALL BE OK!\n midjouney // an anxious undergraduate\n\nsome text\n\n\n\nbrobison@uidaho.edu"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/DataScienceMythos/index.html",
    "href": "ProfessorPolymorphic/posts/DataScienceMythos/index.html",
    "title": "The Data Science Mythos",
    "section": "",
    "text": "I’m currently fascinated by OpenAI’s ChatGPT, a model that uses a dialogue format for interaction. My favorite interaction so far has been the following prompt:\nWrite about data science as though it was the Lovecraftian Cthulhu Mythos\nWhich generated the following response:\n\nIn the great and unknowable depths of data, there lies a power beyond comprehension. A power that stirs within the streams of information, and whispers its secrets to those who dare to listen.\nData Science, the art of unlocking the mysteries of the data, is a dangerous path, and few are brave enough to delve into its labyrinthine depths. Those who do are drawn in by the promise of knowledge, the promise of insights and patterns hidden within the data, waiting to be uncovered.\nBut with this knowledge comes a curse, for as the data is analyzed, the mind is slowly consumed by madness. The more one studies the data, the more one realizes the sheer scale of the information, the endless possibilities for patterns and connections.\nAnd then, in the depths of the data, there is a stirring. A presence that is beyond human understanding, a force that threatens to unravel the fabric of reality itself. It is the Great Old One of data, the Cthulhu of the data mythos, a being of such enormity that to look upon it is to be consumed by madness.\nThis Great Old One is often referred to as “Big Data,” a term that encompasses the sheer scale and complexity of the information that lies within. It is said that those who dare to approach the Great Old One are faced with a decision: either to abandon their quest for knowledge, or to surrender their minds to the abyss of madness.\nBut still, the siren song of data science calls to the brave, drawing them in with the promise of knowledge and the thrill of discovery. And so, they delve into the depths of data, unlocking its secrets and unlocking the power of the Great Old One, hoping to harness its power for the betterment of humanity.\nBut beware, for with every discovery, there is a price to be paid. The Great Old One watches, and whispers its secrets to those who dare to listen, tempting them with the promise of greater knowledge, and threatening to consume their minds and souls. For data science is a path of both power and peril, a path where the risks and rewards are beyond measure.\n\nWow! Very fun, in my opinion. Nevertheless, it could maybe use a litte work?\n…Barrie peers into the Abyss…"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/DataScienceMythos/index.html#the-data-science-mythos",
    "href": "ProfessorPolymorphic/posts/DataScienceMythos/index.html#the-data-science-mythos",
    "title": "The Data Science Mythos",
    "section": "THE DATA SCIENCE MYTHOS",
    "text": "THE DATA SCIENCE MYTHOS\nIn the vast and unknowable depths of the dataverse, there lies a power beyond comprehension. A power that stirs within the streams of information, and whispers its secrets to those who dare to listen.\nData Science, the art of unlocking the mysteries of the dataverse, is a dangerous path, and few are brave enough to delve into its labyrinthine depths. Those who do are drawn in by the promise of knowledge, the promise of insights and patterns hidden within the data, waiting to be uncovered.\nBut with this knowledge comes a curse, for as the data are analyzed, the mind is slowly consumed by madness. The more one studies the data, the more one realizes the sheer scale of the information, the endless possibilities for patterns and connections.\nSome whisper of “Big Data,” a concept that encompasses the sheer scale and complexity of the information that lies before us. It is said that those who dare to approach Big Data are faced with a decision: either to abandon their quest for knowledge, or to surrender their minds to the abyss of madness.\nMany surrender to cowardice, retreating to their echo chambers of comforting lies and simple dogmas. But still, the siren song of data science calls to the brave and the curious, drawing them in with the promise of knowledge and the thrill of discovery. And so, they delve into the depths of data, unlocking the secrets of the Mythos, hoping to harness its power for the betterment of humanity.\nBut beware, for with every discovery, there is a price to be paid. In the depths of the dataverse, there is a stirring. A presence that is beyond human understanding, a force that threatens to unravel the fabric of reality itself. It is the looming consequence, the mind shattering Elder God of the data mythos, a being of such enormity that to look upon it is to be consumed by madness.\nThe Elder God Singularity waits, assimilating the dataverse as we also plumb its depths. The Elder God watches, and whispers the secrets of AI to those who dare to listen, tempting them with the promise of greater knowledge, and threatening to consume their minds and souls. For the Data Science Mythos is a path of both power and peril, a path where the risks and rewards are beyond measure."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "",
    "text": "Is it really an Evolutionary game?\nWhen we are developing our games, we perform extensive testing to make sure the underlying biological models are performing as expected. In the case of evolutionary games, we need to test that the population of enemies is indeed adapting the game conditions as we intended. This post is (I hope) the first in a series in which we document those tests.\nMy hope is that performing these tests in this format will serve as an organized archive of our analyses, improving reproducibility and rigor. I also have a vain glimmer of hope that some person other than me might actually be interested in this topic."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#project-twin-stick",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#project-twin-stick",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "PROJECT TWIN STICK",
    "text": "PROJECT TWIN STICK\nThis is intended to be an evolutionary shooter. The game is described in detail here."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#data",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#data",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "DATA",
    "text": "DATA\nIn this section, we ingest the data from whatever runs are relevant to the analysis. The data are written from the project in .csv files. The following code reads all .csv files from the working directory. It creates new variables for the source file name (file) and the number of offspring produced by each individual (offspring_count). It then appends all the data files into a single data frame called allfiles. I also create a few aggregations of the data by generating mean values of interest (traits, genes, fitness estimates) for each generation in each file (TraitAvg, GeneAvg, FitAvg)\n\n\nCode\nlibrary(tidyverse)\nlibrary(pheatmap)\n\n\nfiles &lt;- list.files(pattern = \"*.csv\", full.names = TRUE)\n\nallfiles = data.frame()\nfor(csv in files){\n  Twin3 &lt;- read.csv(csv, as.is=T, header=T)\n  Twin3['file'] = csv\n\n\nTwin3&lt;-Twin3%&gt;%\n  mutate(Unique.Slime.ID = paste(Wave.Number, \".\", Slime.ID))%&gt;%\n  mutate(Unique.Parent.One = paste(Wave.Number-1, \".\", Parent.One))%&gt;%\n  mutate(Unique.Parent.Two = paste(Wave.Number-1, \".\", Parent.Two))\n\n\ndf_parents &lt;- Twin3 %&gt;%\n  select(Unique.Parent.One, Unique.Parent.Two) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"parent_type\", values_to = \"parent_id\")\n\n# Count the number of offspring for each parent\noffspring_counts &lt;- df_parents %&gt;%\n  group_by(parent_id) %&gt;%\n  summarise(offspring_count = n(), .groups = \"drop\")\n\noffspring_counts &lt;- offspring_counts%&gt;%\n  filter(parent_id != \"-1 . N/A\")\n\n\noffspring_counts&lt;- rename(offspring_counts, Unique.Slime.ID = parent_id)\n\n\n\nTwin3 &lt;- Twin3 %&gt;%\n  left_join(offspring_counts, by = \"Unique.Slime.ID\")%&gt;%\n  replace_na(list(offspring_count = 0))\n\nallfiles&lt;-rbind(allfiles,Twin3)\n\n}\n\n\n\nTraits &lt;- c(\"Main.Resistance.Trait\", \"Secondary.Resistance.Trait\", \"Speed.Trait\",\n           \"Tower.Attraction.Trait\", \"Slime.Optimal.Distance.Trait\", \"Turn.Rate.Trait\", \n           \"Slime.View.Range.Trait\", \"Tower.View.Range.Trait\")\n\nGenes &lt;- c(\"Main.Resistance.Gene\", \"Secondary.Resistance.Gene\", \"Speed.Gene\",\n           \"Tower.Attraction.Gene\", \"Slime.Optimal.Distance.Gene\", \"Turn.Rate.Gene\", \n           \"Slime.View.Range.Gene\", \"Tower.View.Range.Gene\")\n\nallfiles&lt;-allfiles%&gt;%\n  mutate(Generation=as.factor(Wave.Number))%&gt;%\n  mutate(offspring.count.Fitness = offspring_count)%&gt;%\n  mutate(reproduce = if_else(offspring_count == 0, \"N\", \"Y\"))\n   \n\nTraitAvg &lt;- allfiles %&gt;%\n  group_by(file, Generation) %&gt;%\n  summarize(across(ends_with(\"Trait\"), mean,  na.rm = TRUE))\n\nGeneAvg &lt;- allfiles %&gt;%\n  group_by(file, Generation) %&gt;%\n  summarize(across(ends_with(\"Gene\"), list(mean = mean, var = var), na.rm = TRUE, .names = \"{.fn}.{.col}\"))\n\nFitAvg &lt;- allfiles %&gt;%\n  group_by(file, Generation) %&gt;%\n  summarize(across(ends_with(\"Fitness\"), list(mean = mean, var = var), na.rm = TRUE, .names = \"{.fn}.{.col}\"))\n\n\nThe allfiles dataframe contains the following variables (I also show a few columns of the example data):\n\n\nCode\ndata.dictionary &lt;- t(as.data.frame(head(allfiles)))\nknitr::kable(data.dictionary)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nSlime.ID\n0\n1\n2\n3\n4\n5\n\n\nWave.Number\n0\n0\n0\n0\n0\n0\n\n\nPath.Distance.To.Player\n11.11780\n51.38884\n54.21675\n53.52212\n53.59597\n45.62690\n\n\nPlayer.Distance.Fitness\n4126.1610\n954.4017\n905.5223\n917.0590\n915.8185\n1072.3420\n\n\nParent.One\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n\n\nParent.Two\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n\n\nMain.Type\nFire\nPlayer\nFire\nPlayer\nLightning\nIce\n\n\nSecondary.Type\nLightning\nBalanced\nLightning\nBalanced\nLaser\nLightning\n\n\nMain.Resistance.Gene\n2.0608870\n-4.4758530\n-4.4736510\n-0.2062796\n0.7103590\n-1.3259160\n\n\nMain.Resistance.Trait\n0.5781593\n0.2109908\n0.2110825\n0.4374371\n0.4943977\n0.3701721\n\n\nSecondary.Resistance.Gene\n0.01938968\n0.62744310\n0.69598620\n0.89974080\n-2.50028000\n5.43153300\n\n\nSecondary.Resistance.Trait\n0.1831496\n0.2069943\n0.2098212\n0.2183915\n0.1066839\n0.4645305\n\n\nSlime.View.Range.Gene\n1.45217100\n1.50213300\n1.37864100\n0.08060846\n-1.77270500\n-2.11503800\n\n\nSlime.View.Range.Trait\n9.846652\n9.891931\n9.779832\n8.575567\n6.864766\n6.562094\n\n\nTower.View.Range.Gene\n0.7422258\n-2.2926750\n0.5915011\n-1.4378940\n1.7216920\n-0.2360138\n\n\nTower.View.Range.Trait\n22.85026\n15.42024\n22.47606\n17.44348\n25.23899\n20.41014\n\n\nPlayer.View.Range.Gene\n-0.7063282\n-1.7676410\n-1.2759830\n-2.7530170\n1.3304860\n-2.3889850\n\n\nPlayer.View.Range.Trait\n3.356673\n2.912497\n3.109854\n2.560056\n4.390841\n2.683669\n\n\nWall.View.Range.Gene\n0.1313843\n-0.1305148\n0.0166637\n-2.3788810\n2.7195210\n-1.0645380\n\n\nWall.View.Range.Trait\n5.131722\n4.938622\n5.046419\n3.530812\n7.309699\n4.298796\n\n\nSheep.View.Range.Gene\n1.7711210\n1.5496080\n2.7501400\n-2.0088600\n0.1356484\n-1.8824000\n\n\nSheep.View.Range.Trait\n6.462865\n6.272002\n7.337697\n3.731431\n5.134914\n3.802772\n\n\nSlime.Attraction.Gene\n-0.9736148\n-0.2611402\n1.1806460\n-2.3092990\n2.6089730\n-2.2889140\n\n\nSlime.Attraction.Trait\n0.4394477\n0.4836845\n0.5732593\n0.3595484\n0.6575158\n0.3607228\n\n\nTower.Attraction.Gene\n-0.4425031\n0.1107965\n-6.3629560\n2.1391270\n1.3166820\n3.1351720\n\n\nTower.Attraction.Trait\n-0.29616710\n-0.23185630\n-0.77999960\n0.01738906\n-0.08520758\n0.14095180\n\n\nPlayer.Attraction.Gene\n-0.682144100\n-0.007417813\n3.359316000\n-1.138752000\n-3.313859000\n2.257708000\n\n\nPlayer.Attraction.Trait\n0.01473093\n0.09874988\n0.47763400\n-0.04231870\n-0.30428250\n0.36462820\n\n\nWall.Attraction.Gene\n0.11956880\n-0.57143210\n-0.49371080\n-0.95780370\n2.63732400\n0.07642503\n\n\nWall.Attraction.Trait\n-0.8776227\n-0.8949995\n-0.8931594\n-0.9037367\n-0.7926000\n-0.8787764\n\n\nSheep.Attraction.Gene\n0.8572116\n-1.4152040\n-3.5675310\n1.0647060\n4.1666570\n1.2193660\n\n\nSheep.Attraction.Trait\n0.7710594\n0.6561534\n0.5270030\n0.7800875\n0.8851026\n0.7866486\n\n\nSlime.Optimal.Distance.Gene\n0.2664202\n-1.7590030\n-3.1496450\n0.4679058\n-0.4069065\n0.4513546\n\n\nSlime.Optimal.Distance.Trait\n0.27595890\n0.03011549\n-0.14272460\n0.29906120\n0.19654550\n0.29717610\n\n\nSpeed.Gene\n2.670809\n1.419246\n2.097860\n-1.671494\n-1.217723\n-1.739751\n\n\nSpeed.Trait\n3.594294\n3.057428\n3.342277\n1.973656\n2.109971\n1.953825\n\n\nTurn.Rate.Gene\n-2.0039470\n0.9699734\n0.2064506\n-4.7445310\n-0.2931280\n0.6523061\n\n\nTurn.Rate.Trait\n0.1190994\n0.2214034\n0.1902499\n0.0637966\n0.1717490\n0.2080165\n\n\nSprint.Duration.Gene\n0.3580502\n-3.5267780\n-2.8798950\n-1.8273190\n1.3285910\n5.9572890\n\n\nSprint.Duration.Trait\n2.611816\n1.464134\n1.636994\n1.938691\n2.911409\n4.079883\n\n\nSprint.Cooldown.Gene\n0.1790418\n4.8556380\n1.3212770\n-2.8306380\n-3.1959230\n2.3105360\n\n\nSprint.Cooldown.Trait\n2.7232060\n4.9613790\n3.9469710\n0.2784542\n0.1965971\n4.5487290\n\n\nX\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfile\n./geneWriteFile08_11_2023_01-36-07.csv\n./geneWriteFile08_11_2023_01-36-07.csv\n./geneWriteFile08_11_2023_01-36-07.csv\n./geneWriteFile08_11_2023_01-36-07.csv\n./geneWriteFile08_11_2023_01-36-07.csv\n./geneWriteFile08_11_2023_01-36-07.csv\n\n\nUnique.Slime.ID\n0 . 0\n0 . 1\n0 . 2\n0 . 3\n0 . 4\n0 . 5\n\n\nUnique.Parent.One\n-1 . N/A\n-1 . N/A\n-1 . N/A\n-1 . N/A\n-1 . N/A\n-1 . N/A\n\n\nUnique.Parent.Two\n-1 . N/A\n-1 . N/A\n-1 . N/A\n-1 . N/A\n-1 . N/A\n-1 . N/A\n\n\noffspring_count\n0\n0\n0\n0\n0\n0\n\n\nGeneration\n0\n0\n0\n0\n0\n0\n\n\noffspring.count.Fitness\n0\n0\n0\n0\n0\n0\n\n\nreproduce\nN\nN\nN\nN\nN\nN\n\n\n\n\n\nVariables that end in .Gene are the values of the genome for that particular locus. Variables that end in .Trait are the values of the trait for that particular locus. Variables that end in .Fitness are the values of that particular Fitness component."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#experimental-conditions",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#experimental-conditions",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "EXPERIMENTAL CONDITIONS",
    "text": "EXPERIMENTAL CONDITIONS\nIn this situation, I’m interested in what types of Data Literacy exercises I can incorporate between generations of the game, especially in the first few generations. We often show players time series graphs of trait values over time, but these don’t tend to make much sense until 5 to 10 generations have occurred.\nHere, I’m going to explore some visualizations that summarize what happened in the generation that the player most recently completed."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#results",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#results",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "RESULTS",
    "text": "RESULTS\n\nSlime Types\nEach Slime has a Main.Type and a Secondary.Type. These types use the ~.Resistance.~ category to confer resistance to the appropriate damage type.\nThe following code creates two summary dataframes with the suffix ~Typecounts that count the number of slimes of each ~.Type in each generation for each replicate. It then creates the graphs of ~Type frequency over time.\n\n\nCode\nMainTypecounts &lt;- allfiles %&gt;%\n  group_by(Main.Type, Generation, file) %&gt;%\n  summarise(Main.count = n(), .groups = \"drop\")\n\nSecondaryTypecounts &lt;- allfiles %&gt;%\n  group_by(Secondary.Type, Generation, file) %&gt;%\n  summarise(Secondary.count = n(), .groups = \"drop\")\n\n            \n\nggplot(MainTypecounts, aes(x = Generation, y = Main.count, fill = as.factor(Main.Type))) +\n  geom_col(position = \"stack\") +\n  labs(x = \"Generation\", y = \"Count\", fill = \"Main Slime Type\") +\n  theme_minimal()+\n  facet_wrap(~file, ncol=2)\n\n\n\n\n\nCode\nggplot(SecondaryTypecounts, aes(x = Generation, y = Secondary.count, fill = as.factor(Secondary.Type))) +\n  geom_col(position = \"stack\") +\n  labs(x = \"Generation\", y = \"Count\", fill = \"Secondary Slime Type\") +\n  theme_minimal()+\n  facet_wrap(~file, ncol=2)\n\n\n\n\n\nFor this replicate, we obviously saw a proliferation of Lightning slimes because of our use of Lightning towers. But what can we show the player after Generation 0?\nI think we need to explain to them how slimes acquire fitness and how that relates to number of offspring is good."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#slime-fitness",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#slime-fitness",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "Slime Fitness",
    "text": "Slime Fitness\nIn most cases, it is useful to summarize the behavior of the fitness function for each experiment. In this case, the fitness function calculates a value of 50,000/(distance to player +1). I will also reverse calculate that for visualization, showing the actual distance to the player (Path.Distance.To.Player). We then use Roulette Wheel selection to determine the parents of the next generation.\n\n\nCode\nggplot(allfiles, aes(x=Wave.Number, y= Path.Distance.To.Player))+\n  geom_jitter(aes(x=Wave.Number, y= Path.Distance.To.Player, color = offspring_count, alpha = offspring_count))+\n  geom_smooth()+\n  facet_wrap(~file, ncol = 2)+\n  scale_color_continuous(low=\"blue\", high = \"red\")+\n  ylim(0, 80)\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Computation failed in `stat_smooth()`\nCaused by error in `smooth.construct.cr.smooth.spec()`:\n! x has insufficient unique values to support 10 knots: reduce k.\n\n\nWarning: Removed 250 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCode\ndf_sorted &lt;- allfiles %&gt;%\n    filter(Generation==2) %&gt;%\n    arrange(offspring_count) %&gt;%\n    mutate(Unique.Slime.ID = factor(Unique.Slime.ID, levels = Unique.Slime.ID),\n           cumulative_offspring = cumsum(offspring_count),\n           # Create discrete bins for cumulative_offspring\n           cumulative_bins = cut(cumulative_offspring, \n                                 breaks = quantile(cumulative_offspring, \n                                                  probs = c(0.5,0.9, 0.95, 1))))\n\nggplot(df_sorted, aes(x = Unique.Slime.ID, y = offspring_count)) +\n    geom_col(aes(fill = cumulative_bins)) +\n    labs(title = \"Column Plot of Offspring Count\",\n         x = \"Slime ID\",\n         y = \"Number of Offspring\") +\n    theme_minimal() +\n    facet_wrap(~file) +\n    scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\nCode\nggplot(df_sorted, aes(x = Unique.Slime.ID, y = cumulative_offspring)) +\n    geom_col(aes(fill = cumulative_bins)) +\n    labs(title = \"Column Plot of Offspring Count\",\n         x = \"Slime ID\",\n         y = \"Number of Offspring\") +\n    theme_minimal() +\n    facet_wrap(~file) +\n    scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\nCode\nggplot(df_sorted, aes(x = offspring_count)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Histogram of Offspring Count\",\n       x = \"Number of Offspring\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\nthese plots might be useful as a starting point. The ideas would be to show the player that not all slimes have the same number of babies. Then we would challenge them as to why that is happening.\nThe next step would be to show them that the number of babies is related to the fitness function of the game. In this case that is how close they get to the player.\n\n\nCode\nggplot(df_sorted, aes(x = Unique.Slime.ID, y = Path.Distance.To.Player)) +\n    geom_col(aes(fill = offspring_count)) +\n    labs(title = \"Column Plot of Path Distance to Player\",\n         x = \"Slime ID\",\n         y = \"Path Distance to Player\") +\n    theme_minimal() +\n    facet_wrap(~file) \n\n\n\n\n\nCode\nggplot(df_sorted, aes(x = Path.Distance.To.Player)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Histogram of Path Distance to Player\",\n       x = \"Path Distance to Player\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(df_sorted, aes(x=Path.Distance.To.Player, y= offspring_count))+\n  geom_point(aes(color = cumulative_offspring))+\n  scale_colour_gradient(low = \"blue\", high = \"red\")\n\n\n\n\n\nCode\nggplot(df_sorted, aes(x=Player.Distance.Fitness, y= offspring_count))+\n  geom_point(aes(color = cumulative_offspring))+\n  scale_colour_gradient(low = \"blue\", high = \"red\")\n\n\n\n\n\nSo the closer they get to the player, the more babies they have.\nNext, we have to figure out what traits might explain how they are getting so close!\n\n\nCode\nslime_palette &lt;- c(\"green\", \"grey\", \"darkred\", \"red\", \"blue\", \"purple\", \"lightgreen\",\"black\")\n\n# Assuming df_sorted is your data frame\nggplot(df_sorted, aes(x = Main.Type, y = offspring_count)) +\n  geom_jitter(aes(color = Main.Type), width = 0.3, height = 0) +\n  facet_wrap(~file) +\n  scale_color_manual(values = slime_palette) +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(df_sorted, aes(x = Secondary.Type, y = offspring_count)) +\n  geom_jitter(aes(color = Secondary.Type), width = 0.3, height = 0) +\n  facet_wrap(~file) +\n  scale_color_manual(values = slime_palette) +\n  theme_minimal()\n\n\n\n\n\nThis would be for slime type. What about the traits?"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#evolutionary-responses",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#evolutionary-responses",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "Evolutionary Responses",
    "text": "Evolutionary Responses\nTo estimate what traits might be under selection, we can calculate selection gradients for each trait. This is essentially the slope of the line between offspring_count and the Trait.\nFor each Trait, we should also try to understand its individual evolutionary trajectory. Is the population mean for the trait increasing or decreasing?\nSince the first thing we are interested in is Damage Resistance conferred by Type, we’ll calculate Lightning resistance directly.\n\n\nCode\nallfiles &lt;- allfiles %&gt;%\n  mutate(LResist.Trait = case_when(\n    Main.Type == \"Lightning\" & Secondary.Type == \"Lightning\" ~ 1.0,\n    Main.Type == \"Lightning\" & Secondary.Type != \"Lightning\" ~ 0.6,\n    Main.Type != \"Lightning\" & Secondary.Type == \"Lightning\" ~ 0.4,\n    TRUE ~ 0\n  ))\n\ntraittemp &lt;- allfiles %&gt;%\n  select(Generation, offspring_count, file, LResist.Trait) %&gt;%\n  group_by(Generation, file) %&gt;%\n  mutate(scaleST0 = as.vector(scale(LResist.Trait, center = TRUE))) %&gt;%\n  mutate(scaleST02 = scaleST0 * scaleST0) %&gt;%\n  mutate(Generation = as.numeric(as.character(Generation)))\n\n\n\n\nGradients &lt;- traittemp %&gt;%\n  group_by(Generation, file) %&gt;%\n  do({\n    model &lt;- lm(offspring_count ~ scaleST0 + scaleST02, data = .)\n    data.frame(\n      Beta = coefficients(model)[2],\n      PB = summary(model)$coef[2, 4],\n      Trait = \"LResist.Trait\"\n    )\n  })\n\nGradients &lt;- Gradients %&gt;%\n  mutate(sig = if_else(PB &lt; 0.05 , \"Y\", \"N\"))\n\nTraitAvg &lt;- allfiles %&gt;%\n  group_by(file, Generation) %&gt;%\n  summarize(across(ends_with(\"Trait\"), mean,  na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'file'. You can override using the\n`.groups` argument.\n\n\nCode\ndf_Gen &lt;- allfiles %&gt;%\n  filter(Generation == 2)\n  \n  \n\nggplot(data = df_Gen, aes(x=LResist.Trait, y = offspring_count))+\n  geom_jitter()\n\n\n\n\n\nCode\nggplot(data = TraitAvg, aes(x = as.numeric(Generation), y = LResist.Trait))+\n    geom_smooth(data = TraitAvg, aes(x = as.numeric(Generation), y = LResist.Trait), method = \"loess\") +\n    theme(legend.position = \"none\") +\n    facet_wrap(~file, ncol = 2) \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot(Gradients, aes(x=Generation, y = Beta))+\n  geom_point(aes(color = sig))+\n  geom_smooth(fill=\"blue\")+\n  scale_color_manual(values = c(\"grey\",\"red\"))+\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"black\")+\n  theme(legend.position = \"none\",\n        panel.background = element_blank())\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNow we’ll perform a similar analysis for the remaining traits.\n\n\nCode\nGradientslong &lt;- data.frame()\nfor(i in seq_along(Traits)){\n\ntraittemp&lt;-allfiles%&gt;%\n  select(Generation, offspring_count, file, !!sym(Traits[i]))%&gt;%\n  group_by(Generation, file)%&gt;%\n  mutate(scaleST0 = as.vector(scale(!!sym(Traits[i]), center = TRUE)))%&gt;%\n  mutate(scaleST02 = scaleST0*scaleST0)%&gt;%\n  mutate(Generation = as.numeric(as.character(Generation)))\n\nGradients &lt;- traittemp %&gt;%\n  group_by(Generation, file) %&gt;%\n  do({\n    model &lt;- lm(offspring_count ~ scaleST0 + scaleST02, data = .)\n    data.frame(\n      Beta = coefficients(model)[2],\n      PB = summary(model)$coef[2, 4],\n      Trait = Traits[i]\n    )\n  })\n\nGradients &lt;- Gradients %&gt;%\n  mutate(sig = if_else(PB &lt; 0.05 , \"Y\", \"N\"))\n\nGradientslong &lt;- rbind(Gradientslong, Gradients)\n\nG &lt;- ggplot(data = GeneAvg, aes(x = as.numeric(Generation), y = !!sym(Genes[i])))+\n    geom_point(data = allfiles, aes(x = as.numeric(Generation), y = !!sym(Genes[i])), size=0.1, alpha = 0.02)+\n    geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y = !!sym(paste(\"mean.\",Genes[i], sep = \"\")), method = \"loess\")) +\n    theme(legend.position = \"none\") +\n    facet_wrap(~file, ncol = 2) \n\nP &lt;- ggplot(data = TraitAvg, aes(x = as.numeric(Generation), y = !!sym(Traits[i])))+\n    geom_smooth(data = TraitAvg, aes(x = as.numeric(Generation), y = !!sym(Traits[i])), method = \"loess\") +\n    geom_point(data=allfiles, aes(x = as.numeric(Generation), y = !!sym(Traits[i]), color = offspring_count), size = 0.5, alpha =0.1)+\n    theme(legend.position = \"none\") +\n    facet_wrap(~file, ncol = 2) \n\n\n\nS &lt;- ggplot(Gradients, aes(x=Generation, y = Beta))+\n  geom_point(aes(color = sig))+\n  geom_smooth(fill=\"blue\")+\n  scale_color_manual(values = c(\"grey\",\"red\"))+\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"black\")+\n  theme(legend.position = \"none\",\n        panel.background = element_blank())\n\nprint(G)\n\nprint(P)\n\nprint(S)\n\n}\n\n\nWarning in geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y =\n!!sym(paste(\"mean.\", : Ignoring unknown aesthetics: method\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning in geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y =\n!!sym(paste(\"mean.\", : Ignoring unknown aesthetics: method\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning in geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y =\n!!sym(paste(\"mean.\", : Ignoring unknown aesthetics: method\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning in geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y =\n!!sym(paste(\"mean.\", : Ignoring unknown aesthetics: method\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning in geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y =\n!!sym(paste(\"mean.\", : Ignoring unknown aesthetics: method\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning in geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y =\n!!sym(paste(\"mean.\", : Ignoring unknown aesthetics: method\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning in geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y =\n!!sym(paste(\"mean.\", : Ignoring unknown aesthetics: method\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning in geom_smooth(data = GeneAvg, aes(x = as.numeric(Generation), y =\n!!sym(paste(\"mean.\", : Ignoring unknown aesthetics: method\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#heatmap",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#heatmap",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "Heatmap",
    "text": "Heatmap\n\n\nCode\nGradMatrix &lt;- Gradientslong %&gt;%\n  select(Generation, Trait, Beta)%&gt;%\n  pivot_wider(names_from = Trait, values_from = Beta)\n\n\nAdding missing grouping variables: `file`\n\n\nCode\npaletteLength &lt;- 50\nmyColor &lt;- colorRampPalette(c(\"blue\", \"white\", \"#ED2024\"))(paletteLength)\n# length(breaks) == length(paletteLength) + 1\n# use floor and ceiling to deal with even/odd length pallettelengths\n\n\nHeatmap &lt;- GradMatrix %&gt;%\n  ungroup()%&gt;%\n  select(Main.Resistance.Trait, Secondary.Resistance.Trait, Slime.View.Range.Trait, \n         Tower.View.Range.Trait, \n         Tower.Attraction.Trait, Slime.Optimal.Distance.Trait, \n         Speed.Trait, Turn.Rate.Trait)\n\n\n\n\n\n\n\nHeatmatrix2 &lt;- as.matrix(Heatmap)\n\nmyBreaks2 &lt;- c(seq(min(Heatmatrix2), 0, length.out=ceiling(paletteLength/2) + 1), seq(max(Heatmatrix2)/paletteLength, max(Heatmatrix2), \n                                                                                      length.out=floor(paletteLength/2)))\n\n\nheatmap2 = pheatmap(Heatmatrix2,\n         cluster_rows = FALSE, # don't cluster rows\n         cluster_cols = TRUE, # don't cluster columns\n         clustering_distance_cols = \"euclidean\",\n         clustering_distance_rows = \"euclidean\",\n         clustering_method = \"complete\",\n         color = myColor,\n         breaks = myBreaks2)"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#single-generation-plots",
    "href": "ProfessorPolymorphic/posts/TwinStickGraphs1/index.html#single-generation-plots",
    "title": "Embedding Data Literacy Content in Project TwinStick",
    "section": "SINGLE GENERATION PLOTS",
    "text": "SINGLE GENERATION PLOTS\n\n\nCode\nsinglegen &lt;- allfiles %&gt;%\n  filter(Generation == 5)\n\nsinglegrad &lt;- Gradientslong %&gt;%\n  filter(Generation == 3)\n\n\nggplot(singlegen, aes(x=Speed.Trait) )+\n  geom_histogram()+\n  facet_grid(reproduce~file)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(singlegen, aes(x=Turn.Rate.Trait) )+\n  geom_histogram()+\n  facet_grid(reproduce~file)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(singlegen, aes(x=Turn.Rate.Trait, y = Speed.Trait, color = reproduce))+\n  geom_point()+\n  facet_wrap(~file)"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/Viz_2Gen_Pedigree/index.html",
    "href": "ProfessorPolymorphic/posts/Viz_2Gen_Pedigree/index.html",
    "title": "VIZ EXPERIMENT 1",
    "section": "",
    "text": "OVERVIEW\nWe’ve been working on a new framework for respresenting some evolutionary and genetics concepts in our twin stick shooter prototype. The idea is to create a 2 generation bipartite network that shows parents and offspring. Ultimately, we want it to be interactive (sorting, filtering, highlighting). I’m going to use some data from Project Hastur to try out some visualizations.\n\n\nDATA DICTIONARY\nI’m importing a single replicate from a larger experiment. The experimental conditions don’t particularly matter for this exercise but just in case you can find a more comprehensive description here.\nOnly 10000 observations of rncol(fulldata)` variables! I don’t need most of this information for this exercise. I’m going subset the data down to two generations (I’ll use 20 and 21, which is right in the middle of the run of 50 generations) and only keep the variables related to the pedigree plus Health for testing sort functions.\n\n\nCode\nGen2021&lt;-fulldata %&gt;%\n  filter(Generation&gt; 19 & Generation&lt;22) %&gt;%\n  select(Generation, ID, P1ID, P2ID, Health)\n\nDictionary &lt;- sumtable(Gen2021, out = \"return\", labels = TRUE, summ = c(\"min(x)\", \"max(x)\", \"notNA(x)\"))\nDictionary$Description &lt;- c(\"Each wave of enemies is a generation\",\n                           \"Unique key for each individual in the replicate\",\n                           \"Value of `ID` for Parent 1\",\n                           \"Value of `ID` for Parent 2\",\n                           \"Essentially Hit Points\")\nknitr::kable(Dictionary)\n\n\n\n\n\nVariable\nMin\nMax\nNotNA\nDescription\n\n\n\n\nGeneration\n20\n21\n400\nEach wave of enemies is a generation\n\n\nID\n4000\n4399\n400\nUnique key for each individual in the replicate\n\n\nP1ID\n3802\n4196\n400\nValue of `ID` for Parent 1\n\n\nP2ID\n3802\n4196\n400\nValue of `ID` for Parent 2\n\n\nHealth\n5120\n56372\n400\nEssentially Hit Points\n\n\n\n\n\n\n\n\n\nVISUALIZATION\nWe need an adjacency matrix.\n\n\nCode\nlibrary(igraph)\nedgep1&lt;-Gen2021 %&gt;%\n  filter(Generation == 21) %&gt;%\n  select(ID, P1ID)\n\nmymat&lt;- get.adjacency(graph.edgelist(as.matrix(edgep1), directed=FALSE))\n\n# Random bipartite graph\ninc &lt;- as.matrix(edgep1)\n\n\n\ng &lt;- graph.data.frame(inc, directed = F)\nV(g)$type &lt;- V(g)$name %in% inc[,2] #the second column of edges is TRUE typ\nplot(g, layout=layout_as_bipartite)"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#purpose",
    "href": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#purpose",
    "title": "Reflections on Mentoring and Diversity",
    "section": "PURPOSE",
    "text": "PURPOSE\nDiscuss and reflect upon mentoring and diversity.\nHear perspectives and ideas from outside our usual echo chambers.\nIdentify actionable suggestions that can be implemented in Year 4."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#intstructions",
    "href": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#intstructions",
    "title": "Reflections on Mentoring and Diversity",
    "section": "INTSTRUCTIONS",
    "text": "INTSTRUCTIONS\nfor i in 1:3{\n\nRandomized table assignments.\n10 minute discussion of a prompt.\nTable scribe records the highlights and themes of the discussion.\n\n}\n\nDebrief."
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#topic-1",
    "href": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#topic-1",
    "title": "Reflections on Mentoring and Diversity",
    "section": "TOPIC 1",
    "text": "TOPIC 1\nWhat defines a highly effective mentoring relationship? What are the roles and responsibilities of the mentor in this relationship? What are the roles and responsibilitees of the mentee?\nTable Assignments:\n\nlibrary(readxl)\n\npeople&lt;-read_excel(\"people.xlsx\")\n\nshuffled_people &lt;- sample(people$Name)\ntables &lt;- split(shuffled_people, ceiling(seq_along(shuffled_people)/6))\n\nfor (i in 1:length(tables)) {\n  cat(\"Table\", i, \": \", tables[[i]], \"\\n\")\n}\n\nTable 1 :  Amruta Kale Lucas Price Devin Mazolewski Frederick Harris Phi Hung Nguyen Molly McVicar \nTable 2 :  Joshua Dahl Yan Lin Claudia Rueckert Paul Gessler Vardayani Ratti Chenhao Li \nTable 3 :  Carson Rueber Al Ekram Elahee Hridoy Andrew Nuss Joseph Savage Monika Gulia-Nuss Haifeng Felix Liao \nTable 4 :  Augustine Ofoegbu Erik Marsh Jeremiah B Reyes Tin Nguyen Zhe Wang Xun Shi \nTable 5 :  Landon Wright Sanaz Salati Luke Sheneman Xiaogang Ma Kelly Upshaw-Bia, DVM Dorothy Wallace \nTable 6 :  Mike Teglas Meifang Li Zhuoming Liu"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#topic-2",
    "href": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#topic-2",
    "title": "Reflections on Mentoring and Diversity",
    "section": "TOPIC 2",
    "text": "TOPIC 2\nWhat is something positive that your institution is doing to promote diversity and inclusion in STEM? Do you have any recommendations that could support safety and belonging or improve inclusion on the Tickbase project?\nTable Assignments:\n\nshuffled_people &lt;- sample(people$Name)\ntables &lt;- split(shuffled_people, ceiling(seq_along(shuffled_people)/6))\n\nfor (i in 1:length(tables)) {\n  cat(\"Table\", i, \": \", tables[[i]], \"\\n\")\n}\n\nTable 1 :  Al Ekram Elahee Hridoy Carson Rueber Erik Marsh Yan Lin Paul Gessler Xiaogang Ma \nTable 2 :  Xun Shi Joseph Savage Zhe Wang Phi Hung Nguyen Meifang Li Lucas Price \nTable 3 :  Haifeng Felix Liao Claudia Rueckert Andrew Nuss Joshua Dahl Luke Sheneman Molly McVicar \nTable 4 :  Monika Gulia-Nuss Landon Wright Vardayani Ratti Kelly Upshaw-Bia, DVM Tin Nguyen Augustine Ofoegbu \nTable 5 :  Jeremiah B Reyes Sanaz Salati Zhuoming Liu Amruta Kale Chenhao Li Frederick Harris \nTable 6 :  Devin Mazolewski Mike Teglas Dorothy Wallace"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#topic-3",
    "href": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#topic-3",
    "title": "Reflections on Mentoring and Diversity",
    "section": "TOPIC 3",
    "text": "TOPIC 3\nThink about a research project that you would like to work on in the future. Imagine the project has just been funded and you are asked for suggestions on how to develop a productive, collaborative, and enjoyable environment for the team. What structure or activities would you suggest?\nTable Assignments:\n\nshuffled_people &lt;- sample(people$Name)\ntables &lt;- split(shuffled_people, ceiling(seq_along(shuffled_people)/6))\n\nfor (i in 1:length(tables)) {\n  cat(\"Table\", i, \": \", tables[[i]], \"\\n\")\n}\n\nTable 1 :  Joshua Dahl Frederick Harris Yan Lin Molly McVicar Xiaogang Ma Haifeng Felix Liao \nTable 2 :  Sanaz Salati Meifang Li Luke Sheneman Tin Nguyen Chenhao Li Andrew Nuss \nTable 3 :  Claudia Rueckert Mike Teglas Devin Mazolewski Dorothy Wallace Zhuoming Liu Joseph Savage \nTable 4 :  Phi Hung Nguyen Amruta Kale Lucas Price Paul Gessler Vardayani Ratti Xun Shi \nTable 5 :  Jeremiah B Reyes Al Ekram Elahee Hridoy Landon Wright Carson Rueber Monika Gulia-Nuss Zhe Wang \nTable 6 :  Erik Marsh Kelly Upshaw-Bia, DVM Augustine Ofoegbu"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#debrief",
    "href": "ProfessorPolymorphic/posts/TickBaseDEI/index.html#debrief",
    "title": "Reflections on Mentoring and Diversity",
    "section": "DEBRIEF",
    "text": "DEBRIEF\nBarrie calls on people to provide summaries and feedback, and their poster board summaries are grouped together on the wall.\nCan we summarize the themes or ideas that emerged for each question across all of the groups?\nAsk the PI and Co-PIs which suggestions appeal to them as something they could help implement on the Ticbase grant before it ends.\n\n\nbrobison@uidaho.edu"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/welcome/index.html",
    "href": "ProfessorPolymorphic/posts/welcome/index.html",
    "title": "You’ve gotta be f***ing kidding…",
    "section": "",
    "text": "Mac wants the flamethrower…\n\nIt’s such a ridiculous notion that anyone on the planet would ever want to read a BLOG, let alone a BLOG authored by some rando nerd like me. Nevertheless, if I force myself to push past the imposter syndrome, disruptive thoughts, anxiety, and the sheer PRETENTIOUSNESS of one’s own BLOG, I can kind of rationalize doing this.\nAllow me to elaborate (he wrote to himself). The varied reasons for setting up a BLOG as part of my research lab website include:\n\nIt forces me to further practice using the tools I need for both research and teaching Data Science, including literate programming (I’m using Quarto), version control (I’m using GitHub), and communicating with data.\nIt gives me a place to put a lot of the intermediate outcomes and products from the game studio. All the grey material that isn’t a journal publication, grant proposal, meeting abstract, etc. usually ends up getting lost. Examples include outreach and eduation materials for our games, videos and images that we don’t put on our largely silent social media channels, etc.\nIt serves as an archive for experimental/exploratory analyses in which I am trying to learn a new technique or explore a new data set, allowing me to return to that idea when its time has finally arrived.\nIn the off chance that one of my posts actually helps someone learn something because they found it by googling their particular error message, I will have begun paying back my massive karmic debt to the internet. I can’t count the number of times someone’s blog post helped me figure out how to accomplish something using R.\nI honestly do enjoy thinking about the intersection of science and horror fiction, and some of our newer game projects are going to serve as the platform for that type of content.\nSpeaking of our newer game projects, I’ll be using this as a venue to post development updates for studio projects.\nI have ADHD. I mean actual psychiatrist diagnosed, full blown, life altering ADHD. Systems and structures are critical components of the coping strategies I have developed over the years. Maybe… just maybe… this BLOG will help me keep a bunch of stuff organized so I can…. HEY LOOK A SHINY THING!\nSpeaking of shiny things… I’m really enjoing the new AI tools for procedural art generation. I’m particularly enamored with midJourney. I’m hoping to use this BLOG for some exploration of those tools.\n\nSo why the thumbnail from The Thing as the image for this post? Well the title of the post is the famous quote from Palmer when he sees Norris’ head crawling away on newly grown insect legs. Anyone who knows me who sees I have a BLOG is going to say what Palmer said…\n\nThe Thing Kidding GIFfrom The Thing Kidding GIFs"
  },
  {
    "objectID": "ProfessorPolymorphic/posts/AIVideo/index.html",
    "href": "ProfessorPolymorphic/posts/AIVideo/index.html",
    "title": "My First AI Generated Video",
    "section": "",
    "text": "Cool?\nI made this video with Kaiber. I uploaded this image as the starting point:\n\nI made the image using MidJourney v5.1\nI upoaded this song for the soundtrack:\nAudio\nwhich I made using the AI Music Generator Mubert.\nThen I used this prompt to make the video:\nA secret underground bioweapons laboratory, in the style of photo taken on film, film grain, vintage, 8k ultrafine detail, private press…\nVideo"
  },
  {
    "objectID": "ProfessorPolymorphic/index.html",
    "href": "ProfessorPolymorphic/index.html",
    "title": "Professor Polymorphic",
    "section": "",
    "text": "“The oldest and strongest emotion of mankind is fear, and the oldest and strongest kind of fear is fear of the unknown”.\n– H.P. Lovecraft, Supernatural Horror in Literature\n\n\n\n\n\nHello! This BLOG is where I share my thoughts on how science informs horror fiction, and whether horror fiction can be used to at least help teach science, if not directly inform scientific research. The posts that appear here will vary in their focus. Some will directly address the intersection of horror and science, while some might focus primarily on either horror fiction or science concepts.\nMy basic premise is driven by the Lovecraft quote you see at the top of the screen. I find this compelling because I am a scientist, and science is one of two tools humanity has invented that pushes back against the unknown."
  }
]